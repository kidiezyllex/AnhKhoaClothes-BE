import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import sys
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import time
import re
import ast
from collections import defaultdict

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

apps_utils_path = os.path.join(current_dir, 'apps', 'utils')
if apps_utils_path not in sys.path:
    sys.path.insert(0, apps_utils_path)

_train_import_error = None
try:
    import train_recommendation
except ImportError as e:
    train_recommendation = None
    _train_import_error = str(e)

_export_import_error = None
try:
    from apps.utils.export_data import export_all_data, ensure_export_directory
except ImportError as e:
    export_all_data = None
    ensure_export_directory = None
    _export_import_error = str(e)

_user_profile_import_error = None
try:
    from apps.utils.user_profile import (
        build_weighted_user_profile, 
        get_interaction_weight, 
        INTERACTION_WEIGHTS,
        compute_cbf_predictions,
        cosine_similarity
    )
except ImportError as e:
    build_weighted_user_profile = None
    get_interaction_weight = None
    INTERACTION_WEIGHTS = None
    compute_cbf_predictions = None
    cosine_similarity = None
    _user_profile_import_error = str(e)

_cbf_utils_import_error = None
try:
    from apps.utils.cbf_utils import (
        apply_personalized_filters,
        apply_articletype_filter,
        apply_age_gender_filter,
        get_allowed_genders
    )
except ImportError as e:
    apply_personalized_filters = None
    apply_articletype_filter = None
    apply_age_gender_filter = None
    get_allowed_genders = None
    _cbf_utils_import_error = str(e)

_outfit_import_error = None
try:
    from apps.utils.outfit_recommendation import (
        generate_outfit_recommendations,
        compute_outfit_score,
        compute_pairwise_compatibility,
        check_usage_compatibility
    )
except ImportError as e:
    generate_outfit_recommendations = None
    compute_outfit_score = None
    compute_pairwise_compatibility = None
    check_usage_compatibility = None
    _outfit_import_error = str(e)

_evaluation_import_error = None
try:
    from apps.utils.evaluation_metrics import (
        compute_cbf_metrics,
        recall_at_k,
        precision_at_k,
        ndcg_at_k,
        diversity,
        coverage
    )
except ImportError as e:
    compute_cbf_metrics = None
    recall_at_k = None
    precision_at_k = None
    ndcg_at_k = None
    diversity = None
    coverage = None
    _evaluation_import_error = str(e)

_gnn_utils_import_error = None
try:
    from apps.utils.gnn_utils import (
        build_graph,
        message_propagation,
        compute_gnn_predictions,
        train_gnn_model
    )
except ImportError as e:
    build_graph = None
    message_propagation = None
    compute_gnn_predictions = None
    train_gnn_model = None
    _gnn_utils_import_error = str(e)

_hybrid_utils_import_error = None
try:
    from apps.utils.hybrid_utils import (
        combine_hybrid_scores
    )
except ImportError as e:
    combine_hybrid_scores = None
    _hybrid_utils_import_error = str(e)

st.set_page_config(
    page_title="Fashion Recommendation System",
    page_icon="üëî",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2ca02c;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .step-header {
        font-size: 1.2rem;
        font-weight: bold;
        color: #d62728;
        margin-top: 1rem;
        background-color: #f0f2f6;
        padding: 0.5rem;
        border-radius: 5px;
    }
    .formula-box {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 5px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_models():
    try:
        with open('recommendation_system/data/preprocessor.pkl', 'rb') as f:
            preprocessor = pickle.load(f)
        
        with open('recommendation_system/models/content_based_model.pkl', 'rb') as f:
            cb_model = pickle.load(f)
        
        with open('recommendation_system/models/gnn_model.pkl', 'rb') as f:
            gnn_model = pickle.load(f)
        
        with open('recommendation_system/models/hybrid_model.pkl', 'rb') as f:
            hybrid_model = pickle.load(f)
        
        return preprocessor, cb_model, gnn_model, hybrid_model
    except Exception as e:
        return None, None, None, None

@st.cache_data
def load_comparison_results():
    try:
        df = pd.read_csv('recommendation_system/evaluation/comparison_results.csv')
        return df
    except:
        return None


ARTIFACTS_DIR = Path(current_dir) / "artifacts"


def _ensure_artifacts_dir() -> Path:
    """ƒê·∫£m b·∫£o th∆∞ m·ª•c artifacts t·ªìn t·∫°i."""
    ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)
    return ARTIFACTS_DIR


def _load_pickle_if_exists(path: Path):
    """Load pickle n·∫øu file t·ªìn t·∫°i, ng∆∞·ª£c l·∫°i tr·∫£ v·ªÅ None."""
    if not path.exists():
        return None
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except Exception:
        return None


def _save_pickle_safely(path: Path, obj) -> None:
    """L∆∞u pickle m·ªôt c√°ch an to√†n, b·ªè qua l·ªói n·∫øu c√≥ v·∫•n ƒë·ªÅ IO."""
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "wb") as f:
            pickle.dump(obj, f)
    except Exception:
        # Kh√¥ng crash app ch·ªâ v√¨ l·ªói ghi file
        pass


def save_predictions_artifact(model_key: str, data: Dict) -> None:
    """
    L∆∞u predictions ra file artifacts cho t·ª´ng lo·∫°i m√¥ h√¨nh.
    model_key: 'cbf' | 'gnn' | 'hybrid'
    """
    base = _ensure_artifacts_dir()
    filename = {
        "cbf": "streamlit_cbf_predictions.pkl",
        "gnn": "streamlit_gnn_predictions.pkl",
        "hybrid": "streamlit_hybrid_predictions.pkl",
    }.get(model_key)
    if not filename:
        return
    _save_pickle_safely(base / filename, data)


def save_intermediate_artifact(key: str, data) -> None:
    """
    L∆∞u intermediate results ra file artifacts.
    key: 'pruned_interactions' | 'feature_encoding' | 'user_profiles' | 
         'gnn_graph' | 'gnn_propagation' | 'gnn_training' |
         'cbf_evaluation_metrics' | 'gnn_evaluation_metrics' | 'hybrid_evaluation_metrics' |
         'personalized_filters' | 'training_time' | 'inference_time' | 
         'gnn_training_time' | 'gnn_inference_time'
    """
    base = _ensure_artifacts_dir()
    filename_mapping = {
        "pruned_interactions": "pruned_interactions.pkl",
        "feature_encoding": "feature_encoding.pkl",
        "user_profiles": "user_profiles.pkl",
        "gnn_graph": "gnn_graph.pkl",
        "gnn_propagation": "gnn_propagation.pkl",
        "gnn_training": "gnn_training.pkl",
        "cbf_evaluation_metrics": "cbf_evaluation_metrics.pkl",
        "gnn_evaluation_metrics": "gnn_evaluation_metrics.pkl",
        "hybrid_evaluation_metrics": "hybrid_evaluation_metrics.pkl",
        "personalized_filters": "personalized_filters.pkl",
        "training_time": "training_time.pkl",
        "inference_time": "inference_time.pkl",
        "gnn_training_time": "gnn_training_time.pkl",
        "gnn_inference_time": "gnn_inference_time.pkl",
    }
    filename = filename_mapping.get(key)
    if filename:
        _save_pickle_safely(base / filename, data)


def load_cached_predictions_into_session() -> None:
    """
    Auto-load predictions ƒë√£ l∆∞u (n·∫øu c√≥) v√†o session_state khi m·ªü app.
    Ch·ªâ n·∫°p n·∫øu session_state ch∆∞a c√≥ key t∆∞∆°ng ·ª©ng.
    """
    base = ARTIFACTS_DIR
    mappings = [
        ("cbf_predictions", "streamlit_cbf_predictions.pkl"),
        ("gnn_predictions", "streamlit_gnn_predictions.pkl"),
        ("hybrid_predictions", "streamlit_hybrid_predictions.pkl"),
    ]
    for state_key, fname in mappings:
        if state_key in st.session_state:
            continue
        path = base / fname
        cached = _load_pickle_if_exists(path)
        if cached:
            st.session_state[state_key] = cached


def _is_valid_data(data) -> bool:
    """Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ h·ª£p l·ªá kh√¥ng (kh√¥ng None, kh√¥ng r·ªóng)."""
    if data is None:
        return False
    if isinstance(data, dict):
        return len(data) > 0
    if isinstance(data, (list, tuple)):
        return len(data) > 0
    if isinstance(data, pd.DataFrame):
        return not data.empty
    # C√°c ki·ªÉu d·ªØ li·ªáu kh√°c (int, float, str) ƒë·ªÅu h·ª£p l·ªá n·∫øu kh√¥ng None
    return True


def restore_all_artifacts() -> None:
    """
    Kh√¥i ph·ª•c t·∫•t c·∫£ c√°c k·∫øt qu·∫£ t·ª´ artifacts v√†o session_state.
    ƒê∆∞·ª£c g·ªçi khi c·∫ßn ƒë·∫£m b·∫£o kh√¥ng m·∫•t d·ªØ li·ªáu sau khi ch·∫°y c√°c b∆∞·ªõc m·ªõi.
    Ch·ªâ restore n·∫øu session_state ch∆∞a c√≥ ho·∫∑c d·ªØ li·ªáu hi·ªán t·∫°i kh√¥ng h·ª£p l·ªá.
    """
    base = ARTIFACTS_DIR
    
    predictions_mappings = [
        ("cbf_predictions", "streamlit_cbf_predictions.pkl"),
        ("gnn_predictions", "streamlit_gnn_predictions.pkl"),
        ("hybrid_predictions", "streamlit_hybrid_predictions.pkl"),
    ]
    for state_key, fname in predictions_mappings:
        path = base / fname
        cached = _load_pickle_if_exists(path)
        if cached:
            # Ch·ªâ restore n·∫øu ch∆∞a c√≥ ho·∫∑c d·ªØ li·ªáu hi·ªán t·∫°i kh√¥ng h·ª£p l·ªá
            if state_key not in st.session_state or not _is_valid_data(st.session_state[state_key]):
                st.session_state[state_key] = cached
    
    # Kh√¥i ph·ª•c c√°c k·∫øt qu·∫£ trung gian (n·∫øu c√≥ l∆∞u)
    intermediate_mappings = [
        ("pruned_interactions", "pruned_interactions.pkl"),
        ("feature_encoding", "feature_encoding.pkl"),
        ("user_profiles", "user_profiles.pkl"),
        ("gnn_graph", "gnn_graph.pkl"),
        ("gnn_propagation", "gnn_propagation.pkl"),
        ("gnn_training", "gnn_training.pkl"),
        ("cbf_evaluation_metrics", "cbf_evaluation_metrics.pkl"),
        ("gnn_evaluation_metrics", "gnn_evaluation_metrics.pkl"),
        ("hybrid_evaluation_metrics", "hybrid_evaluation_metrics.pkl"),
        ("personalized_filters", "personalized_filters.pkl"),
        ("training_time", "training_time.pkl"),
        ("inference_time", "inference_time.pkl"),
        ("gnn_training_time", "gnn_training_time.pkl"),
        ("gnn_inference_time", "gnn_inference_time.pkl"),
    ]
    for state_key, fname in intermediate_mappings:
        path = base / fname
        cached = _load_pickle_if_exists(path)
        if cached:
            # Ch·ªâ restore n·∫øu ch∆∞a c√≥ ho·∫∑c d·ªØ li·ªáu hi·ªán t·∫°i kh√¥ng h·ª£p l·ªá
            if state_key not in st.session_state or not _is_valid_data(st.session_state[state_key]):
                st.session_state[state_key] = cached


def get_artifacts_status() -> Dict[str, bool]:
    """
    Ki·ªÉm tra tr·∫°ng th√°i c·ªßa c√°c artifacts (ƒë√£ l∆∞u hay ch∆∞a).
    Tr·∫£ v·ªÅ dict v·ªõi key l√† t√™n artifact v√† value l√† True n·∫øu ƒë√£ t·ªìn t·∫°i.
    """
    base = ARTIFACTS_DIR
    status = {}
    
    all_mappings = [
        ("cbf_predictions", "streamlit_cbf_predictions.pkl"),
        ("gnn_predictions", "streamlit_gnn_predictions.pkl"),
        ("hybrid_predictions", "streamlit_hybrid_predictions.pkl"),
        ("pruned_interactions", "pruned_interactions.pkl"),
        ("feature_encoding", "feature_encoding.pkl"),
        ("user_profiles", "user_profiles.pkl"),
        ("gnn_graph", "gnn_graph.pkl"),
        ("gnn_propagation", "gnn_propagation.pkl"),
        ("gnn_training", "gnn_training.pkl"),
        ("cbf_evaluation_metrics", "cbf_evaluation_metrics.pkl"),
        ("gnn_evaluation_metrics", "gnn_evaluation_metrics.pkl"),
        ("hybrid_evaluation_metrics", "hybrid_evaluation_metrics.pkl"),
    ]
    
    for state_key, fname in all_mappings:
        path = base / fname
        status[state_key] = path.exists()
    
    return status


def display_pruning_results(result: Dict) -> None:
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ Pruning t·ª´ session_state ho·∫∑c artifacts."""
    if result is None or result.get('pruned_interactions') is None or result['pruned_interactions'].empty:
        return
    
    st.markdown("### üìä Th·ªëng k√™ k·∫øt qu·∫£ Pruning")
    
    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
    with col_stat1:
        st.metric("Users ban ƒë·∫ßu", result['original_users'])
        st.metric("Users sau pruning", result['original_users'] - result['removed_users'])
    with col_stat2:
        st.metric("Products ban ƒë·∫ßu", result['original_products'])
        st.metric("Products sau pruning", result['original_products'] - result['removed_products'])
    with col_stat3:
        st.metric("Interactions ban ƒë·∫ßu", result['original_interactions'])
        st.metric("Interactions sau pruning", len(result['pruned_interactions']))
    with col_stat4:
        st.metric("S·ªë l·∫ßn l·∫∑p", result['iterations'])
        reduction_pct = ((result['original_interactions'] - len(result['pruned_interactions'])) / result['original_interactions'] * 100) if result['original_interactions'] > 0 else 0
        st.metric("Gi·∫£m ƒëi", f"{reduction_pct:.2f}%")
    
    pruned_users = result['original_users'] - result['removed_users']
    pruned_products = result['original_products'] - result['removed_products']
    
    original_density = result['original_interactions'] / (result['original_users'] * result['original_products']) if (result['original_users'] * result['original_products']) > 0 else 0
    original_sparsity = 1 - original_density
    
    pruned_density = len(result['pruned_interactions']) / (pruned_users * pruned_products) if (pruned_users * pruned_products) > 0 else 0
    pruned_sparsity = 1 - pruned_density
    
    improvement = original_sparsity - pruned_sparsity
    
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìã Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ l√†m s·∫°ch",
        "üìâ So s√°nh ƒë·ªô th∆∞a th·ªõt",
        "üìà Qu√° tr√¨nh Pruning qua c√°c l·∫ßn l·∫∑p",
        "üî• Ma tr·∫≠n t∆∞∆°ng t√°c (Heatmap)"
    ])
    
    with tab1:
        st.markdown("### üìã Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ l√†m s·∫°ch $R_{pruned}$")
        st.dataframe(
            result['pruned_interactions'].head(100),
            use_container_width=True
        )
        
    
    with tab2:
        st.markdown("### üìâ So s√°nh ƒë·ªô th∆∞a th·ªõt")
        
        col_sparse1, col_sparse2 = st.columns(2)
        with col_sparse1:
            st.metric("ƒê·ªô th∆∞a ban ƒë·∫ßu", f"{original_sparsity:.4f}")
            st.metric("M·∫≠t ƒë·ªô ban ƒë·∫ßu", f"{original_density:.6f}")
        with col_sparse2:
            st.metric("ƒê·ªô th∆∞a sau pruning", f"{pruned_sparsity:.4f}")
            st.metric("M·∫≠t ƒë·ªô sau pruning", f"{pruned_density:.6f}")
        
        if improvement > 0:
            st.success(f"‚úÖ ƒê·ªô th∆∞a gi·∫£m {improvement:.4f} ({improvement/original_sparsity*100:.2f}%) - M·∫≠t ƒë·ªô d·ªØ li·ªáu tƒÉng!")
        else:
            st.info("‚ÑπÔ∏è M·∫≠t ƒë·ªô d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán cho c√°c users/products c√≤n l·∫°i.")
    
    with tab3:
        if result.get('stats'):
            st.markdown("### üìà Qu√° tr√¨nh Pruning qua c√°c l·∫ßn l·∫∑p")
            stats_df = pd.DataFrame(result['stats'])
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=stats_df['iteration'],
                y=stats_df['users'],
                mode='lines+markers',
                name='Users',
                line=dict(color='#1f77b4')
            ))
            fig.add_trace(go.Scatter(
                x=stats_df['iteration'],
                y=stats_df['products'],
                mode='lines+markers',
                name='Products',
                line=dict(color='#2ca02c')
            ))
            fig.add_trace(go.Scatter(
                x=stats_df['iteration'],
                y=stats_df['interactions'],
                mode='lines+markers',
                name='Interactions',
                line=dict(color='#d62728')
            ))
            fig.update_layout(
                title="Thay ƒë·ªïi s·ªë l∆∞·ª£ng Users, Products v√† Interactions qua c√°c l·∫ßn l·∫∑p",
                xaxis_title="S·ªë l·∫ßn l·∫∑p",
                yaxis_title="S·ªë l∆∞·ª£ng",
                hovermode='x unified'
            )
            st.plotly_chart(fig, use_container_width=True, key="pruning_stats_chart_saved")
        else:
            st.info("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu th·ªëng k√™ qu√° tr√¨nh pruning.")
    
    with tab4:
        if pruned_users <= 100 and pruned_products <= 100:
            st.markdown("### üî• Ma tr·∫≠n t∆∞∆°ng t√°c (Heatmap)")
            st.info("‚ÑπÔ∏è Hi·ªÉn th·ªã ma tr·∫≠n t∆∞∆°ng t√°c d∆∞·ªõi d·∫°ng heatmap (1 = c√≥ t∆∞∆°ng t√°c, 0 = kh√¥ng c√≥ t∆∞∆°ng t√°c)")
            
            # Create interaction matrix
            interaction_matrix = result['pruned_interactions'].pivot_table(
                index='user_id',
                columns='product_id',
                aggfunc='size',
                fill_value=0
            )
            
            interaction_matrix = (interaction_matrix > 0).astype(int)
            
            fig_heatmap = go.Figure(data=go.Heatmap(
                z=interaction_matrix.values,
                x=interaction_matrix.columns,
                y=interaction_matrix.index,
                colorscale='YlOrRd',
                showscale=True,
                colorbar=dict(title="Interaction")
            ))
            fig_heatmap.update_layout(
                title="Ma tr·∫≠n t∆∞∆°ng t√°c User-Product (1 = c√≥ t∆∞∆°ng t√°c, 0 = kh√¥ng c√≥)",
                xaxis_title="Product ID",
                yaxis_title="User ID",
                width=800,
                height=600
            )
            st.plotly_chart(fig_heatmap, use_container_width=True, key="pruning_heatmap_chart_saved")
        else:
            st.info(f"‚ÑπÔ∏è Ma tr·∫≠n qu√° l·ªõn ({pruned_users} users √ó {pruned_products} products) ƒë·ªÉ hi·ªÉn th·ªã heatmap. Ch·ªâ hi·ªÉn th·ªã d·ªØ li·ªáu d·∫°ng b·∫£ng.")
            st.markdown("**üí° G·ª£i √Ω:** Xem d·ªØ li·ªáu d·∫°ng b·∫£ng trong tab 'üìã Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ l√†m s·∫°ch'")
    
    st.markdown("""
    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
    - ‚úÖ Ma tr·∫≠n t∆∞∆°ng t√°c th∆∞a th·ªõt $R$ ƒë∆∞·ª£c l√†m s·∫°ch, gi·∫£m nhi·ªÖu (noise) do t∆∞∆°ng t√°c ng·∫´u nhi√™n ho·∫∑c kh√¥ng ƒë·ªß d·ªØ li·ªáu
    - ‚úÖ TƒÉng m·∫≠t ƒë·ªô d·ªØ li·ªáu t∆∞∆°ng t√°c cho c√°c thu·∫≠t to√°n c·ªông t√°c (GNN)
    - ‚úÖ Lo·∫°i b·ªè c√°c users v√† products c√≥ qu√° √≠t t∆∞∆°ng t√°c, gi√∫p model h·ªçc ƒë∆∞·ª£c patterns r√µ r√†ng h∆°n
    """)


@st.cache_data
def load_products_data(path: str = None):
    """Load products dataset from exports directory."""
    csv_path = path or os.path.join(current_dir, 'apps', 'exports', 'products.csv')
    if not os.path.exists(csv_path):
        return None
    try:
        df = pd.read_csv(csv_path)
        if 'id' in df.columns:
            df['id'] = df['id'].astype(str)
            df = df.set_index('id')
        return df
    except Exception:
        return None


@st.cache_data
def load_users_data(path: str = None):
    """Load users dataset from exports directory."""
    csv_path = path or os.path.join(current_dir, 'apps', 'exports', 'users.csv')
    if not os.path.exists(csv_path):
        return None
    try:
        df = pd.read_csv(csv_path)
        if 'id' in df.columns:
            df['id'] = df['id'].astype(str)
            df = df.set_index('id')
        return df
    except Exception:
        return None


@st.cache_data
def load_interactions_data(path: str = None):
    """Load interactions dataset from exports directory."""
    csv_path = path or os.path.join(current_dir, 'apps', 'exports', 'interactions.csv')
    if not os.path.exists(csv_path):
        return None
    try:
        df = pd.read_csv(csv_path)
        if 'user_id' in df.columns:
            df['user_id'] = df['user_id'].astype(str)
        if 'product_id' in df.columns:
            df['product_id'] = df['product_id'].astype(str)
        return df
    except Exception:
        return None


def get_user_record(user_id: str, users_df: pd.DataFrame):
    if users_df is None or user_id is None:
        return None
    try:
        if user_id in users_df.index:
            return users_df.loc[user_id]
        return users_df.loc[users_df.index.astype(str) == str(user_id)].iloc[0]
    except Exception:
        return None


def get_product_record(product_id: str, products_df: pd.DataFrame):
    if products_df is None or product_id is None:
        return None
    product_key = str(product_id)
    try:
        if products_df.index.name is not None or not isinstance(products_df.index, pd.RangeIndex):
            if product_key in products_df.index.astype(str):
                return products_df.loc[product_key]
        if 'id' in products_df.columns:
            match = products_df[products_df['id'].astype(str) == product_key]
            if not match.empty:
                return match.iloc[0]
    except Exception:
        return None
    return None


def ensure_hybrid_predictions(alpha: float, candidate_pool: int = 200):
    """
    Ensure hybrid predictions are available in session state.
    Recompute when alpha changes or cached predictions missing.
    """
    existing = st.session_state.get('hybrid_predictions')
    if existing and abs(existing.get('alpha', alpha) - alpha) < 1e-6:
        return existing

    cbf_predictions = st.session_state.get('cbf_predictions')
    gnn_predictions = (
        st.session_state.get('gnn_predictions')
        or st.session_state.get('gnn_training')
    )

    if cbf_predictions and gnn_predictions:
        combined = combine_hybrid_scores(
            cbf_predictions,
            gnn_predictions,
            alpha=alpha,
            top_k=max(candidate_pool, 50)
        )
        st.session_state['hybrid_predictions'] = combined
        # l∆∞u l·∫°i hybrid predictions ra artifacts
        save_predictions_artifact("hybrid", combined)
        return combined

    if cbf_predictions and not gnn_predictions:
        fallback = {
            'predictions': cbf_predictions.get('predictions', {}),
            'rankings': cbf_predictions.get('rankings', {}),
            'alpha': alpha,
            'stats': {'note': 'Fallback to CBF scores (GNN predictions missing)'}
        }
        st.session_state['hybrid_predictions'] = fallback
        return fallback

    return existing


def build_user_interaction_preferences(
    user_id: str,
    interactions_df: pd.DataFrame,
    products_df: pd.DataFrame
) -> Dict[str, Dict[str, float]]:
    """
    Derive normalized preference weights from user interaction history.
    Returns dict with article, usage, gender preference maps in [0,1].
    """
    preference_maps = {
        'articleType': defaultdict(float),
        'usage': defaultdict(float),
        'gender': defaultdict(float)
    }

    if (
        interactions_df is None
        or products_df is None
        or interactions_df.empty
        or user_id is None
    ):
        return {k: {} for k in preference_maps}

    user_history = interactions_df[interactions_df['user_id'] == str(user_id)]
    if user_history.empty:
        return {k: {} for k in preference_maps}

    for _, row in user_history.iterrows():
        product_id = str(row.get('product_id'))
        interaction_type = row.get('interaction_type', '').lower()
        weight = INTERACTION_WEIGHTS.get(interaction_type, 1.0)
        product_row = get_product_record(product_id, products_df)
        if product_row is None:
            continue

        article = str(product_row.get('articleType', '')).strip()
        usage = str(product_row.get('usage', '')).strip()
        gender = str(product_row.get('gender', '')).strip()

        if article:
            preference_maps['articleType'][article] += weight
        if usage:
            preference_maps['usage'][usage] += weight
        if gender:
            preference_maps['gender'][gender] += weight

    normalized = {}
    for key, counter in preference_maps.items():
        if not counter:
            normalized[key] = {}
            continue
        max_val = max(counter.values())
        if max_val == 0:
            normalized[key] = {k: 0.0 for k in counter}
        else:
            normalized[key] = {k: v / max_val for k, v in counter.items()}

    return normalized


def build_personalized_candidates(
    user_id: str,
    payload_product_id: str,
    hybrid_predictions: Dict,
    products_df: pd.DataFrame,
    users_df: pd.DataFrame,
    interactions_df: pd.DataFrame,
    top_k: int = 10,
    usage_bonus: float = 0.08,
    gender_primary_bonus: float = 0.06,
    gender_secondary_bonus: float = 0.03,
    interaction_weight: float = 0.05,
    usage_pref_weight: float = 0.04
) -> List[Dict]:
    """Compute prioritized personalized recommendations."""
    if (
        hybrid_predictions is None
        or products_df is None
        or payload_product_id is None
    ):
        return []

    payload_row = get_product_record(payload_product_id, products_df)
    if payload_row is None:
        return []

    payload_article = str(payload_row.get('articleType', '')).strip()
    payload_usage = str(payload_row.get('usage', '')).strip()
    payload_gender = str(payload_row.get('gender', '')).strip()
    payload_gender_lower = payload_gender.lower()

    user_record = get_user_record(user_id, users_df)
    user_age = None
    if user_record is not None:
        try:
            user_age = int(user_record.get('age')) if pd.notna(user_record.get('age')) else None
        except (ValueError, TypeError):
            user_age = None
    user_gender = user_record.get('gender') if user_record is not None else None

    allowed_genders = get_allowed_genders(user_age, user_gender)
    preference_maps = build_user_interaction_preferences(
        user_id,
        interactions_df,
        products_df
    )

    predictions_by_user = hybrid_predictions.get('predictions', {}) or {}
    user_scores = None
    user_key_str = str(user_id)
    if user_key_str in predictions_by_user:
        user_scores = predictions_by_user[user_key_str]
    else:
        for key, val in predictions_by_user.items():
            if str(key) == user_key_str:
                user_scores = val
                break

    if not user_scores:
        return []

    prioritized = []
    seen_product_ids = set()  # Theo d√µi ƒë·ªÉ tr√°nh tr√πng l·∫∑p
    
    for product_id, base_score in sorted(
        user_scores.items(),
        key=lambda x: x[1],
        reverse=True
    ):
        product_id_str = str(product_id)
        
        if product_id_str == str(payload_product_id):
            continue
        
        if product_id_str in seen_product_ids:
            continue
        
        product_row = get_product_record(product_id, products_df)
        if product_row is None:
            continue

        article_type = str(product_row.get('articleType', '')).strip()
        if not article_type or article_type != payload_article:
            continue  # strict articleType requirement

        product_usage = str(product_row.get('usage', '')).strip()
        product_gender = str(product_row.get('gender', '')).strip() or 'Unspecified'
        product_gender_lower = product_gender.lower()
        payload_gender_match = False
        payload_unisex_fallback = False

        if payload_gender:
            if product_gender_lower == payload_gender_lower:
                payload_gender_match = True
            elif product_gender_lower == 'unisex':
                payload_gender_match = True
                payload_unisex_fallback = True
            else:
                continue  # enforce payload gender alignment

        score = float(base_score)
        reasons = []

        if payload_usage and product_usage and product_usage == payload_usage:
            score += usage_bonus
            reasons.append("∆Øu ti√™n do c√πng usage")

        if payload_gender:
            if payload_gender_match and not payload_unisex_fallback:
                score += gender_primary_bonus
                reasons.append("Ph√π h·ª£p gender v·ªõi s·∫£n ph·∫©m ƒëang xem")
            elif payload_unisex_fallback:
                score += gender_secondary_bonus
                reasons.append("Unisex ph√π h·ª£p v·ªõi s·∫£n ph·∫©m ƒëang xem")
        else:
            if product_gender in allowed_genders:
                score += gender_primary_bonus
                reasons.append("Ph√π h·ª£p gi·ªõi t√≠nh/ƒë·ªô tu·ªïi")
            elif product_gender_lower == 'unisex' and (user_age or 0) >= 13:
                score += gender_secondary_bonus
                reasons.append("Unisex ph√π h·ª£p (>=13)")
            else:
                score -= 0.01

        article_pref = preference_maps.get('articleType', {}).get(article_type, 0.0)
        if article_pref > 0:
            score += interaction_weight * article_pref
            reasons.append("Tr·ªçng s·ªë l·ªãch s·ª≠ articleType")

        usage_pref = preference_maps.get('usage', {}).get(product_usage, 0.0)
        if usage_pref > 0:
            score += usage_pref_weight * usage_pref
            reasons.append("Tr·ªçng s·ªë l·ªãch s·ª≠ usage")

        prioritized.append({
            'product_id': product_id_str,
            'score': score,
            'base_score': base_score,
            'usage_match': product_usage == payload_usage and bool(payload_usage),
            'gender_match': payload_gender_match if payload_gender else (product_gender in allowed_genders),
            'reasons': reasons,
            'product_row': product_row
        })
        seen_product_ids.add(product_id_str)

    prioritized.sort(key=lambda x: (-x['score'], x['product_id']))
    return prioritized[:top_k]


def prepare_outfit_data(
    payload_product_id: str,
    payload_row: pd.Series,
    products_df: pd.DataFrame,
    personalized_items: List[Dict],
    hybrid_predictions: Dict,
    user_id: str,
    user_age: Optional[int],
    user_gender: Optional[str]
) -> Dict:
    """T√≠nh to√°n c√°c d·ªØ li·ªáu c·∫ßn thi·∫øt cho outfit suggestions v√† hi·ªÉn th·ªã c√°c b∆∞·ªõc."""
    
    # Item-Item complement dictionary
    complement = {
        # ===== TOPS =====
        'Tshirts': [
            # Men combinations (4 items)
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Jeans', 'Sports Shoes'],
            ['Watches', 'Trousers', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Shorts', 'Sports Shoes'],
            ['Watches', 'Shorts', 'Casual Shoes'],
            # Women combinations (4 items)
            ['Watches', 'Skirts', 'Flats'],
            ['Watches', 'Skirts', 'Heels'],
            ['Watches', 'Jeans', 'Flats'],
            ['Handbags', 'Skirts', 'Casual Shoes'],
        ],
        
        'Shirts': [
            # Men formal (4 items)
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Belts', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Belts', 'Jeans', 'Casual Shoes'],
            # Men casual (4 items)
            ['Watches', 'Shorts', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Casual Shoes'],
        ],
        
        'Tops': [
            # Women combinations (4 items)
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Casual Shoes'],
            ['Watches', 'Skirts', 'Flats'],
            ['Watches', 'Skirts', 'Heels'],
            ['Handbags', 'Shorts', 'Casual Shoes'],
            ['Watches', 'Capris', 'Sports Shoes'],
        ],
        
        'Sweaters': [
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Skirts', 'Flats'],  # Women
        ],
        
        'Sweatshirts': [
            ['Watches', 'Jeans', 'Sports Shoes'],
            ['Caps', 'Shorts', 'Sports Shoes'],
            ['Watches', 'Track Pants', 'Sports Shoes'],
            ['Backpacks', 'Trousers', 'Casual Shoes'],
        ],
        
        'Jackets': [
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Skirts', 'Heels'],  # Women
        ],
        
        # ===== DRESSES (Women only - 3 items v√¨ kh√¥ng c√≥ Bottoms) =====
        'Dresses': [
            ['Watches', 'Heels'],
            ['Watches', 'Flats'],
            ['Handbags', 'Heels'],
            ['Handbags', 'Flats'],
            ['Watches', 'Casual Shoes'],
        ],
        
        # ===== BOTTOMS =====
        'Jeans': [
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Shirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Casual Shoes'],  # Women
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Sweaters', 'Watches', 'Casual Shoes'],
        ],
        
        'Trousers': [
            ['Shirts', 'Watches', 'Formal Shoes'],
            ['Shirts', 'Belts', 'Formal Shoes'],
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Sweaters', 'Watches', 'Formal Shoes'],
            ['Tops', 'Watches', 'Casual Shoes'],  # Women
        ],
        
        'Shorts': [
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Sports Shoes'],  # Women
            ['Sweatshirts', 'Caps', 'Sports Shoes'],
        ],
        
        'Skirts': [
            # Women only (4 items)
            ['Tshirts', 'Watches', 'Flats'],
            ['Tshirts', 'Watches', 'Heels'],
            ['Tops', 'Watches', 'Flats'],
            ['Tops', 'Handbags', 'Heels'],
            ['Tshirts', 'Handbags', 'Casual Shoes'],
        ],
        
        'Capris': [
            # Women only (4 items)
            ['Tops', 'Watches', 'Sports Shoes'],
            ['Tshirts', 'Caps', 'Sports Shoes'],
        ],
        
        'Track Pants': [
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Sweatshirts', 'Watches', 'Sports Shoes'],
            ['Tops', 'Watches', 'Sports Shoes'],  # Women
        ],
        
        # ===== SHOES =====
        'Casual Shoes': [
            ['Tshirts', 'Watches', 'Jeans'],
            ['Shirts', 'Watches', 'Trousers'],
            ['Tops', 'Watches', 'Skirts'],  # Women
        ],
        
        'Formal Shoes': [
            ['Shirts', 'Watches', 'Trousers'],
            ['Shirts', 'Belts', 'Trousers'],
        ],
        
        'Sports Shoes': [
            ['Tshirts', 'Watches', 'Shorts'],
            ['Tshirts', 'Watches', 'Track Pants'],
            ['Sweatshirts', 'Caps', 'Shorts'],
            ['Tops', 'Watches', 'Capris'],  # Women
        ],
        
        'Heels': [
            # Women only (3-4 items)
            ['Dresses', 'Watches'],
            ['Tshirts', 'Watches', 'Skirts'],
            ['Tops', 'Handbags', 'Skirts'],
        ],
        
        'Flats': [
            # Women only (3-4 items)
            ['Dresses', 'Watches'],
            ['Tshirts', 'Watches', 'Skirts'],
            ['Tops', 'Watches', 'Jeans'],
            ['Dresses', 'Handbags'],
        ],
        
        'Flip Flops': [
            ['Tshirts', 'Watches', 'Jeans'],
            ['Tshirts', 'Watches', 'Shorts'],
            ['Dresses', 'Handbags'],  # Women
        ],
        
        'Sandals': [
            ['Tshirts', 'Watches', 'Shorts'],
            ['Tshirts', 'Watches', 'Jeans'],
            ['Tops', 'Watches', 'Skirts'],  # Women
        ],
        
        # ===== ACCESSORIES =====
        'Watches': [
            ['Tshirts', 'Jeans', 'Casual Shoes'],
            ['Shirts', 'Trousers', 'Formal Shoes'],
            ['Tops', 'Skirts', 'Flats'],  # Women
            ['Dresses', 'Heels'],  # Women
        ],
        
        'Handbags': [
            # Women only (3-4 items)
            ['Dresses', 'Heels'],
            ['Dresses', 'Flats'],
            ['Tshirts', 'Skirts', 'Casual Shoes'],
            ['Tops', 'Skirts', 'Heels'],
        ],
        
        'Belts': [
            ['Shirts', 'Trousers', 'Formal Shoes'],
            ['Shirts', 'Jeans', 'Casual Shoes'],
            ['Tshirts', 'Jeans', 'Casual Shoes'],
        ],
        
        'Caps': [
            ['Tshirts', 'Shorts', 'Sports Shoes'],
            ['Sweatshirts', 'Track Pants', 'Sports Shoes'],
            ['Tshirts', 'Capris', 'Sports Shoes'],  # Women
        ],
        
        'Backpacks': [
            ['Tshirts', 'Jeans', 'Casual Shoes'],
            ['Sweatshirts', 'Trousers', 'Sports Shoes'],
            ['Shirts', 'Jeans', 'Casual Shoes'],
        ],
        
        # ===== ADDITIONAL MIXED RULES (Fallbacks for Kids/Simple outfits) =====
        'Skirts': [
            ['Tshirts', 'Watches', 'Flats'],
            ['Tshirts', 'Watches', 'Heels'],
            ['Tops', 'Watches', 'Flats'],
            ['Tops', 'Handbags', 'Heels'],
            ['Tshirts', 'Handbags', 'Casual Shoes'],
            # New flexible rules
            ['Tops', 'Casual Shoes'],
            ['Tshirts', 'Sports Shoes'],
            ['Tops', 'Sandals'],
        ],
        
        'Jeans': [
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Shirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Casual Shoes'],
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Sweaters', 'Watches', 'Casual Shoes'],
            # New flexible rules
            ['Tops', 'Sports Shoes'],
            ['Tshirts', 'Sandals'],
        ],
        
        'Shorts': [
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Sports Shoes'],
            ['Sweatshirts', 'Caps', 'Sports Shoes'],
            # New flexible rules
            ['Tops', 'Casual Shoes'],
            ['Tops', 'Sandals'],
        ],
    }
    
    target_gender = str(payload_row.get('gender', '')).strip()
    
    # Ki·ªÉm tra xem gender c√≥ ph√π h·ª£p v·ªõi target gender kh√¥ng
    def gender_allowed(gender_value: str) -> bool:
        gender_clean = str(gender_value).strip()
        if not target_gender:
            return True
        if not gender_clean:
            return False
        gender_lower = gender_clean.lower()
        target_lower = target_gender.lower()
        if gender_lower == target_lower:
            return True
        return gender_lower == 'unisex'
    
    # Map articleType v·ªõi complement dictionary key
    def map_to_complement_key(row) -> Optional[str]:
        article_type = str(row.get('articleType', '')).strip()
        if article_type in complement:
            return article_type
        article_lower = article_type.lower()
        mappings = {
            't-shirt': 'Tshirts', 't shirt': 'Tshirts', 'tshirt': 'Tshirts',
            'dress': 'Dresses',
            'formal shoe': 'Formal Shoes', 'formal': 'Formal Shoes',
            'casual shoe': 'Casual Shoes', 'casual': 'Casual Shoes',
            'sports shoe': 'Sports Shoes', 'sport shoe': 'Sports Shoes',
            'flip flop': 'Flip Flops', 'flipflop': 'Flip Flops',
            'sandal': 'Sandals',
            'heel': 'Heels',
            'flat': 'Flats',
            'handbag': 'Handbags', 'bag': 'Handbags',
            'sweater': 'Sweaters',
            'sweatshirt': 'Sweatshirts',
            'jacket': 'Jackets',
            'short': 'Shorts',
            'skirt': 'Skirts',
            'jean': 'Jeans',
            'trouser': 'Trousers', 'pant': 'Trousers',
            'shirt': 'Shirts',
            'top': 'Tops',
            'track pant': 'Track Pants', 'trackpant': 'Track Pants',
            'capri': 'Capris',
            'tunic': 'Tunics',
            'backpack': 'Backpacks',
            'belt': 'Belts',
            'cap': 'Caps', 'hat': 'Caps'
        }
        for key, value in mappings.items():
            if key in article_lower:
                return value
        return None
    
    payload_complement_key = map_to_complement_key(payload_row)
    if payload_complement_key is None:
        payload_sub = str(payload_row.get('subCategory', '')).strip().lower()
        payload_article = str(payload_row.get('articleType', '')).strip().lower()
        if payload_sub == 'bottomwear':
            if 'trouser' in payload_article or 'pant' in payload_article:
                payload_complement_key = 'Trousers'
            elif 'jean' in payload_article:
                payload_complement_key = 'Jeans'
            elif 'short' in payload_article:
                payload_complement_key = 'Shorts'
            elif 'skirt' in payload_article:
                payload_complement_key = 'Skirts'
            else:
                payload_complement_key = 'Trousers'
        elif payload_sub == 'topwear':
            if 'tshirt' in payload_article or 't-shirt' in payload_article:
                payload_complement_key = 'Tshirts'
            elif 'shirt' in payload_article:
                payload_complement_key = 'Shirts'
            elif 'top' in payload_article:
                payload_complement_key = 'Tops'
            elif 'sweater' in payload_article:
                payload_complement_key = 'Sweaters'
            elif 'sweatshirt' in payload_article:
                payload_complement_key = 'Sweatshirts'
            elif 'jacket' in payload_article:
                payload_complement_key = 'Jackets'
            else:
                payload_complement_key = 'Tshirts'
        elif payload_sub == 'dress':
            payload_complement_key = 'Dresses'
        elif payload_sub in ['shoes', 'sandal', 'flip flops']:
            if 'formal' in payload_article:
                payload_complement_key = 'Formal Shoes'
            elif 'casual' in payload_article:
                payload_complement_key = 'Casual Shoes'
            elif 'sport' in payload_article:
                payload_complement_key = 'Sports Shoes'
            elif 'heel' in payload_article:
                payload_complement_key = 'Heels'
            elif 'flat' in payload_article:
                payload_complement_key = 'Flats'
            elif 'sandal' in payload_article:
                payload_complement_key = 'Sandals'
            elif 'flip' in payload_article:
                payload_complement_key = 'Flip Flops'
            else:
                payload_complement_key = 'Casual Shoes'
        elif payload_sub == 'bags':
            if 'backpack' in payload_article:
                payload_complement_key = 'Backpacks'
            else:
                payload_complement_key = 'Handbags'
        elif payload_sub in ['accessories', 'wallets', 'belts']:
            if 'belt' in payload_article:
                payload_complement_key = 'Belts'
            elif 'cap' in payload_article or 'hat' in payload_article:
                payload_complement_key = 'Caps'
            elif 'watch' in payload_article:
                payload_complement_key = 'Watches'
            else:
                payload_complement_key = 'Tshirts'
        else:
            payload_complement_key = 'Tshirts'
    
    # L·∫•y c√°c lo·∫°i s·∫£n ph·∫©m t∆∞∆°ng th√≠ch cho payload
    # X·ª≠ l√Ω c·∫£ ƒë·ªãnh d·∫°ng c≈© (danh s√°ch ph·∫≥ng) v√† ƒë·ªãnh d·∫°ng m·ªõi (danh s√°ch c√°c danh s√°ch)
    complement_value = complement.get(payload_complement_key, [])
    if complement_value and isinstance(complement_value[0], list):
        # ƒê·ªãnh d·∫°ng m·ªõi: danh s√°ch c√°c danh s√°ch - l√†m ph·∫≥ng v√† l·∫•y c√°c lo·∫°i duy nh·∫•t
        compatible_types = list(set([item for sublist in complement_value for item in sublist]))
        complement_rules = complement_value  # L∆∞u c√°c quy t·∫Øc ƒë·ªÉ x√¢y d·ª±ng outfit
    else:
        # ƒê·ªãnh d·∫°ng c≈©: danh s√°ch ph·∫≥ng
        compatible_types = complement_value if complement_value else []
        complement_rules = [compatible_types] if compatible_types else []  # Xem nh∆∞ m·ªôt quy t·∫Øc ƒë∆°n
    
    # L·ªçc s·∫£n ph·∫©m theo gi·ªõi t√≠nh
    gender_filtered = products_df.copy()
    if 'gender' in gender_filtered.columns and target_gender:
        gender_filtered = gender_filtered[gender_filtered['gender'].apply(gender_allowed)]
    if gender_filtered.empty:
        gender_filtered = products_df.copy()
    
    allowed_genders_for_user = get_allowed_genders(user_age, user_gender) if get_allowed_genders else []
    user_gender_filtered = products_df.copy()
    if 'gender' in user_gender_filtered.columns and allowed_genders_for_user:
        allowed_set = {str(g).strip().lower() for g in allowed_genders_for_user + ["Unisex"]}
        user_gender_filtered = user_gender_filtered[
            user_gender_filtered['gender'].astype(str).str.strip().str.lower().isin(allowed_set)
        ]
    if user_gender_filtered.empty:
        user_gender_filtered = products_df.copy()
    
    unisex_filtered = products_df.copy()
    if 'gender' in unisex_filtered.columns:
        unisex_filtered = unisex_filtered[
            unisex_filtered['gender'].astype(str).str.strip().str.lower() == 'unisex'
        ]
    if unisex_filtered.empty:
        unisex_filtered = products_df.copy()
    
    score_lookup = {
        item['product_id']: item['score']
        for item in personalized_items
    }
    predictions_by_user = hybrid_predictions.get('predictions', {}) or {}
    user_scores = None
    user_key_str = str(user_id)
    if user_key_str in predictions_by_user:
        user_scores = predictions_by_user[user_key_str]
    else:
        for key, val in predictions_by_user.items():
            if str(key) == user_key_str:
                user_scores = val
                break
    if user_scores is None:
        user_scores = {}
    
    def get_product_score(pid: str) -> float:
        if pid in score_lookup:
            return score_lookup[pid]
        pid_str = str(pid)
        if pid_str in user_scores:
            return user_scores[pid_str]
        try:
            pid_int = int(pid)
            if pid_int in user_scores:
                return user_scores[pid_int]
        except (ValueError, TypeError):
            pass
        for key, val in user_scores.items():
            if str(key) == pid_str:
                return val
        return 0.0
    
    def get_products_by_complement_type(complement_type: str, df: pd.DataFrame) -> pd.DataFrame:
        exact_match = df[df['articleType'].astype(str).str.strip() == complement_type]
        if not exact_match.empty:
            return exact_match
        article_lower = complement_type.lower()
        mask = df['articleType'].astype(str).str.lower().str.strip() == article_lower
        return df[mask]
    
    def build_candidate_pool(complement_type: str, df: pd.DataFrame) -> List[str]:
        type_df = get_products_by_complement_type(complement_type, df)
        if type_df.empty:
            return []
        ids = type_df.index.astype(str)
        scores = [get_product_score(pid) for pid in ids]
        ordered = sorted(zip(ids, scores), key=lambda x: (-x[1], x[0]))
        return [pid for pid, _ in ordered]
    
    candidates_gender = {}
    candidates_user_gender = {}
    candidates_unisex = {}
    candidates_any = {}
    
    for comp_type in compatible_types:
        candidates_gender[comp_type] = build_candidate_pool(comp_type, gender_filtered)
        candidates_user_gender[comp_type] = build_candidate_pool(comp_type, user_gender_filtered)
        candidates_unisex[comp_type] = build_candidate_pool(comp_type, unisex_filtered)
        candidates_any[comp_type] = build_candidate_pool(comp_type, products_df)
    
    if 'Shoes' not in compatible_types:
        compatible_types.append('Shoes')
        candidates_gender['Shoes'] = build_candidate_pool('Shoes', gender_filtered)
        candidates_user_gender['Shoes'] = build_candidate_pool('Shoes', user_gender_filtered)
        candidates_unisex['Shoes'] = build_candidate_pool('Shoes', unisex_filtered)
        candidates_any['Shoes'] = build_candidate_pool('Shoes', products_df)
    
    return {
        'complement': complement,
        'payload_complement_key': payload_complement_key,
        'compatible_types': compatible_types,
        'candidates_gender': candidates_gender,
        'candidates_user_gender': candidates_user_gender,
        'candidates_unisex': candidates_unisex,
        'candidates_any': candidates_any,
        'get_product_score': get_product_score,
        'score_lookup': score_lookup,
        'user_scores': user_scores
    }


def display_outfit_building_steps(
    payload_product_id: str,
    payload_row: pd.Series,
    products_df: pd.DataFrame,
    personalized_items: List[Dict],
    hybrid_predictions: Dict,
    user_id: str,
    outfit_data: Dict
):
    """Hi·ªÉn th·ªã c√°c b∆∞·ªõc th·ª±c t·∫ø trong qu√° tr√¨nh x√¢y d·ª±ng outfit suggestions."""
    
    # B∆∞·ªõc 1: X√¢y d·ª±ng Vector cho Payload Product
    st.markdown("#### 1Ô∏è‚É£ X√¢y d·ª±ng Vector cho Payload Product")
    
    payload_features = {
        'articleType': payload_row.get('articleType', 'N/A'),
        'masterCategory': payload_row.get('masterCategory', 'N/A'),
        'subCategory': payload_row.get('subCategory', 'N/A'),
        'baseColour': payload_row.get('baseColour', 'N/A'),
        'usage': payload_row.get('usage', 'N/A'),
        'gender': payload_row.get('gender', 'N/A')
    }
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**Th√¥ng tin Payload Product:**")
        st.write(f"- Product ID: `{payload_product_id}`")
        st.write(f"- ArticleType: `{payload_features['articleType']}`")
        st.write(f"- MasterCategory: `{payload_features['masterCategory']}`")
        st.write(f"- SubCategory: `{payload_features['subCategory']}`")
        st.write(f"- BaseColour: `{payload_features['baseColour']}`")
        st.write(f"- Usage: `{payload_features['usage']}`")
        st.write(f"- Gender: `{payload_features['gender']}`")
    
    with col2:
        st.markdown("**Vector Representation (One-Hot Encoding):**")
        # T√≠nh vector th·ª±c t·∫ø
        encoding_result = apply_feature_encoding(products_df, ['masterCategory', 'subCategory', 'articleType', 'baseColour', 'usage'])
        
        if encoding_result and 'product_ids' in encoding_result and len(encoding_result['encoded_matrix']) > 0:
            vector = None
            payload_idx = None
            
            # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ t√¨m payload_product_id
            product_ids = encoding_result['product_ids']
            
            # Th·ª≠ t√¨m v·ªõi string
            try:
                payload_idx = product_ids.index(str(payload_product_id))
            except (ValueError, AttributeError):
                # Th·ª≠ t√¨m v·ªõi int
                try:
                    payload_idx = product_ids.index(int(payload_product_id))
                except (ValueError, TypeError):
                    # Th·ª≠ t√¨m b·∫±ng c√°ch so s√°nh tr·ª±c ti·∫øp
                    try:
                        for idx, pid in enumerate(product_ids):
                            if str(pid) == str(payload_product_id) or pid == payload_product_id:
                                payload_idx = idx
                                break
                    except:
                        pass
            
            # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, th·ª≠ t√¨m trong products_df b·∫±ng c√°ch t∆∞∆°ng t·ª± get_product_record
            if payload_idx is None:
                try:
                    product_key = str(payload_product_id)
                    # Th·ª≠ t√¨m b·∫±ng index c·ªßa dataframe
                    if products_df.index.name is not None or not isinstance(products_df.index, pd.RangeIndex):
                        if product_key in products_df.index.astype(str):
                            df_idx = products_df.index.get_loc(product_key)
                            if isinstance(df_idx, slice):
                                df_idx = df_idx.start
                            elif isinstance(df_idx, np.ndarray):
                                df_idx = df_idx[0] if len(df_idx) > 0 else None
                            
                            if df_idx is not None and df_idx < len(product_ids):
                                payload_idx = df_idx
                    # Th·ª≠ t√¨m b·∫±ng c·ªôt 'id'
                    if payload_idx is None and 'id' in products_df.columns:
                        match_idx = products_df[products_df['id'].astype(str) == product_key].index
                        if len(match_idx) > 0:
                            # T√¨m v·ªã tr√≠ c·ªßa match_idx trong product_ids
                            for i, pid in enumerate(product_ids):
                                if str(pid) == str(match_idx[0]) or pid == match_idx[0]:
                                    payload_idx = i
                                    break
                            # N·∫øu kh√¥ng t√¨m th·∫•y, d√πng v·ªã tr√≠ trong dataframe
                            if payload_idx is None:
                                df_pos = products_df.index.get_loc(match_idx[0])
                                if isinstance(df_pos, (int, np.integer)):
                                    payload_idx = int(df_pos)
                                elif isinstance(df_pos, slice):
                                    payload_idx = df_pos.start
                                elif isinstance(df_pos, np.ndarray) and len(df_pos) > 0:
                                    payload_idx = int(df_pos[0])
                except Exception as e:
                    pass
            
            if payload_idx is not None and payload_idx < len(encoding_result['encoded_matrix']):
                vector = encoding_result['encoded_matrix'][payload_idx]
                
                if vector is not None and len(vector) > 0:
                    st.write(f"- Vector dimension: **{len(vector)}**")
                    st.write(f"- Non-zero elements: **{int(np.sum(vector))}**")
                    st.write(f"- Sparsity: **{1 - np.sum(vector)/len(vector):.2%}**")
                    
                    # Hi·ªÉn th·ªã m·ªôt ph·∫ßn vector
                    non_zero_indices = np.where(vector > 0)[0]
                    if len(non_zero_indices) > 0:
                        st.write("**Active features:**")
                        feature_names = encoding_result.get('feature_names', [])
                        for idx in non_zero_indices[:15]:  # Hi·ªÉn th·ªã 15 ƒë·∫∑c tr∆∞ng ƒë·∫ßu
                            if idx < len(feature_names):
                                st.write(f"  - `{feature_names[idx]}`: **1**")
                        if len(non_zero_indices) > 15:
                            st.write(f"  - ... v√† {len(non_zero_indices) - 15} features kh√°c")
                    
                    # Hi·ªÉn th·ªã vector d·∫°ng b·∫£ng tr·ª±c ti·∫øp
                    st.markdown("**üìä Vector Representation:**")
                    feature_names = encoding_result.get('feature_names', [])
                    
                    # T·∫°o d·ªØ li·ªáu cho b·∫£ng - hi·ªÉn th·ªã t·∫•t c·∫£ features (k·ªÉ c·∫£ gi√° tr·ªã 0)
                    table_data = []
                    for idx in range(len(vector)):
                        feature_name = feature_names[idx] if idx < len(feature_names) else f"Feature_{idx}"
                        table_data.append({
                            'Index': idx,
                            'Feature Name': feature_name,
                            'Value': int(vector[idx])
                        })
                    
                    if table_data:
                        vector_df = pd.DataFrame(table_data)
                        items_per_page = 50  # S·ªë items hi·ªÉn th·ªã m·ªói trang
                        
                        # Ph√¢n trang n·∫øu c√≥ nhi·ªÅu items
                        if len(table_data) > items_per_page:
                            total_pages = (len(table_data) + items_per_page - 1) // items_per_page
                            page_num = st.number_input(
                                f"Trang (1-{total_pages})",
                                min_value=1,
                                max_value=total_pages,
                                value=1,
                                key=f"vector_page_{payload_product_id}"
                            )
                            start_idx = (page_num - 1) * items_per_page
                            end_idx = start_idx + items_per_page
                            display_df = vector_df.iloc[start_idx:end_idx]
                            st.dataframe(
                                display_df,
                                use_container_width=True,
                                hide_index=True
                            )
                            st.caption(f"Hi·ªÉn th·ªã {start_idx + 1}-{min(end_idx, len(table_data))} / {len(table_data)} features")
                        else:
                            st.dataframe(
                                vector_df,
                                use_container_width=True,
                                hide_index=True
                            )
                        
                        # Th·ªëng k√™
                        st.caption(f"üìà T·ªïng s·ªë features: {len(vector)} | Active features: {len(non_zero_indices)} | Zero features: {len(vector) - len(non_zero_indices)}")
                    else:
                        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã")
                else:
                    st.warning("‚ö†Ô∏è Vector r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá")
            else:
                st.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y payload product ID `{payload_product_id}` trong encoding result")
                st.write(f"**Debug info:**")
                st.write(f"- S·ªë l∆∞·ª£ng products trong encoding: {len(product_ids)}")
                st.write(f"- Payload ID type: {type(payload_product_id)}")
                if len(product_ids) > 0:
                    st.write(f"- Sample product IDs (5 ƒë·∫ßu): {product_ids[:5]}")
        else:
            if not encoding_result:
                st.error("‚ùå Kh√¥ng th·ªÉ t·∫°o encoding result")
            elif 'product_ids' not in encoding_result:
                st.error("‚ùå Encoding result thi·∫øu 'product_ids'")
            elif len(encoding_result.get('encoded_matrix', [])) == 0:
                st.error("‚ùå Encoded matrix r·ªóng")
    
    st.divider()
    
    # B∆∞·ªõc 2: C·∫•u tr√∫c C√¢y (Complement Types)
    st.markdown("#### 2Ô∏è‚É£ C·∫•u tr√∫c C√¢y - C√°c nh√≥m s·∫£n ph·∫©m t∆∞∆°ng th√≠ch")
    
    payload_complement_key = outfit_data.get('payload_complement_key', 'Unknown')
    compatible_types = outfit_data.get('compatible_types', [])
    candidates_gender = outfit_data.get('candidates_gender', {})
    candidates_user_gender = outfit_data.get('candidates_user_gender', {})
    candidates_unisex = outfit_data.get('candidates_unisex', {})
    candidates_any = outfit_data.get('candidates_any', {})
    
    st.write(f"**Payload Product Type:** `{payload_complement_key}`")
    st.write(f"**Compatible Types (t·ª´ Complement Dictionary):** {len(compatible_types)} nh√≥m")
    
    # Hi·ªÉn th·ªã c·∫•u tr√∫c c√¢y
    tree_structure = f"Payload Product: {payload_complement_key} (ID: {payload_product_id})\n"
    tree_structure += f"‚îú‚îÄ‚îÄ Compatible Groups ({len(compatible_types[:4])} nh√≥m ƒë∆∞·ª£c s·ª≠ d·ª•ng):\n"
    for i, comp_type in enumerate(compatible_types[:4], 1):
        count_gender = len(candidates_gender.get(comp_type, []))
        tree_structure += f"‚îÇ   {i}. {comp_type} ({count_gender} candidates)\n"
    
    st.code(tree_structure, language='text')
    
    st.divider()
    
    # B∆∞·ªõc 3: T√≠nh ƒêi·ªÉm
    st.markdown("#### 3Ô∏è‚É£ T√≠nh ƒêi·ªÉm cho c√°c s·∫£n ph·∫©m trong m·ªói nh√≥m")
    
    get_product_score_func = outfit_data.get('get_product_score')
    score_lookup = outfit_data.get('score_lookup', {})
    user_scores = outfit_data.get('user_scores', {})
    
    def get_product_score(pid: str) -> float:
        """Helper function ƒë·ªÉ t√≠nh ƒëi·ªÉm s·∫£n ph·∫©m."""
        if get_product_score_func:
            return get_product_score_func(pid)
        # D·ª± ph√≤ng n·∫øu kh√¥ng c√≥ function
        if pid in score_lookup:
            return score_lookup[pid]
        pid_str = str(pid)
        if pid_str in user_scores:
            return user_scores[pid_str]
        try:
            pid_int = int(pid)
            if pid_int in user_scores:
                return user_scores[pid_int]
        except (ValueError, TypeError):
            pass
        for key, val in user_scores.items():
            if str(key) == pid_str:
                return val
        return 0.0
    
    # Hi·ªÉn th·ªã ƒëi·ªÉm s·ªë cho top 5 s·∫£n ph·∫©m trong m·ªói nh√≥m
    for comp_type in compatible_types[:4]:
        st.markdown(f"**Nh√≥m: {comp_type}**")
        
        # L·∫•y top candidates t·ª´ pool gender (∆∞u ti√™n)
        pool = candidates_gender.get(comp_type, [])
        if not pool:
            pool = candidates_user_gender.get(comp_type, [])
        if not pool:
            pool = candidates_unisex.get(comp_type, [])
        if not pool:
            pool = candidates_any.get(comp_type, [])
        
        if pool:
            scores_data = []
            for pid in pool[:5]:  # Top 5
                score = get_product_score(pid)
                product_row = get_product_record(pid, products_df)
                if product_row is not None:
                    personalized_score = score_lookup.get(pid, 0.0)
                    scores_data.append({
                        'Product ID': pid,
                        'ArticleType': product_row.get('articleType', 'N/A'),
                        'Hybrid Score': f"{score:.4f}",
                        'Personalized Score': f"{personalized_score:.4f}",
                        'Total Score': f"{score:.4f}"
                    })
            
            if scores_data:
                scores_df = pd.DataFrame(scores_data)
                st.dataframe(scores_df, use_container_width=True, hide_index=True)
        else:
            st.info(f"Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o trong nh√≥m {comp_type}")
    
    st.divider()
    
    # B∆∞·ªõc 4: Item-Item Matching
    st.markdown("#### 4Ô∏è‚É£ Item-Item Matching & Outfit Construction")
    st.write("**Quy tr√¨nh ch·ªçn s·∫£n ph·∫©m:**")
    st.write("""
    1. B·∫Øt ƒë·∫ßu t·ª´ payload product
    2. Duy·ªát qua c√°c nh√≥m t∆∞∆°ng th√≠ch (theo th·ª© t·ª± ∆∞u ti√™n)
    3. Ch·ªçn s·∫£n ph·∫©m c√≥ ƒëi·ªÉm cao nh·∫•t t·ª´ m·ªói nh√≥m
    4. Ki·ªÉm tra t∆∞∆°ng th√≠ch v·ªÅ gender v√† complement relationship
    5. T·ªïng h·ª£p th√†nh outfit ho√†n ch·ªânh
    """)
    
    st.info("üí° C√°c outfit suggestions s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã b√™n d∆∞·ªõi sau khi ho√†n t·∫•t qu√° tr√¨nh matching.")


def build_outfit_suggestions(
    user_id: str,
    payload_product_id: str,
    personalized_items: List[Dict],
    products_df: pd.DataFrame,
    hybrid_predictions: Dict,
    user_age: Optional[int],
    user_gender: Optional[str],
    max_outfits: int = 3
) -> List[Dict]:
    """
    Create outfits based on Item-Item complement relationships.
    Uses complement dictionary to find compatible items instead of usage-based filtering.
    """
    if (
        products_df is None
        or personalized_items is None
        or hybrid_predictions is None
    ):
        return []

    payload_row = get_product_record(payload_product_id, products_df)
    if payload_row is None:
        return []

    # Item-Item complement dictionary
    complement = {
        # ===== TOPS =====
        'Tshirts': [
            # Men combinations (4 items)
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Jeans', 'Sports Shoes'],
            ['Watches', 'Trousers', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Shorts', 'Sports Shoes'],
            ['Watches', 'Shorts', 'Casual Shoes'],
            # Women combinations (4 items)
            ['Watches', 'Skirts', 'Flats'],
            ['Watches', 'Skirts', 'Heels'],
            ['Watches', 'Jeans', 'Flats'],
            ['Handbags', 'Skirts', 'Casual Shoes'],
        ],
        
        'Shirts': [
            # Men formal (4 items)
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Belts', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Belts', 'Jeans', 'Casual Shoes'],
            # Men casual (4 items)
            ['Watches', 'Shorts', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Casual Shoes'],
        ],
        
        'Tops': [
            # Women combinations (4 items)
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Casual Shoes'],
            ['Watches', 'Skirts', 'Flats'],
            ['Watches', 'Skirts', 'Heels'],
            ['Handbags', 'Shorts', 'Casual Shoes'],
            ['Watches', 'Capris', 'Sports Shoes'],
        ],
        
        'Sweaters': [
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Skirts', 'Flats'],  # Women
        ],
        
        'Sweatshirts': [
            ['Watches', 'Jeans', 'Sports Shoes'],
            ['Caps', 'Shorts', 'Sports Shoes'],
            ['Watches', 'Track Pants', 'Sports Shoes'],
            ['Backpacks', 'Trousers', 'Casual Shoes'],
        ],
        
        'Jackets': [
            ['Watches', 'Jeans', 'Casual Shoes'],
            ['Watches', 'Trousers', 'Formal Shoes'],
            ['Watches', 'Skirts', 'Heels'],  # Women
        ],
        
        # ===== DRESSES (Women only - 3 items v√¨ kh√¥ng c√≥ Bottoms) =====
        'Dresses': [
            ['Watches', 'Heels'],
            ['Watches', 'Flats'],
            ['Handbags', 'Heels'],
            ['Handbags', 'Flats'],
            ['Watches', 'Casual Shoes'],
        ],
        
        # ===== BOTTOMS =====
        'Jeans': [
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Shirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Casual Shoes'],  # Women
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Sweaters', 'Watches', 'Casual Shoes'],
        ],
        
        'Trousers': [
            ['Shirts', 'Watches', 'Formal Shoes'],
            ['Shirts', 'Belts', 'Formal Shoes'],
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Sweaters', 'Watches', 'Formal Shoes'],
            ['Tops', 'Watches', 'Casual Shoes'],  # Women
        ],
        
        'Shorts': [
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Sports Shoes'],  # Women
            ['Sweatshirts', 'Caps', 'Sports Shoes'],
        ],
        
        'Skirts': [
            # Women only (4 items)
            ['Tshirts', 'Watches', 'Flats'],
            ['Tshirts', 'Watches', 'Heels'],
            ['Tops', 'Watches', 'Flats'],
            ['Tops', 'Handbags', 'Heels'],
            ['Tshirts', 'Handbags', 'Casual Shoes'],
        ],
        
        'Capris': [
            # Women only (4 items)
            ['Tops', 'Watches', 'Sports Shoes'],
            ['Tshirts', 'Caps', 'Sports Shoes'],
        ],
        
        'Track Pants': [
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Sweatshirts', 'Watches', 'Sports Shoes'],
            ['Tops', 'Watches', 'Sports Shoes'],  # Women
        ],
        
        # ===== SHOES =====
        'Casual Shoes': [
            ['Tshirts', 'Watches', 'Jeans'],
            ['Shirts', 'Watches', 'Trousers'],
            ['Tops', 'Watches', 'Skirts'],  # Women
        ],
        
        'Formal Shoes': [
            ['Shirts', 'Watches', 'Trousers'],
            ['Shirts', 'Belts', 'Trousers'],
        ],
        
        'Sports Shoes': [
            ['Tshirts', 'Watches', 'Shorts'],
            ['Tshirts', 'Watches', 'Track Pants'],
            ['Sweatshirts', 'Caps', 'Shorts'],
            ['Tops', 'Watches', 'Capris'],  # Women
        ],
        
        'Heels': [
            # Women only (3-4 items)
            ['Dresses', 'Watches'],
            ['Tshirts', 'Watches', 'Skirts'],
            ['Tops', 'Handbags', 'Skirts'],
        ],
        
        'Flats': [
            # Women only (3-4 items)
            ['Dresses', 'Watches'],
            ['Tshirts', 'Watches', 'Skirts'],
            ['Tops', 'Watches', 'Jeans'],
            ['Dresses', 'Handbags'],
        ],
        
        'Flip Flops': [
            ['Tshirts', 'Watches', 'Jeans'],
            ['Tshirts', 'Watches', 'Shorts'],
            ['Dresses', 'Handbags'],  # Women
        ],
        
        'Sandals': [
            ['Tshirts', 'Watches', 'Shorts'],
            ['Tshirts', 'Watches', 'Jeans'],
            ['Tops', 'Watches', 'Skirts'],  # Women
        ],
        
        # ===== ACCESSORIES =====
        'Watches': [
            ['Tshirts', 'Jeans', 'Casual Shoes'],
            ['Shirts', 'Trousers', 'Formal Shoes'],
            ['Tops', 'Skirts', 'Flats'],  # Women
            ['Dresses', 'Heels'],  # Women
        ],
        
        'Handbags': [
            # Women only (3-4 items)
            ['Dresses', 'Heels'],
            ['Dresses', 'Flats'],
            ['Tshirts', 'Skirts', 'Casual Shoes'],
            ['Tops', 'Skirts', 'Heels'],
        ],
        
        'Belts': [
            ['Shirts', 'Trousers', 'Formal Shoes'],
            ['Shirts', 'Jeans', 'Casual Shoes'],
            ['Tshirts', 'Jeans', 'Casual Shoes'],
        ],
        
        'Caps': [
            ['Tshirts', 'Shorts', 'Sports Shoes'],
            ['Sweatshirts', 'Track Pants', 'Sports Shoes'],
            ['Tshirts', 'Capris', 'Sports Shoes'],  # Women
        ],
        
        'Backpacks': [
            ['Tshirts', 'Jeans', 'Casual Shoes'],
            ['Sweatshirts', 'Trousers', 'Sports Shoes'],
            ['Shirts', 'Jeans', 'Casual Shoes'],
        ],
        
        # ===== ADDITIONAL MIXED RULES (Fallbacks for Kids/Simple outfits) =====
        'Skirts': [
            ['Tshirts', 'Watches', 'Flats'],
            ['Tshirts', 'Watches', 'Heels'],
            ['Tops', 'Watches', 'Flats'],
            ['Tops', 'Handbags', 'Heels'],
            ['Tshirts', 'Handbags', 'Casual Shoes'],
            # New flexible rules
            ['Tops', 'Casual Shoes'],
            ['Tshirts', 'Sports Shoes'],
            ['Tops', 'Sandals'],
        ],
        
        'Jeans': [
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Shirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Casual Shoes'],
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Sweaters', 'Watches', 'Casual Shoes'],
            # New flexible rules
            ['Tops', 'Sports Shoes'],
            ['Tshirts', 'Sandals'],
        ],
        
        'Shorts': [
            ['Tshirts', 'Watches', 'Sports Shoes'],
            ['Tshirts', 'Watches', 'Casual Shoes'],
            ['Tops', 'Watches', 'Sports Shoes'],
            ['Sweatshirts', 'Caps', 'Sports Shoes'],
            # New flexible rules
            ['Tops', 'Casual Shoes'],
            ['Tops', 'Sandals'],
        ],
    }

    target_gender = str(payload_row.get('gender', '')).strip()
    
    # L·∫•y c√°c gi·ªõi t√≠nh ƒë∆∞·ª£c ph√©p cho user
    allowed_genders_for_user = get_allowed_genders(user_age, user_gender) if get_allowed_genders else []
    
    def gender_allowed(gender_value: str) -> bool:
        gender_clean = str(gender_value).strip()
        if not target_gender:
            return True
        if not gender_clean:
            return False
        gender_lower = gender_clean.lower()
        target_lower = target_gender.lower()
        if gender_lower == target_lower:
            return True
        return gender_lower == 'unisex'

    # √Ånh x·∫° articleType tr·ª±c ti·∫øp t·ªõi c√°c kh√≥a b·ªï tr·ª£ (s·ª≠ d·ª•ng articleType ch√≠nh x√°c t·ª´ CSV)
    def map_to_complement_key(row) -> Optional[str]:
        """√Ånh x·∫° articleType c·ªßa s·∫£n ph·∫©m t·ªõi kh√≥a t·ª´ ƒëi·ªÉn b·ªï tr·ª£ (s·ª≠ d·ª•ng articleType ch√≠nh x√°c t·ª´ CSV)."""
        article_type = str(row.get('articleType', '')).strip()
        
        # √Ånh x·∫° tr·ª±c ti·∫øp: s·ª≠ d·ª•ng articleType nh∆∞ hi·ªán t·∫°i n·∫øu n√≥ t·ªìn t·∫°i trong t·ª´ ƒëi·ªÉn b·ªï tr·ª£
        if article_type in complement:
            return article_type
        
        # Chu·∫©n h√≥a v√† √°nh x·∫° c√°c bi·∫øn th·ªÉ ph·ªï bi·∫øn
        article_lower = article_type.lower()
        
        # √Ånh x·∫° c√°c bi·∫øn th·ªÉ t·ªõi c√°c kh√≥a articleType chu·∫©n
        if article_lower in ['t-shirt', 't shirt', 'tshirt']:
            return 'Tshirts'
        if article_lower in ['dress']:
            return 'Dresses'
        if article_lower in ['formal shoe', 'formal']:
            return 'Formal Shoes'
        if article_lower in ['casual shoe', 'casual']:
            return 'Casual Shoes'
        if article_lower in ['sports shoe', 'sport shoe']:
            return 'Sports Shoes'
        if article_lower in ['flip flop', 'flipflop']:
            return 'Flip Flops'
        if article_lower in ['sandal']:
            return 'Sandals'
        if article_lower in ['heel']:
            return 'Heels'
        if article_lower in ['flat']:
            return 'Flats'
        if article_lower in ['handbag', 'bag']:
            return 'Handbags'
        if article_lower in ['sweater']:
            return 'Sweaters'
        if article_lower in ['sweatshirt']:
            return 'Sweatshirts'
        if article_lower in ['jacket']:
            return 'Jackets'
        if article_lower in ['short']:
            return 'Shorts'
        if article_lower in ['skirt']:
            return 'Skirts'
        if article_lower in ['jean']:
            return 'Jeans'
        if article_lower in ['trouser', 'pant']:
            return 'Trousers'
        if article_lower in ['shirt']:
            return 'Shirts'
        if article_lower in ['top']:
            return 'Tops'
        if article_lower in ['track pant', 'trackpant']:
            return 'Track Pants'
        if article_lower in ['capri']:
            return 'Capris'
        if article_lower in ['tunic']:
            return 'Tunics'
        if article_lower in ['backpack']:
            return 'Backpacks'
        if article_lower in ['belt']:
            return 'Belts'
        if article_lower in ['cap', 'hat']:
            return 'Caps'
        if article_lower in ['watch', 'watches']:
            return 'Watches'
        if article_lower in ['shoe', 'shoes']:
            return 'Casual Shoes'
        
        return None

    payload_complement_key = map_to_complement_key(payload_row)
    if payload_complement_key is None:
        # D·ª± ph√≤ng: th·ª≠ suy lu·∫≠n t·ª´ subCategory
        payload_sub = str(payload_row.get('subCategory', '')).strip().lower()
        payload_article = str(payload_row.get('articleType', '')).strip().lower()
        
        if payload_sub == 'bottomwear':
            if 'trouser' in payload_article or 'pant' in payload_article:
                payload_complement_key = 'Trousers'
            elif 'jean' in payload_article:
                payload_complement_key = 'Jeans'
            elif 'short' in payload_article:
                payload_complement_key = 'Shorts'
            elif 'skirt' in payload_article:
                payload_complement_key = 'Skirts'
            else:
                payload_complement_key = 'Trousers'
        elif payload_sub == 'topwear':
            if 'tshirt' in payload_article or 't-shirt' in payload_article:
                payload_complement_key = 'Tshirts'
            elif 'shirt' in payload_article:
                payload_complement_key = 'Shirts'
            elif 'top' in payload_article:
                payload_complement_key = 'Tops'
            elif 'sweater' in payload_article:
                payload_complement_key = 'Sweaters'
            elif 'sweatshirt' in payload_article:
                payload_complement_key = 'Sweatshirts'
            elif 'jacket' in payload_article:
                payload_complement_key = 'Jackets'
            else:
                payload_complement_key = 'Tshirts'
        elif payload_sub == 'dress':
            payload_complement_key = 'Dresses'
        elif payload_sub in ['shoes', 'sandal', 'flip flops']:
            if 'formal' in payload_article:
                payload_complement_key = 'Formal Shoes'
            elif 'casual' in payload_article:
                payload_complement_key = 'Casual Shoes'
            elif 'sport' in payload_article:
                payload_complement_key = 'Sports Shoes'
            elif 'heel' in payload_article:
                payload_complement_key = 'Heels'
            elif 'flat' in payload_article:
                payload_complement_key = 'Flats'
            elif 'sandal' in payload_article:
                payload_complement_key = 'Sandals'
            elif 'flip' in payload_article:
                payload_complement_key = 'Flip Flops'
            else:
                payload_complement_key = 'Casual Shoes'
        elif payload_sub == 'bags':
            payload_complement_key = 'Handbags'
        else:
            # D·ª± ph√≤ng m·∫∑c ƒë·ªãnh
            payload_complement_key = 'Tshirts'

    # L·∫•y c√°c lo·∫°i s·∫£n ph·∫©m t∆∞∆°ng th√≠ch cho payload
    # L·∫•y c√°c lo·∫°i s·∫£n ph·∫©m t∆∞∆°ng th√≠ch cho payload
    # X·ª≠ l√Ω c·∫£ ƒë·ªãnh d·∫°ng c≈© (danh s√°ch ph·∫≥ng) v√† ƒë·ªãnh d·∫°ng m·ªõi (danh s√°ch c√°c danh s√°ch)
    complement_value = complement.get(payload_complement_key, [])
    if complement_value and isinstance(complement_value[0], list):
        # ƒê·ªãnh d·∫°ng m·ªõi: danh s√°ch c√°c danh s√°ch - l√†m ph·∫≥ng v√† l·∫•y c√°c lo·∫°i duy nh·∫•t
        compatible_types = list(set([item for sublist in complement_value for item in sublist]))
        complement_rules = complement_value  # L∆∞u c√°c quy t·∫Øc ƒë·ªÉ x√¢y d·ª±ng outfit
    else:
        # ƒê·ªãnh d·∫°ng c≈©: danh s√°ch ph·∫≥ng
        compatible_types = complement_value if complement_value else []
        complement_rules = [compatible_types] if compatible_types else []  # Xem nh∆∞ m·ªôt quy t·∫Øc ƒë∆°n

    # L·ªçc s·∫£n ph·∫©m theo t∆∞∆°ng th√≠ch gi·ªõi t√≠nh
    gender_filtered = products_df.copy()
    if 'gender' in gender_filtered.columns and target_gender:
        gender_filtered = gender_filtered[gender_filtered['gender'].apply(gender_allowed)]
    if gender_filtered.empty:
        gender_filtered = products_df.copy()

    user_gender_filtered = products_df.copy()
    if 'gender' in user_gender_filtered.columns and allowed_genders_for_user:
        allowed_set = {str(g).strip().lower() for g in allowed_genders_for_user + ["Unisex"]}
        user_gender_filtered = user_gender_filtered[
            user_gender_filtered['gender'].astype(str).str.strip().str.lower().isin(allowed_set)
        ]
    if user_gender_filtered.empty:
        user_gender_filtered = products_df.copy()

    unisex_filtered = products_df.copy()
    if 'gender' in unisex_filtered.columns:
        unisex_filtered = unisex_filtered[
            unisex_filtered['gender'].astype(str).str.strip().str.lower() == 'unisex'
        ]
    if unisex_filtered.empty:
        unisex_filtered = products_df.copy()

    score_lookup = {
        item['product_id']: item['score']
        for item in personalized_items
    }
    predictions_by_user = hybrid_predictions.get('predictions', {}) or {}
    user_scores = None
    user_key_str = str(user_id)
    if user_key_str in predictions_by_user:
        user_scores = predictions_by_user[user_key_str]
    else:
        for key, val in predictions_by_user.items():
            if str(key) == user_key_str:
                user_scores = val
                break
    if user_scores is None:
        user_scores = {}

    def get_product_score(pid: str) -> float:
        """Tra c·ª©u ƒëi·ªÉm s·∫£n ph·∫©m m·∫°nh m·∫Ω t·ª´ score_lookup ho·∫∑c user_scores."""
        if pid in score_lookup:
            return score_lookup[pid]
        pid_str = str(pid)
        if pid_str in user_scores:
            return user_scores[pid_str]
        try:
            pid_int = int(pid)
            if pid_int in user_scores:
                return user_scores[pid_int]
        except (ValueError, TypeError):
            pass
        for key, val in user_scores.items():
            if str(key) == pid_str:
                return val
        return 0.0

    def is_compatible_with_payload(product_row) -> bool:
        """Ki·ªÉm tra xem s·∫£n ph·∫©m c√≥ t∆∞∆°ng th√≠ch v·ªõi payload d·ª±a tr√™n c√°c quy t·∫Øc b·ªï tr·ª£ kh√¥ng."""
        product_complement_key = map_to_complement_key(product_row)
        if product_complement_key is None:
            return False
        
        # Ki·ªÉm tra xem kh√≥a b·ªï tr·ª£ c·ªßa s·∫£n ph·∫©m c√≥ trong compatible_types kh√¥ng
        return product_complement_key in compatible_types

    def get_products_by_complement_type(complement_type: str, df: pd.DataFrame) -> pd.DataFrame:
        """L·∫•y c√°c s·∫£n ph·∫©m kh·ªõp v·ªõi m·ªôt lo·∫°i b·ªï tr·ª£ (s·ª≠ d·ª•ng logic map_to_complement_key)."""
        # S·ª≠ d·ª•ng c√πng logic mapping ƒë·ªÉ t√¨m s·∫£n ph·∫©m
        matching_products = []
        
        for idx, row in df.iterrows():
            product_complement_key = map_to_complement_key(row)
            if product_complement_key == complement_type:
                matching_products.append(idx)
        
        if matching_products:
            return df.loc[matching_products]
        
        # D·ª± ph√≤ng: th·ª≠ kh·ªõp tr·ª±c ti·∫øp
        exact_match = df[df['articleType'].astype(str).str.strip() == complement_type]
        if not exact_match.empty:
            return exact_match
        
        # D·ª± ph√≤ng: kh·ªõp kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng
        article_lower = complement_type.lower()
        mask = df['articleType'].astype(str).str.lower().str.strip() == article_lower
        
        return df[mask]

    # X√¢y d·ª±ng c√°c nh√≥m ·ª©ng vi√™n cho m·ªói lo·∫°i t∆∞∆°ng th√≠ch
    def build_candidate_pool(complement_type: str, df: pd.DataFrame) -> List[str]:
        """X√¢y d·ª±ng danh s√°ch ·ª©ng vi√™n ƒë√£ s·∫Øp x·∫øp cho m·ªôt lo·∫°i b·ªï tr·ª£."""
        type_df = get_products_by_complement_type(complement_type, df)
        if type_df.empty:
            return []
        
        ids = type_df.index.astype(str)
        scores = [get_product_score(pid) for pid in ids]
        ordered = sorted(zip(ids, scores), key=lambda x: (-x[1], x[0]))
        return [pid for pid, _ in ordered]

    # X√¢y d·ª±ng c√°c nh√≥m ·ª©ng vi√™n v·ªõi c√°c chi·∫øn l∆∞·ª£c l·ªçc kh√°c nhau
    candidates_gender = {}
    candidates_user_gender = {}
    candidates_unisex = {}
    candidates_any = {}

    for comp_type in compatible_types:
        candidates_gender[comp_type] = build_candidate_pool(comp_type, gender_filtered)
        candidates_user_gender[comp_type] = build_candidate_pool(comp_type, user_gender_filtered)
        candidates_unisex[comp_type] = build_candidate_pool(comp_type, unisex_filtered)
        candidates_any[comp_type] = build_candidate_pool(comp_type, products_df)

    # C≈©ng bao g·ªìm Shoes v√† Bag v√¨ ch√∫ng l√† c√°c b·ªï tr·ª£ ph·ªï bi·∫øn
    if 'Shoes' not in compatible_types:
        compatible_types.append('Shoes')
        candidates_gender['Shoes'] = build_candidate_pool('Shoes', gender_filtered)
        candidates_user_gender['Shoes'] = build_candidate_pool('Shoes', user_gender_filtered)
        candidates_unisex['Shoes'] = build_candidate_pool('Shoes', unisex_filtered)
        candidates_any['Shoes'] = build_candidate_pool('Shoes', products_df)

    # Handbags ƒë√£ ƒë∆∞·ª£c bao g·ªìm trong t·ª´ ƒëi·ªÉn b·ªï tr·ª£ cho Dresses
    # Kh√¥ng c·∫ßn x·ª≠ l√Ω ri√™ng

    outfits = []
    category_offsets = defaultdict(int)

    def pick_candidate(comp_type: str, used: set) -> Optional[str]:
        """Ch·ªçn m·ªôt s·∫£n ph·∫©m ·ª©ng vi√™n cho m·ªôt lo·∫°i b·ªï tr·ª£."""
        is_payload_unisex = str(target_gender).strip().lower() == 'unisex'
        # Strict gender compatibility: only use gender-matched or unisex items
        if is_payload_unisex:
            pools = [
                ('gender', candidates_gender.get(comp_type, [])),
                ('unisex', candidates_unisex.get(comp_type, [])),
            ]
        else:
            # For gendered payloads: try exact gender match first, then unisex
            pools = [
                ('gender', candidates_gender.get(comp_type, [])),
                ('unisex', candidates_unisex.get(comp_type, [])),
            ]
        
        for pool_key, pool in pools:
            if not pool:
                continue
            offset_key = f"{comp_type}:{pool_key}"
            start = category_offsets[offset_key]
            for shift in range(len(pool)):
                idx = (start + shift) % len(pool)
                pid = pool[idx]
                if pid in used or pid == str(payload_product_id):
                    continue
                # Verify product matches the required complement type
                product_row = get_product_record(pid, products_df)
                if product_row is not None:
                    # Check if product's articleType maps to the required comp_type
                    product_comp_key = map_to_complement_key(product_row)
                    if product_comp_key == comp_type:
                        category_offsets[offset_key] = idx + 1
                        return pid
        return None

    # X√¢y d·ª±ng outfits s·ª≠ d·ª•ng c√°c m·ªëi quan h·ªá b·ªï tr·ª£ (complement rules)
    for outfit_idx in range(max_outfits):
        used = {str(payload_product_id)}
        ordered_products = [str(payload_product_id)]
        
        # Try multiple rules until we find a complete outfit
        if complement_rules:
            # Minimum items required (payload + at least 3 complementary items = 4 total)
            min_items = 4 
            best_partial_products = [str(payload_product_id)]
            
            # Try each rule in order, starting from outfit_idx
            for rule_offset in range(len(complement_rules)):
                rule_idx = (outfit_idx + rule_offset) % len(complement_rules)
                selected_rule = complement_rules[rule_idx]
                
                # Reset for this rule attempt
                temp_used = {str(payload_product_id)}
                temp_products = [str(payload_product_id)]
                
                # Try to fill each position in the rule
                for comp_type in selected_rule:
                    if len(temp_products) >= 5:  # Limit outfit size
                        break
                    candidate = pick_candidate(comp_type, temp_used)
                    if candidate:
                        temp_used.add(candidate)
                        temp_products.append(candidate)
                
                # If this rule gave us enough items, use it
                if len(temp_products) >= min_items:
                    used = temp_used
                    ordered_products = temp_products
                    best_partial_products = temp_products
                    break
                
                # Keep track of the best partial outfit found so far
                if len(temp_products) > len(best_partial_products):
                    best_partial_products = temp_products
            
            # If still not enough items after trying all rules, use the best partial one
            if len(ordered_products) < min_items:
                ordered_products = best_partial_products
                used = set(ordered_products)

        # T√≠nh ƒëi·ªÉm outfit d·ª±a tr√™n t∆∞∆°ng th√≠ch b·ªï tr·ª£
        base_score = sum(get_product_score(pid) for pid in ordered_products)
        
        # ƒêi·ªÉm th∆∞·ªüng cho t∆∞∆°ng th√≠ch b·ªï tr·ª£
        complement_bonus = 0.0
        for pid in ordered_products[1:]:  # B·ªè qua payload
            product_row = get_product_record(pid, products_df)
            if product_row is not None and is_compatible_with_payload(product_row):
                complement_bonus += 0.1
        
        final_score = base_score + complement_bonus
        
        if len(ordered_products) > 1:  # √çt nh·∫•t payload + 1 item
            outfits.append({
                'products': ordered_products,
                'score': final_score
            })

    return outfits
def compute_sparsity(df: pd.DataFrame) -> pd.Series:
    if df.empty:
        return pd.Series(dtype=float)
    non_null_counts = df.count()
    sparsity = 1 - (non_null_counts / len(df))
    return sparsity.sort_values(ascending=False)

def render_sparsity_chart(df: pd.DataFrame, title: str, key: str):
    sparsity = compute_sparsity(df)
    if sparsity.empty:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh ƒë·ªô th∆∞a.")
        return
    sparsity_df = sparsity.reset_index()
    sparsity_df.columns = ['Column', 'Sparsity']
    fig = px.bar(
        sparsity_df,
        x='Column',
        y='Sparsity',
        title=title,
        labels={'Column': 'C·ªôt', 'Sparsity': 'ƒê·ªô th∆∞a (t·ªâ l·ªá null)'}
    )
    st.plotly_chart(fig, use_container_width=True)

def render_distribution_chart(df: pd.DataFrame, dataset_key: str):
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    available_cols = categorical_cols + numeric_cols
    if not available_cols:
        st.info("Kh√¥ng c√≥ c·ªôt ph√π h·ª£p ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì t·ªâ l·ªá.")
        return
    selected_col = st.selectbox(
        "Ch·ªçn c·ªôt ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì t·ªâ l·ªá",
        available_cols,
        key=f"{dataset_key}_distribution_column"
    )
    if selected_col in categorical_cols:
        value_counts = df[selected_col].fillna("N/A").value_counts().head(10)
        fig = px.pie(
            values=value_counts.values,
            names=value_counts.index,
            title=f"T·ªâ l·ªá ph√¢n b·ªë c·ªßa '{selected_col}'"
        )
    else:
        numeric_series = df[selected_col].dropna()
        if numeric_series.empty:
            st.info("C·ªôt ƒë√£ ch·ªçn kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì.")
            return
        hist_data = pd.cut(numeric_series, bins=10).value_counts().sort_index()
        hist_df = hist_data.reset_index()
        hist_df.columns = ['Range', 'Count']
        hist_df['Range'] = hist_df['Range'].astype(str)
        fig = px.bar(
            hist_df,
            x='Range',
            y='Count',
            title=f"Ph√¢n b·ªë gi√° tr·ªã c·ªßa '{selected_col}'",
            labels={'Range': 'Kho·∫£ng gi√° tr·ªã', 'Count': 'S·ªë l∆∞·ª£ng'}
        )
    st.plotly_chart(fig, use_container_width=True)

def render_data_statistics(df: pd.DataFrame):
    if df.empty:
        st.info("Dataset tr·ªëng, kh√¥ng th·ªÉ th·ªëng k√™.")
        return
    numeric_df = df.select_dtypes(include=[np.number])
    if numeric_df.empty:
        st.info("Kh√¥ng c√≥ c·ªôt s·ªë ƒë·ªÉ th·ªëng k√™.")
        return
    stats_df = numeric_df.describe().T
    st.dataframe(stats_df, use_container_width=True)

def render_dataset_upload_section(
    dataset_key: str,
    display_name: str,
    purpose_text: str
):
    st.markdown(f"#### {display_name}")
    st.write(purpose_text)
    uploaded_file = st.file_uploader(
        f"T·∫£i l√™n {display_name}",
        type=['csv'],
        key=f"{dataset_key}_file_uploader"
    )
    if uploaded_file is None:
        st.info("Ch∆∞a c√≥ file ƒë∆∞·ª£c t·∫£i l√™n.")
        return
    try:
        df = pd.read_csv(uploaded_file)
    except Exception as exc:
        st.error(f"L·ªói khi ƒë·ªçc file CSV: {exc}")
        return
    st.success(f"ƒê√£ t·∫£i {display_name}: {len(df)} rows √ó {len(df.columns)} columns")
    col_rows, col_cols = st.columns(2)
    with col_rows:
        st.metric("S·ªë d√≤ng (rows)", len(df))
    with col_cols:
        st.metric("S·ªë c·ªôt (columns)", len(df.columns))
    st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
    st.dataframe(df.head(100), use_container_width=True)
    st.markdown("**üìâ Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (t·ªâ l·ªá gi√° tr·ªã null tr√™n m·ªói c·ªôt):**")
    render_sparsity_chart(df, f"ƒê·ªô th∆∞a - {display_name}", dataset_key)
    st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
    render_distribution_chart(df, dataset_key)
    st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu (count, mean, std, min, 25%, 50%, 75%, max):**")
    render_data_statistics(df)

def display_product_info(product_info: Dict, score: float = None):
    col1, col2 = st.columns([1, 3])

    with col1:
        if score is not None:
            st.metric("Score", f"{score:.4f}")
        image_url = extract_primary_image_url(product_info)
        if image_url:
            st.image(
                image_url,
                caption=product_info.get('productDisplayName', 'Product image'),
                use_container_width=True
            )

    with col2:
        st.markdown(f"**{product_info.get('productDisplayName', 'N/A')}**")
        st.write(
            f"üè∑Ô∏è **Category**: "
            f"{product_info.get('masterCategory', 'N/A')} > "
            f"{product_info.get('subCategory', 'N/A')} > "
            f"{product_info.get('articleType', 'N/A')}"
        )
        st.write(f"üë§ **Gender**: {product_info.get('gender', 'N/A')}")
        st.write(f"üß© **Usage**: {product_info.get('usage', 'N/A')}")
        st.write(f"üé® **Color**: {product_info.get('baseColour', 'N/A')}")


def extract_primary_image_url(product_info: Dict) -> Optional[str]:
    """Tr·∫£ v·ªÅ URL h√¨nh ·∫£nh h·ª£p l·ªá ƒë·∫ßu ti√™n t·ª´ b·∫£n ghi s·∫£n ph·∫©m n·∫øu c√≥ s·∫µn."""
    if not product_info:
        return None

    images_field = product_info.get('images')
    if images_field is None or (isinstance(images_field, float) and pd.isna(images_field)):
        return None

    if isinstance(images_field, list) and images_field:
        return images_field[0]

    if isinstance(images_field, str):
        stripped = images_field.strip()
        if stripped.startswith('['):
            try:
                parsed = ast.literal_eval(stripped)
                if isinstance(parsed, (list, tuple)) and parsed:
                    return parsed[0]
            except (ValueError, SyntaxError):
                pass
        if stripped.startswith('http'):
            return stripped

    return None

def render_metrics_table(df, highlight_model=None):
    if df is None:
        st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y t√≠nh to√°n tr∆∞·ªõc.")
        return

    st.markdown("### üìä B·∫£ng T·ªïng H·ª£p Ch·ªâ S·ªë C√°c M√¥ H√¨nh")
    
    required_cols = ['model_name', 'recall@10', 'recall@20', 'ndcg@10', 'ndcg@20', 
                     'precision@10', 'precision@20', 'training_time', 'avg_inference_time',
                     'coverage@10', 'diversity@10']
    
    display_df = df.copy()
    available_cols = [col for col in required_cols if col in display_df.columns]
    display_df = display_df[available_cols]
    
    column_mapping = {
        'model_name': 'Model',
        'recall@10': 'Recall@10',
        'recall@20': 'Recall@20',
        'ndcg@10': 'NDCG@10',
        'ndcg@20': 'NDCG@20',
        'precision@10': 'Precision@10',
        'precision@20': 'Precision@20',
        'training_time': 'Training Time (s)',
        'avg_inference_time': 'Inference Time (s)',
        'coverage@10': 'Coverage@10',
        'diversity@10': 'Diversity@10'
    }
    display_df = display_df.rename(columns=column_mapping)
    
    numeric_cols = display_df.select_dtypes(include=[np.number]).columns
    display_df[numeric_cols] = display_df[numeric_cols].round(4)
    
    def highlight_row(row):
        model_name = row.get('Model', '')
        if model_name == highlight_model:
            return ['background-color: #e6ffe6'] * len(row)
        return [''] * len(row)

    st.dataframe(display_df.style.apply(highlight_row, axis=1), use_container_width=True)

def slugify_model_name(model_name: str) -> str:
    return re.sub(r'[^a-z0-9]+', '_', model_name.lower()).strip('_')

def apply_5core_pruning(interactions_df: pd.DataFrame, min_interactions: int = 2) -> Dict:

    if interactions_df.empty:
        return {
            'pruned_interactions': pd.DataFrame(),
            'removed_users': 0,
            'removed_products': 0,
            'iterations': 0,
            'stats': []
        }

    df = interactions_df.copy()

    if 'user_id' not in df.columns or 'product_id' not in df.columns:
        raise ValueError("DataFrame ph·∫£i c√≥ columns 'user_id' v√† 'product_id'")

    original_users = df['user_id'].nunique()
    original_products = df['product_id'].nunique()
    original_interactions = len(df)

    stats = [{
        'iteration': 0,
        'users': original_users,
        'products': original_products,
        'interactions': original_interactions,
        'removed_users': 0,
        'removed_products': 0
    }]

    iteration = 0
    changed = True

    while changed:
        iteration += 1
        changed = False

        user_counts = df['user_id'].value_counts()
        users_to_keep = user_counts[user_counts >= min_interactions].index

        product_counts = df['product_id'].value_counts()
        products_to_keep = product_counts[product_counts >= min_interactions].index

        before_len = len(df)
        df = df[df['user_id'].isin(users_to_keep) & df['product_id'].isin(products_to_keep)]
        after_len = len(df)

        if before_len != after_len:
            changed = True

        removed_users = original_users - df['user_id'].nunique()
        removed_products = original_products - df['product_id'].nunique()

        stats.append({
            'iteration': iteration,
            'users': df['user_id'].nunique(),
            'products': df['product_id'].nunique(),
            'interactions': len(df),
            'removed_users': removed_users,
            'removed_products': removed_products
        })

        if iteration >= 100:
            break

    total_removed_users = original_users - df['user_id'].nunique()
    total_removed_products = original_products - df['product_id'].nunique()

    return {
        'pruned_interactions': df,
        'removed_users': total_removed_users,
        'removed_products': total_removed_products,
        'iterations': iteration,
        'stats': stats,
        'original_users': original_users,
        'original_products': original_products,
        'original_interactions': original_interactions
    }

def apply_feature_encoding(products_df: pd.DataFrame, features: List[str] = None) -> Dict:

    if products_df.empty:
        return {
            'encoded_matrix': np.array([]),
            'feature_mapping': {},
            'feature_dims': {},
            'total_dims': 0,
            'feature_names': []
        }

    if features is None:
        features = ['masterCategory', 'subCategory', 'articleType', 'baseColour', 'usage']

    available_features = [f for f in features if f in products_df.columns]

    if not available_features:
        return {
            'encoded_matrix': np.array([]),
            'feature_mapping': {},
            'feature_dims': {},
            'total_dims': 0,
            'feature_names': []
        }

    feature_mapping = {}
    feature_dims = {}
    encoded_parts = []
    feature_names = []
    start_idx = 0

    for feat in available_features:
        unique_values = sorted(products_df[feat].dropna().unique())
        n_values = len(unique_values)

        value_to_idx = {val: idx for idx, val in enumerate(unique_values)}
        feature_mapping[feat] = {
            'value_to_idx': value_to_idx,
            'idx_to_value': {idx: val for val, idx in value_to_idx.items()},
            'start_idx': start_idx,
            'end_idx': start_idx + n_values
        }

        one_hot = np.zeros((len(products_df), n_values))
        for i, val in enumerate(products_df[feat]):
            if pd.notna(val) and val in value_to_idx:
                one_hot[i, value_to_idx[val]] = 1

        encoded_parts.append(one_hot)
        feature_dims[feat] = n_values

        for val in unique_values:
            feature_names.append(f"{feat}_{val}")

        start_idx += n_values

    if encoded_parts:
        encoded_matrix = np.hstack(encoded_parts)
    else:
        encoded_matrix = np.array([])

    return {
        'encoded_matrix': encoded_matrix,
        'feature_mapping': feature_mapping,
        'feature_dims': feature_dims,
        'total_dims': encoded_matrix.shape[1] if len(encoded_matrix.shape) > 1 else 0,
        'feature_names': feature_names,
        'product_ids': products_df.index.tolist() if hasattr(products_df.index, 'tolist') else list(range(len(products_df)))
    }

def load_evaluation_log(model_name: str):
    slug = slugify_model_name(model_name)
    log_path = os.path.join('recommendation_system', 'evaluation', 'logs', f'{slug}.log')
    if os.path.exists(log_path):
        with open(log_path, 'r', encoding='utf-8') as f:
            return slug, f.read()
    return slug, None

def parse_evaluation_log(log_text: str) -> Dict:

    if not log_text:
        return {'metrics': {}, 'examples': {}, 'formulas': {}}
    
    metrics = {}
    examples = {}
    formulas = {}
    
    lines = log_text.split('\n')
    i = 0
    current_metric = None
    
    while i < len(lines):
        line = lines[i].strip()
        
        if not line or line.startswith('===') or line.startswith('[') or 'EVALUATING' in line or 'RESULTS FOR' in line:
            i += 1
            continue
        
        if ':' in line and not line.startswith('üìê') and not line.startswith('üßÆ'):
            parts = line.split(':', 1)
            if len(parts) == 2:
                metric_name = parts[0].strip()
                value_str = parts[1].strip()
                
                value_str = value_str.split()[0] if value_str.split() else value_str
                
                try:
                    value = float(value_str)
                    metrics[metric_name] = value
                    current_metric = metric_name
                except ValueError:
                    pass
        
        if 'üìê C√¥ng th·ª©c:' in line:
            formula = line.split('üìê C√¥ng th·ª©c:', 1)[1].strip()
            if current_metric:
                formulas[current_metric] = formula
        
        if 'V√≠ d·ª• √°p d·ª•ng:' in line:
            example = line.split('V√≠ d·ª• √°p d·ª•ng:', 1)[1].strip()
            if current_metric:
                examples[current_metric] = example
        
        i += 1
    
    return {
        'metrics': metrics,
        'examples': examples,
        'formulas': formulas
    }

def render_metrics_in_step(
    metrics_data,
    metric_keys: List[str],
    step_title: str,
    key_suffix: str,
    model_name: str = None
):

    if metrics_data is None:
        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y train & evaluate tr∆∞·ªõc.")
        return
    elif isinstance(metrics_data, pd.Series):
        if metrics_data.empty:
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y train & evaluate tr∆∞·ªõc.")
            return
    elif isinstance(metrics_data, dict):
        if not metrics_data or (isinstance(metrics_data, dict) and 'metrics' in metrics_data and not metrics_data['metrics']):
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y train & evaluate tr∆∞·ªõc.")
            return
    
    parsed_log = None
    if model_name:
        _, log_text = load_evaluation_log(model_name)
        if log_text:
            parsed_log = parse_evaluation_log(log_text)
    
    n_cols = 2
    cols = st.columns(n_cols)
    
    for idx, metric_key in enumerate(metric_keys):
        col_idx = idx % n_cols
        with cols[col_idx]:
            value = None
            formula = ''
            example = ''
            
            if isinstance(metrics_data, dict) and 'metrics' in metrics_data:
                value = metrics_data['metrics'].get(metric_key, None)
                formula = metrics_data['formulas'].get(metric_key, '')
                example = metrics_data['examples'].get(metric_key, '')
            elif isinstance(metrics_data, pd.Series):
                value = metrics_data.get(metric_key, None)
                if parsed_log:
                    formula = parsed_log['formulas'].get(metric_key, '')
                    example = parsed_log['examples'].get(metric_key, '')
            
            if value is not None:
                display_name = metric_key.replace('@', '@').replace('_', ' ').title()
                
                st.metric(display_name, f"{value:.4f}")
                
                with st.expander(f"Chi ti·∫øt {display_name}", expanded=False):
                    if formula:
                        st.markdown(f"**C√¥ng th·ª©c:** {formula}")
                    
                    if example:
                        if "| Trung b√¨nh" in example:
                            parts = example.split(" | ")
                            user_examples = []
                            avg_formula = None
                            
                            for part in parts:
                                if "Trung b√¨nh" in part:
                                    avg_formula = part
                                else:
                                    user_examples.append(part)
                            
                            st.markdown("#### V√≠ d·ª• t√≠nh to√°n cho t·ª´ng user:")
                            for i, user_ex in enumerate(user_examples, 1):
                                st.markdown(f"**{i}. {user_ex}**")
                            
                            if avg_formula:
                                st.markdown("#### C√¥ng th·ª©c t√≠nh trung b√¨nh:")
                                
                                if "=" in avg_formula:
                                    formula_parts = avg_formula.split("=")
                                    if len(formula_parts) >= 2:
                                        left_side = formula_parts[0].strip()
                                        right_side = "=".join(formula_parts[1:]).strip()
                                        
                                        import re
                                        n_users_match = re.search(r'user(\\d+)', right_side)
                                        n_users = n_users_match.group(1) if n_users_match else "N"
                                        
                                        metric_var = display_name.replace(" ", "_").lower()
                                        
                                        st.markdown(f"""
                                        **C√¥ng th·ª©c:**
                                        $$\\text{{Trung b√¨nh}} = \\frac{{\\sum_{{u=1}}^{{{n_users}}} {display_name}_u}}{{{n_users}}}$$

                                        **D·∫°ng m·ªü r·ªông:**
                                        $$\\text{{Trung b√¨nh}} = \\frac{{{display_name}_{{user1}} + {display_name}_{{user2}} + \\ldots + {display_name}_{{user{n_users}}}}}{{{n_users}}}$$
                                        """)

    slug, log_text = load_evaluation_log(model_name)
    with st.expander("üìú Evaluation Log (Raw)", expanded=False):
        if log_text:
            st.text_area(
                "Chi ti·∫øt log t√≠nh to√°n",
                log_text,
                height=320,
                key=f"log_text_{key_suffix}"
            )
            st.download_button(
                "‚¨áÔ∏è T·∫£i log",
                log_text,
                file_name=f"{slug}.log",
                mime="text/plain",
                key=f"log_download_{key_suffix}"
            )
        else:
            st.info("Ch∆∞a c√≥ log evaluation. H√£y ch·∫°y train & evaluate ƒë·ªÉ t·∫°o log.")

def run_training(model_type: str):
    import io
    from contextlib import redirect_stdout
    
    model_names = {
        "all": "T·∫•t C·∫£ Models",
        "content_based": "Content-Based Filtering",
        "gnn": "GNN",
        "hybrid": "Hybrid (GNN + Content-Based)"
    }
    
    model_name = model_names.get(model_type, model_type)
    
    with st.status(f"ƒêang train {model_name}...", expanded=True) as status:
        st.write(f"üöÄ B·∫Øt ƒë·∫ßu training {model_name}...")
        try:
            f = io.StringIO()
            with redirect_stdout(f):
                if model_type == "all":
                    train_recommendation.train_and_evaluate()
                elif model_type == "content_based":
                    train_recommendation.train_content_based(evaluate=True)
                elif model_type == "gnn":
                    train_recommendation.train_gnn(evaluate=True)
                elif model_type == "hybrid":
                    train_recommendation.train_hybrid(evaluate=True)
                else:
                    raise ValueError(f"Unknown model type: {model_type}")
            
            output_log = f.getvalue()
            st.text_area("Logs", output_log, height=300)
            
            st.cache_resource.clear()
            st.cache_data.clear()
            
            restore_all_artifacts()
            
            preprocessor, cb_model, gnn_model, hybrid_model = load_models()
            comparison_df = load_comparison_results()
            
            status.update(label=f"‚úÖ Ho√†n th√†nh training {model_name}!", state="complete", expanded=False)
            st.success(f"‚úÖ ƒê√£ ho√†n th√†nh training {model_name} v√† c·∫≠p nh·∫≠t s·ªë li·ªáu!")
        except Exception as e:
            status.update(label=f"‚ùå L·ªói khi train {model_name}", state="error")
            st.error(f"L·ªói: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

def main():
    
    st.markdown('<div class="main-header">üëî Fashion Recommendation System</div>', unsafe_allow_html=True)
    
    st.sidebar.title("‚öôÔ∏è Menu")
    page = st.sidebar.radio(
        "Ch·ªçn ch·ª©c nƒÉng",
        ["üìö Algorithms & Steps", "üëó Recommendations"]
    )
    
    load_cached_predictions_into_session()
    restore_all_artifacts()

    preprocessor, cb_model, gnn_model, hybrid_model = load_models()
    comparison_df = load_comparison_results()

    if page == "üìö Algorithms & Steps":
        st.markdown("## üìö Algorithms & Steps")
        st.markdown('<div class="sub-header">üìö PH·∫¶N I: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU & T·∫†O T·∫¨P D·ªÆ LI·ªÜU CHUNG (D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO)</div>', unsafe_allow_html=True)
        st.markdown("")
        with st.expander("B∆∞·ªõc 1.1: Xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB th√†nh CSV", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (products, users, interactions) th√†nh c√°c file CSV ƒë·ªÉ s·ª≠ d·ª•ng cho training v√† evaluation.")
            
            if export_all_data is None:
                st.error(f"‚ùå Kh√¥ng th·ªÉ import export_data module: {_export_import_error}")
                st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/export_data.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
            else:
                export_button_clicked = st.button("üì• Xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB", type="primary", use_container_width=True)
                
                if export_button_clicked:
                    with st.spinner("ƒêang xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB..."):
                        try:
                            result = export_all_data()
                            
                            if result['success']:
                                st.success(f"‚úÖ {result['message']}")

                                st.markdown("### üìä K·∫øt qu·∫£ xu·∫•t d·ªØ li·ªáu:")
                                col_res1, col_res2, col_res3 = st.columns(3)
                                
                                with col_res1:
                                    products_result = result['results']['products']
                                    if products_result['success']:
                                        st.success(f"‚úÖ Products: {products_result['count']} records")
                                    else:
                                        st.error(f"‚ùå Products: {products_result.get('error', 'L·ªói')}")
                                
                                with col_res2:
                                    users_result = result['results']['users']
                                    if users_result['success']:
                                        st.success(f"‚úÖ Users: {users_result['count']} records")
                                    else:
                                        st.error(f"‚ùå Users: {users_result.get('error', 'L·ªói')}")
                                
                                with col_res3:
                                    interactions_result = result['results']['interactions']
                                    if interactions_result['success']:
                                        st.success(f"‚úÖ Interactions: {interactions_result['count']} records")
                                    else:
                                        st.error(f"‚ùå Interactions: {interactions_result.get('error', 'L·ªói')}")
                                st.markdown("### üìÅ Xem chi ti·∫øt d·ªØ li·ªáu ƒë√£ xu·∫•t:")

                                export_dir = ensure_export_directory()
                                
                                tab1, tab2, tab3 = st.tabs(["üì¶ Products Data", "üë• Users Data", "üîó Interactions Data"])
                                
                                with tab1:
                                    products_path = export_dir / 'products.csv'
                                    if products_path.exists() and products_result['success']:
                                        st.markdown("#### üì¶ Products Data:")
                                        try:
                                            products_df = pd.read_csv(products_path)
                                            col_p1, col_p2 = st.columns(2)
                                            with col_p1:
                                                st.metric("S·ªë d√≤ng (rows)", len(products_df))
                                            with col_p2:
                                                st.metric("S·ªë c·ªôt (columns)", len(products_df.columns))
                                            
                                            st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
                                            st.dataframe(products_df.head(100), use_container_width=True)
                                            
                                            st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
                                            render_distribution_chart(products_df, "products_export")
                                            
                                            st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu:**")
                                            render_data_statistics(products_df)
                                        except Exception as e:
                                            st.error(f"L·ªói khi ƒë·ªçc products.csv: {str(e)}")
                                    else:
                                        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu Products ƒë·ªÉ hi·ªÉn th·ªã.")
                                
                                with tab2:
                                    users_path = export_dir / 'users.csv'
                                    if users_path.exists() and users_result['success']:
                                        st.markdown("#### üë• Users Data:")
                                        try:
                                            users_df = pd.read_csv(users_path)
                                            col_u1, col_u2 = st.columns(2)
                                            with col_u1:
                                                st.metric("S·ªë d√≤ng (rows)", len(users_df))
                                            with col_u2:
                                                st.metric("S·ªë c·ªôt (columns)", len(users_df.columns))
                                            
                                            st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
                                            st.dataframe(users_df.head(100), use_container_width=True)
                                            
                                            st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
                                            render_distribution_chart(users_df, "users_export")
                                            
                                            st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu:**")
                                            render_data_statistics(users_df)
                                        except Exception as e:
                                            st.error(f"L·ªói khi ƒë·ªçc users.csv: {str(e)}")
                                    else:
                                        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu Users ƒë·ªÉ hi·ªÉn th·ªã.")
                                
                                with tab3:
                                    interactions_path = export_dir / 'interactions.csv'
                                    if interactions_path.exists() and interactions_result['success']:
                                        st.markdown("#### üîó Interactions Data:")
                                        try:
                                            interactions_df = pd.read_csv(interactions_path)
                                            col_i1, col_i2 = st.columns(2)
                                            with col_i1:
                                                st.metric("S·ªë d√≤ng (rows)", len(interactions_df))
                                            with col_i2:
                                                st.metric("S·ªë c·ªôt (columns)", len(interactions_df.columns))
                                            
                                            st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
                                            st.dataframe(interactions_df.head(100), use_container_width=True)
                                            
                                            st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
                                            render_distribution_chart(interactions_df, "interactions_export")
                                            
                                            st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu:**")
                                            render_data_statistics(interactions_df)
                                        except Exception as e:
                                            st.error(f"L·ªói khi ƒë·ªçc interactions.csv: {str(e)}")
                                    else:
                                        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu Interactions ƒë·ªÉ hi·ªÉn th·ªã.")
                                
                                st.session_state['exported_data'] = {
                                    'products_path': str(products_path) if products_path.exists() else None,
                                    'users_path': str(users_path) if users_path.exists() else None,
                                    'interactions_path': str(interactions_path) if interactions_path.exists() else None,
                                    'export_dir': str(export_dir)
                                }
                                
                            else:
                                st.error(f"‚ùå C√≥ l·ªói x·∫£y ra khi xu·∫•t d·ªØ li·ªáu")
                                for key, res in result['results'].items():
                                    if not res['success']:
                                        st.error(f"‚ùå {key}: {res.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}")
                        
                        except Exception as e:
                            st.error(f"‚ùå L·ªói: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                
                export_dir = ensure_export_directory() if ensure_export_directory else None
                if export_dir:
                    st.info(f"üí° **L∆∞u √Ω:** C√°c file CSV s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: `{export_dir}`")
        
        with st.expander("B∆∞·ªõc 1.2: L√†m s·∫°ch v√† L·ªçc D·ªØ li·ªáu (Pruning & Sparsity Handling)", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** √Åp d·ª•ng k·ªπ thu·∫≠t k-Core Pruning ƒë·ªÉ lo·∫°i b·ªè ƒë·ªá quy c√°c ng∆∞·ªùi d√πng v√† s·∫£n ph·∫©m c√≥ d∆∞·ªõi s·ªë l∆∞·ª£ng t∆∞∆°ng t√°c t·ªëi thi·ªÉu (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh) nh·∫±m gi·∫£m ƒë·ªô th∆∞a th·ªõt c·ªßa d·ªØ li·ªáu.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** `interactions.csv`")
            
            # T·∫°o c√°c tab: Hi·ªán th·ª±c (tr√°i) v√† Thu·∫≠t to√°n (ph·∫£i)
            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # L·ª±a ch·ªçn ngu·ªìn d·ªØ li·ªáu
                col_source1, col_source2 = st.columns([2, 1])
                with col_source1:
                    use_exported = st.checkbox(
                        "S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ xu·∫•t t·ª´ MongoDB (B∆∞·ªõc 1.1)",
                        value=True,
                        key="pruning_use_exported"
                    )
                
                interactions_df = None
                
                if use_exported and 'exported_data' in st.session_state and st.session_state['exported_data'].get('interactions_path'):
                    interactions_path = st.session_state['exported_data']['interactions_path']
                    if os.path.exists(interactions_path):
                        try:
                            interactions_df = pd.read_csv(interactions_path)
                            st.success(f"‚úÖ ƒê√£ t·∫£i interactions.csv t·ª´ d·ªØ li·ªáu ƒë√£ xu·∫•t: {len(interactions_df)} rows")
                        except Exception as e:
                            st.error(f"L·ªói khi ƒë·ªçc file: {str(e)}")
                    else:
                        st.warning("File interactions.csv kh√¥ng t·ªìn t·∫°i. Vui l√≤ng t·∫£i l√™n file ho·∫∑c xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB.")
                
                if interactions_df is None:
                    # Auto import t·ª´ apps/exports
                    export_dir = ensure_export_directory() if ensure_export_directory else None
                    if export_dir:
                        interactions_path_auto = export_dir / 'interactions.csv'
                        if interactions_path_auto.exists():
                            try:
                                interactions_df = pd.read_csv(interactions_path_auto)
                                st.success(f"‚úÖ ƒê√£ t·ª± ƒë·ªông t·∫£i interactions.csv t·ª´ apps/exports: {len(interactions_df)} rows √ó {len(interactions_df.columns)} columns")
                            except Exception as e:
                                st.error(f"L·ªói khi ƒë·ªçc file t·ª´ apps/exports: {str(e)}")
                        else:
                            st.info("üí° File interactions.csv kh√¥ng t·ªìn t·∫°i trong apps/exports. Vui l√≤ng xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (B∆∞·ªõc 1.1) ho·∫∑c ƒë·∫£m b·∫£o file t·ªìn t·∫°i.")
                    else:
                        st.info("üí° Kh√¥ng th·ªÉ truy c·∫≠p th∆∞ m·ª•c apps/exports. Vui l√≤ng xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (B∆∞·ªõc 1.1).")
                
                if interactions_df is not None:
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        min_interactions = st.number_input(
                            "S·ªë l∆∞·ª£ng t∆∞∆°ng t√°c t·ªëi thi·ªÉu (min_interactions)",
                            min_value=1,
                            value=2,
                            step=1,
                            key="pruning_min_interactions"
                        )
                    
                    with col_config2:
                        st.write("")  # Kho·∫£ng tr·ªëng
                        process_button = st.button(
                            f"üîß √Åp d·ª•ng {min_interactions}-Core Pruning",
                            type="primary",
                            use_container_width=True,
                            key="pruning_process_button"
                        )
                    
                    if process_button:
                        with st.spinner(f"ƒêang √°p d·ª•ng {min_interactions}-Core Pruning..."):
                            try:
                                result = apply_5core_pruning(interactions_df, min_interactions)
                                
                                if result['pruned_interactions'].empty:
                                    st.error("‚ùå **K·∫øt qu·∫£:** T·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ b·ªã lo·∫°i b·ªè!")
                                    st.warning(f"""
                            **Nguy√™n nh√¢n:**
                                    - V·ªõi min_interactions = {min_interactions}, t·∫•t c·∫£ users v√†/ho·∫∑c products ƒë·ªÅu c√≥ √≠t h∆°n {min_interactions} interactions
                            - ƒêi·ªÅu n√†y t·∫°o ra hi·ªáu ·ª©ng cascade: khi lo·∫°i b·ªè users/products, c√°c interactions li√™n quan c≈©ng b·ªã lo·∫°i b·ªè, khi·∫øn c√°c users/products kh√°c c≈©ng kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán

                            **Gi·∫£i ph√°p:**
                            1. Gi·∫£m min_interactions xu·ªëng (v√≠ d·ª•: {max(1, min_interactions - 1)} ho·∫∑c {max(1, min_interactions - 2)})
                            2. Thu th·∫≠p th√™m d·ªØ li·ªáu interactions
                            3. Ch·∫•p nh·∫≠n d·ªØ li·ªáu th∆∞a th·ªõt v√† kh√¥ng √°p d·ª•ng pruning
                                    """)
                                else:
                                    st.success("‚úÖ **Ho√†n th√†nh!** Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch.")
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['pruned_interactions'] = result
                                    # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                    save_intermediate_artifact('pruned_interactions', result)
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    st.markdown("### üìä Th·ªëng k√™ k·∫øt qu·∫£ Pruning")
                                    
                                    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                                    with col_stat1:
                                        st.metric("Users ban ƒë·∫ßu", result['original_users'])
                                        st.metric("Users sau pruning", result['original_users'] - result['removed_users'])
                                    with col_stat2:
                                        st.metric("Products ban ƒë·∫ßu", result['original_products'])
                                        st.metric("Products sau pruning", result['original_products'] - result['removed_products'])
                                    with col_stat3:
                                        st.metric("Interactions ban ƒë·∫ßu", result['original_interactions'])
                                        st.metric("Interactions sau pruning", len(result['pruned_interactions']))
                                    with col_stat4:
                                        st.metric("S·ªë l·∫ßn l·∫∑p", result['iterations'])
                                        reduction_pct = ((result['original_interactions'] - len(result['pruned_interactions'])) / result['original_interactions'] * 100) if result['original_interactions'] > 0 else 0
                                        st.metric("Gi·∫£m ƒëi", f"{reduction_pct:.2f}%")
                                    
                                    # T√≠nh to√°n c√°c gi√° tr·ªã cho c√°c tab
                                    pruned_users = result['original_users'] - result['removed_users']
                                    pruned_products = result['original_products'] - result['removed_products']
                                    
                                    # T·∫°o c√°c tab cho c√°c h√¨nh ·∫£nh h√≥a kh√°c nhau
                                    tab1, tab2, tab3 = st.tabs([
                                        "üìã Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ l√†m s·∫°ch",
                                        "üìà Qu√° tr√¨nh Pruning qua c√°c l·∫ßn l·∫∑p",
                                        "üî• Ma tr·∫≠n t∆∞∆°ng t√°c (Heatmap)"
                                    ])
                                    
                                    with tab1:
                                        st.markdown("### üìã Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ l√†m s·∫°ch $R_{pruned}$")
                                        st.dataframe(
                                            result['pruned_interactions'].head(100),
                                            use_container_width=True
                                        )
                                        
                                    
                                    with tab2:
                                        if result['stats']:
                                            st.markdown("### üìà Qu√° tr√¨nh Pruning qua c√°c l·∫ßn l·∫∑p")
                                            stats_df = pd.DataFrame(result['stats'])
                                            fig = go.Figure()
                                            fig.add_trace(go.Scatter(
                                                x=stats_df['iteration'],
                                                y=stats_df['users'],
                                                mode='lines+markers',
                                                name='Users',
                                                line=dict(color='#1f77b4')
                                            ))
                                            fig.add_trace(go.Scatter(
                                                x=stats_df['iteration'],
                                                y=stats_df['products'],
                                                mode='lines+markers',
                                                name='Products',
                                                line=dict(color='#2ca02c')
                                            ))
                                            fig.add_trace(go.Scatter(
                                                x=stats_df['iteration'],
                                                y=stats_df['interactions'],
                                                mode='lines+markers',
                                                name='Interactions',
                                                line=dict(color='#d62728')
                                            ))
                                            fig.update_layout(
                                                title="Thay ƒë·ªïi s·ªë l∆∞·ª£ng Users, Products v√† Interactions qua c√°c l·∫ßn l·∫∑p",
                                                xaxis_title="S·ªë l·∫ßn l·∫∑p",
                                                yaxis_title="S·ªë l∆∞·ª£ng",
                                                hovermode='x unified'
                                            )
                                            st.plotly_chart(fig, use_container_width=True, key="pruning_stats_chart_new")
                                        else:
                                            st.info("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu th·ªëng k√™ qu√° tr√¨nh pruning.")
                                    
                                    with tab3:
                                        if pruned_users <= 100 and pruned_products <= 100:
                                            st.markdown("### üî• Ma tr·∫≠n t∆∞∆°ng t√°c (Heatmap)")
                                            st.info("‚ÑπÔ∏è Hi·ªÉn th·ªã ma tr·∫≠n t∆∞∆°ng t√°c d∆∞·ªõi d·∫°ng heatmap (1 = c√≥ t∆∞∆°ng t√°c, 0 = kh√¥ng c√≥ t∆∞∆°ng t√°c)")
                                            
                                            # T·∫°o ma tr·∫≠n t∆∞∆°ng t√°c
                                            interaction_matrix = result['pruned_interactions'].pivot_table(
                                                index='user_id',
                                                columns='product_id',
                                                aggfunc='size',
                                                fill_value=0
                                            )
                                            
                                            interaction_matrix = (interaction_matrix > 0).astype(int)
                                            
                                            fig_heatmap = go.Figure(data=go.Heatmap(
                                                z=interaction_matrix.values,
                                                x=interaction_matrix.columns,
                                                y=interaction_matrix.index,
                                                colorscale='YlOrRd',
                                                showscale=True,
                                                colorbar=dict(title="Interaction")
                                            ))
                                            fig_heatmap.update_layout(
                                                title="Ma tr·∫≠n t∆∞∆°ng t√°c User-Product (1 = c√≥ t∆∞∆°ng t√°c, 0 = kh√¥ng c√≥)",
                                                xaxis_title="Product ID",
                                                yaxis_title="User ID",
                                                width=800,
                                                height=600
                                            )
                                            st.plotly_chart(fig_heatmap, use_container_width=True, key="pruning_heatmap_chart_new")
                                        else:
                                            st.info(f"‚ÑπÔ∏è Ma tr·∫≠n qu√° l·ªõn ({pruned_users} users √ó {pruned_products} products) ƒë·ªÉ hi·ªÉn th·ªã heatmap. Ch·ªâ hi·ªÉn th·ªã d·ªØ li·ªáu d·∫°ng b·∫£ng.")
                                            st.markdown("**üí° G·ª£i √Ω:** Xem d·ªØ li·ªáu d·∫°ng b·∫£ng trong tab 'üìã Ma tr·∫≠n t∆∞∆°ng t√°c ƒë√£ l√†m s·∫°ch'")
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ Ma tr·∫≠n t∆∞∆°ng t√°c th∆∞a th·ªõt $R$ ƒë∆∞·ª£c l√†m s·∫°ch, gi·∫£m nhi·ªÖu (noise) do t∆∞∆°ng t√°c ng·∫´u nhi√™n ho·∫∑c kh√¥ng ƒë·ªß d·ªØ li·ªáu
                                    - ‚úÖ TƒÉng m·∫≠t ƒë·ªô d·ªØ li·ªáu t∆∞∆°ng t√°c cho c√°c thu·∫≠t to√°n c·ªông t√°c (GNN)
                                    - ‚úÖ Lo·∫°i b·ªè c√°c users v√† products c√≥ qu√° √≠t t∆∞∆°ng t√°c, gi√∫p model h·ªçc ƒë∆∞·ª£c patterns r√µ r√†ng h∆°n
                                    """)
                            
                            except Exception as e:
                                st.error(f"‚ùå L·ªói khi √°p d·ª•ng pruning: {str(e)}")
                                import traceback
                                st.code(traceback.format_exc())
                else:
                    st.info("üí° Vui l√≤ng t·∫£i l√™n file interactions.csv ho·∫∑c xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (B∆∞·ªõc 1.1) ƒë·ªÉ ti·∫øp t·ª•c.")
            
            with tab_algorithm:
                # L·∫•y gi√° tr·ªã min_interactions t·ª´ session_state ho·∫∑c s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh
                min_interactions_algo = st.session_state.get('pruning_min_interactions', 2)
                
                st.markdown(f"""
                **Thu·∫≠t to√°n {min_interactions_algo}-Core Pruning:**

                1. **Kh·ªüi t·∫°o:** ƒê·∫øm s·ªë l∆∞·ª£ng t∆∞∆°ng t√°c cho m·ªói user v√† m·ªói product
                2. **L·∫∑p ƒë·ªá quy:**
                   - Lo·∫°i b·ªè t·∫•t c·∫£ users c√≥ < {min_interactions_algo} interactions
                   - Lo·∫°i b·ªè t·∫•t c·∫£ products c√≥ < {min_interactions_algo} interactions
                   - C·∫≠p nh·∫≠t l·∫°i s·ªë l∆∞·ª£ng interactions c·ªßa c√°c users/products c√≤n l·∫°i
                   - L·∫∑p l·∫°i cho ƒë·∫øn khi kh√¥ng c√≤n user/product n√†o b·ªã lo·∫°i b·ªè
                3. **K·∫øt qu·∫£:** Ma tr·∫≠n t∆∞∆°ng t√°c $R$ ƒë∆∞·ª£c l√†m s·∫°ch, ch·ªâ gi·ªØ l·∫°i c√°c users v√† products c√≥ ƒë·ªß d·ªØ li·ªáu

                **C√¥ng th·ª©c:**
                $$R_{{pruned}} = \\{{(u, i) \\in R : |I_u| \\geq {min_interactions_algo} \\land |U_i| \\geq {min_interactions_algo}\\}}$$

                Trong ƒë√≥:
                - $R$: Ma tr·∫≠n t∆∞∆°ng t√°c g·ªëc
                - $I_u$: T·∫≠p s·∫£n ph·∫©m m√† user $u$ ƒë√£ t∆∞∆°ng t√°c
                - $U_i$: T·∫≠p users ƒë√£ t∆∞∆°ng t√°c v·ªõi s·∫£n ph·∫©m $i$
                - $R_{{pruned}}$: Ma tr·∫≠n sau khi pruning
                - ${min_interactions_algo}$: S·ªë l∆∞·ª£ng t∆∞∆°ng t√°c t·ªëi thi·ªÉu (min_interactions)
                """)

        with st.expander("B∆∞·ªõc 1.3: M√£ h√≥a ƒê·∫∑c tr∆∞ng N·ªôi dung (Feature Encoding)", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Chuy·ªÉn ƒë·ªïi c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i c·ªßa s·∫£n ph·∫©m (masterCategory, subCategory, articleType, baseColour, usage) th√†nh Item Profile Vector $\\mathbf{v}_i$ b·∫±ng One-Hot Encoding ho·∫∑c Categorical Embedding.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** `products.csv`")
            
            # T·∫°o c√°c tab: Hi·ªán th·ª±c (tr√°i) v√† Thu·∫≠t to√°n (ph·∫£i)
            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # L·ª±a ch·ªçn ngu·ªìn d·ªØ li·ªáu
                col_source1, col_source2 = st.columns([2, 1])
                with col_source1:
                    use_exported = st.checkbox(
                        "S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ xu·∫•t t·ª´ MongoDB (B∆∞·ªõc 1.1)",
                        value=True,
                        key="encoding_use_exported"
                    )
                
                products_df = None
                
                if use_exported and 'exported_data' in st.session_state and st.session_state['exported_data'].get('products_path'):
                    products_path = st.session_state['exported_data']['products_path']
                    if os.path.exists(products_path):
                        try:
                            products_df = pd.read_csv(products_path)
                            # Set product_id as index if available
                            if 'id' in products_df.columns:
                                products_df = products_df.set_index('id')
                            st.success(f"‚úÖ ƒê√£ t·∫£i products.csv t·ª´ d·ªØ li·ªáu ƒë√£ xu·∫•t: {len(products_df)} rows")
                        except Exception as e:
                            st.error(f"L·ªói khi ƒë·ªçc file: {str(e)}")
                    else:
                        st.warning("File products.csv kh√¥ng t·ªìn t·∫°i. Vui l√≤ng t·∫£i l√™n file ho·∫∑c xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB.")
                
                if products_df is None:
                    # Auto import t·ª´ apps/exports
                    export_dir = ensure_export_directory() if ensure_export_directory else None
                    if export_dir:
                        products_path_auto = export_dir / 'products.csv'
                        if products_path_auto.exists():
                            try:
                                products_df = pd.read_csv(products_path_auto)
                                # Set product_id as index if available
                                if 'id' in products_df.columns:
                                    products_df = products_df.set_index('id')
                                st.success(f"‚úÖ ƒê√£ t·ª± ƒë·ªông t·∫£i products.csv t·ª´ apps/exports: {len(products_df)} rows √ó {len(products_df.columns)} columns")
                            except Exception as e:
                                st.error(f"L·ªói khi ƒë·ªçc file t·ª´ apps/exports: {str(e)}")
                        else:
                            st.info("üí° File products.csv kh√¥ng t·ªìn t·∫°i trong apps/exports. Vui l√≤ng xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (B∆∞·ªõc 1.1) ho·∫∑c ƒë·∫£m b·∫£o file t·ªìn t·∫°i.")
                    else:
                        st.info("üí° Kh√¥ng th·ªÉ truy c·∫≠p th∆∞ m·ª•c apps/exports. Vui l√≤ng xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (B∆∞·ªõc 1.1).")
                
                if products_df is not None:
                    # L·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng
                    default_features = ['masterCategory', 'subCategory', 'articleType', 'baseColour', 'usage']
                    available_features = [f for f in default_features if f in products_df.columns]
                    
                    if not available_features:
                        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c√°c features m·∫∑c ƒë·ªãnh. Vui l√≤ng ch·ªçn features t·ª´ danh s√°ch c√≥ s·∫µn.")
                        categorical_cols = products_df.select_dtypes(include=['object', 'category']).columns.tolist()
                        selected_features = st.multiselect(
                            "Ch·ªçn c√°c features ƒë·ªÉ m√£ h√≥a",
                            categorical_cols,
                            default=categorical_cols[:5] if len(categorical_cols) >= 5 else categorical_cols,
                            key="encoding_features"
                        )
                    else:
                        selected_features = st.multiselect(
                            "Ch·ªçn c√°c features ƒë·ªÉ m√£ h√≥a",
                            available_features,
                            default=available_features,
                            key="encoding_features"
                        )
                    
                    col_config1, col_config2 = st.columns([1, 1])
                    with col_config1:
                        st.write("")  # Kho·∫£ng tr·ªëng
                    with col_config2:
                        process_button = st.button(
                            "üîß √Åp d·ª•ng Feature Encoding",
                            type="primary",
                            use_container_width=True,
                            key="encoding_process_button"
                        )
                    
                    if process_button:
                        if not selected_features:
                            st.error("‚ùå Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt feature ƒë·ªÉ m√£ h√≥a.")
                        else:
                            with st.spinner("ƒêang m√£ h√≥a ƒë·∫∑c tr∆∞ng n·ªôi dung..."):
                                try:
                                    result = apply_feature_encoding(products_df, selected_features)
                                    
                                    if result['total_dims'] == 0:
                                        st.error("‚ùå Kh√¥ng th·ªÉ m√£ h√≥a. Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
                                    else:
                                        st.success("‚úÖ **Ho√†n th√†nh!** ƒê·∫∑c tr∆∞ng n·ªôi dung ƒë√£ ƒë∆∞·ª£c m√£ h√≥a.")
                                        
                                        # L∆∞u v√†o session state
                                        st.session_state['feature_encoding'] = result
                                        # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                        save_intermediate_artifact('feature_encoding', result)
                                        
                                        # Hi·ªÉn th·ªã th·ªëng k√™
                                        st.markdown("### üìä Th·ªëng k√™ k·∫øt qu·∫£ Feature Encoding")
                                        
                                        col_stat1, col_stat2, col_stat3 = st.columns(3)
                                        with col_stat1:
                                            st.metric("S·ªë l∆∞·ª£ng s·∫£n ph·∫©m", len(products_df))
                                            st.metric("S·ªë features ƒë∆∞·ª£c m√£ h√≥a", len(selected_features))
                                        with col_stat2:
                                            st.metric("T·ªïng s·ªë chi·ªÅu", result['total_dims'])
                                            st.metric("K√≠ch th∆∞·ªõc ma tr·∫≠n", f"{len(products_df)} √ó {result['total_dims']}")
                                        with col_stat3:
                                            memory_size_mb = (len(products_df) * result['total_dims'] * 4) / (1024 * 1024)  # Gi·∫£ ƒë·ªãnh float32
                                            st.metric("K√≠ch th∆∞·ªõc b·ªô nh·ªõ (∆∞·ªõc t√≠nh)", f"{memory_size_mb:.2f} MB")
                                        
                                        # Hi·ªÉn th·ªã k√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng
                                        st.markdown("### üìê Chi ti·∫øt c√°c Features")
                                        feature_dims_df = pd.DataFrame([
                                            {
                                                'Feature': feat,
                                                'S·ªë gi√° tr·ªã unique': result['feature_dims'].get(feat, 0),
                                                'Start Index': result['feature_mapping'].get(feat, {}).get('start_idx', 0),
                                                'End Index': result['feature_mapping'].get(feat, {}).get('end_idx', 0)
                                            }
                                            for feat in selected_features
                                        ])
                                        st.dataframe(feature_dims_df, use_container_width=True)
                                        
                                        # Hi·ªÉn th·ªã c√°c vector ƒë√£ m√£ h√≥a m·∫´u
                                        st.markdown("### üî¢ M·∫´u Vector ƒë√£ m√£ h√≥a (5 s·∫£n ph·∫©m ƒë·∫ßu ti√™n)")
                                        sample_indices = min(5, len(products_df))
                                        sample_matrix = result['encoded_matrix'][:sample_indices, :]
                                        
                                        # Gi·ªõi h·∫°n 20 ƒë·∫∑c tr∆∞ng ƒë·∫ßu ti√™n ƒë·ªÉ hi·ªÉn th·ªã
                                        max_features_display = min(20, len(result['feature_names']))
                                        sample_matrix_display = sample_matrix[:, :max_features_display]
                                        feature_names_display = result['feature_names'][:max_features_display]
                                        
                                        # T·∫°o hi·ªÉn th·ªã d·ªÖ ƒë·ªçc h∆°n
                                        sample_df = pd.DataFrame(
                                            sample_matrix_display,
                                            index=[f"Product {i+1}" for i in range(sample_indices)],
                                            columns=feature_names_display
                                        )
                                        st.dataframe(sample_df, use_container_width=True)
                                        
                                        if len(result['feature_names']) > 20:
                                            st.info(f"‚ÑπÔ∏è Ch·ªâ hi·ªÉn th·ªã 20 features ƒë·∫ßu ti√™n. T·ªïng c·ªông c√≥ {len(result['feature_names'])} features.")
                                        
                                        # Hi·ªÉn th·ªã chi ti·∫øt √°nh x·∫° ƒë·∫∑c tr∆∞ng
                                        with st.expander("üìã Chi ti·∫øt Feature Mapping", expanded=False):
                                            for feat in selected_features:
                                                if feat in result['feature_mapping']:
                                                    mapping = result['feature_mapping'][feat]
                                                    st.markdown(f"#### {feat}")
                                                    st.write(f"- **S·ªë gi√° tr·ªã unique:** {result['feature_dims'][feat]}")
                                                    st.write(f"- **Ch·ªâ s·ªë b·∫Øt ƒë·∫ßu:** {mapping['start_idx']}")
                                                    st.write(f"- **Ch·ªâ s·ªë k·∫øt th√∫c:** {mapping['end_idx']}")
                                                    values_str = ', '.join(list(mapping['value_to_idx'].keys())[:10])
                                                    if len(mapping['value_to_idx']) > 10:
                                                        values_str += f" ... v√† {len(mapping['value_to_idx']) - 10} gi√° tr·ªã kh√°c"
                                                    st.write(f"- **C√°c gi√° tr·ªã:** {values_str}")
                                        
                                        # Tr·ª±c quan h√≥a ph√¢n b·ªë ƒë·∫∑c tr∆∞ng
                                        st.markdown("### üìä Ph√¢n b·ªë s·ªë l∆∞·ª£ng gi√° tr·ªã unique theo Feature")
                                        dims_data = {
                                            'Feature': list(result['feature_dims'].keys()),
                                            'S·ªë gi√° tr·ªã unique': list(result['feature_dims'].values())
                                        }
                                        dims_df = pd.DataFrame(dims_data)
                                        fig = px.bar(
                                            dims_df,
                                            x='Feature',
                                            y='S·ªë gi√° tr·ªã unique',
                                            title="S·ªë l∆∞·ª£ng gi√° tr·ªã unique c·ªßa m·ªói feature",
                                            labels={'Feature': 'Feature', 'S·ªë gi√° tr·ªã unique': 'S·ªë gi√° tr·ªã unique'}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                        # Hi·ªÉn th·ªã th√¥ng tin ma tr·∫≠n
                                        st.markdown("### üìê Th√¥ng tin Ma tr·∫≠n ƒë·∫∑c tr∆∞ng $P$")
                                        st.latex(f"P \\in \\mathbb{{R}}^{{{len(products_df)} \\times {result['total_dims']}}}")
                                        
                                        # ƒê·ªô th∆∞a c·ªßa ma tr·∫≠n ƒë√£ m√£ h√≥a
                                        total_elements = len(products_df) * result['total_dims']
                                        non_zero_elements = np.count_nonzero(result['encoded_matrix'])
                                        sparsity = 1 - (non_zero_elements / total_elements) if total_elements > 0 else 0
                                        
                                        col_matrix1, col_matrix2 = st.columns(2)
                                        with col_matrix1:
                                            st.metric("T·ªïng s·ªë ph·∫ßn t·ª≠", f"{total_elements:,}")
                                            st.metric("Ph·∫ßn t·ª≠ kh√°c 0", f"{non_zero_elements:,}")
                                        with col_matrix2:
                                            st.metric("ƒê·ªô th∆∞a", f"{sparsity:.4f}")
                                            density = 1 - sparsity
                                            st.metric("M·∫≠t ƒë·ªô", f"{density:.4f}")
                                        
                                        st.info("‚ÑπÔ∏è Ma tr·∫≠n One-Hot Encoding th∆∞·ªùng c√≥ ƒë·ªô th∆∞a cao v√¨ m·ªói h√†ng ch·ªâ c√≥ m·ªôt s·ªë ph·∫ßn t·ª≠ b·∫±ng 1 (t∆∞∆°ng ·ª©ng v·ªõi c√°c gi√° tr·ªã c·ªßa features).")
                                        
                                        st.markdown("""
                                        **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                        - ‚úÖ Vector $\\mathbf{v}_i$ cho m·ªói s·∫£n ph·∫©m $i$ trong h·ªá th·ªëng, ƒë·∫°i di·ªán cho thu·ªôc t√≠nh n·ªôi dung c·ªßa n√≥
                                        - ‚úÖ Ma tr·∫≠n ƒë·∫∑c tr∆∞ng $P \\in \\mathbb{R}^{|I| \\times d_c}$ ƒë∆∞·ª£c t·∫°o th√†nh
                                        - ‚úÖ C√°c vector n√†y l√† ƒë·∫ßu v√†o c∆° s·ªü cho CBF (Content-Based Filtering) v√† Diversity (ILD) metric
                                        - ‚úÖ M·ªói s·∫£n ph·∫©m ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector s·ªë h·ªçc, c√≥ th·ªÉ t√≠nh to√°n similarity v√† distance
                                        """)
                            
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi m√£ h√≥a features: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
                else:
                    st.info("üí° Vui l√≤ng t·∫£i l√™n file products.csv ho·∫∑c xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (B∆∞·ªõc 1.1) ƒë·ªÉ ti·∫øp t·ª•c.")
            
            with tab_algorithm:
                st.markdown("""
                **Ph∆∞∆°ng ph√°p m√£ h√≥a:**

                **1. One-Hot Encoding:**
                - M·ªói gi√° tr·ªã ph√¢n lo·∫°i ƒë∆∞·ª£c chuy·ªÉn th√†nh m·ªôt vector nh·ªã ph√¢n
                - V√≠ d·ª•: masterCategory c√≥ 3 gi√° tr·ªã ‚Üí 3 chi·ªÅu binary vector
                - T·ªïng s·ªë chi·ªÅu = t·ªïng s·ªë gi√° tr·ªã unique c·ªßa t·∫•t c·∫£ c√°c features

                **2. Categorical Embedding (Alternative):**
                - S·ª≠ d·ª•ng embedding layer ƒë·ªÉ h·ªçc vector ƒë·∫°i di·ªán
                - K√≠ch th∆∞·ªõc nh·ªè g·ªçn h∆°n One-Hot
                - C√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá gi·ªØa c√°c categories

                **C√¥ng th·ª©c:**
                $$\\mathbf{v}_i = [\\text{OneHot}(\\text{masterCategory}_i), \\text{OneHot}(\\text{subCategory}_i), \\text{OneHot}(\\text{articleType}_i), \\text{OneHot}(\\text{baseColour}_i), \\text{OneHot}(\\text{usage}_i)]$$

                Trong ƒë√≥:
                - $\\mathbf{v}_i$: Item Profile Vector c·ªßa s·∫£n ph·∫©m $i$
                - $\\text{OneHot}(x)$: Vector one-hot encoding c·ªßa gi√° tr·ªã $x$
                - K·∫øt qu·∫£: Vector concatenation c·ªßa t·∫•t c·∫£ c√°c features

                **K·∫øt qu·∫£ t√≠nh to√°n:**
                - Ma tr·∫≠n ƒë·∫∑c tr∆∞ng $P \\in \\mathbb{R}^{|I| \\times d_c}$, n∆°i $d_c$ l√† t·ªïng s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng n·ªôi dung (t·ªïng s·ªë gi√° tr·ªã unique c·ªßa t·∫•t c·∫£ features)
                - $|I|$: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m

                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - Vector $\\mathbf{v}_i$ cho m·ªói s·∫£n ph·∫©m $i$ trong h·ªá th·ªëng, ƒë·∫°i di·ªán cho thu·ªôc t√≠nh n·ªôi dung c·ªßa n√≥
                - C√°c vector n√†y l√† ƒë·∫ßu v√†o c∆° s·ªü cho CBF (Content-Based Filtering) v√† Diversity (ILD) metric
                - M·ªói s·∫£n ph·∫©m ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector s·ªë h·ªçc, c√≥ th·ªÉ t√≠nh to√°n similarity v√† distance
                """)

        st.markdown('<div class="sub-header">üìö PH·∫¶N II: M√î H√åNH L·ªåC D·ª∞A TR√äN N·ªòI DUNG (CONTENT-BASED FILTERING - CBF)</div>', unsafe_allow_html=True)
        st.markdown("")

        with st.expander("B∆∞·ªõc 2.1: X√¢y d·ª±ng H·ªì s∆° Ng∆∞·ªùi d√πng C√≥ Tr·ªçng s·ªë (Weighted User Profile)", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Vector H·ªì s∆° Ng∆∞·ªùi d√πng $\\mathbf{P}_u$ ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng c√°ch t·ªïng h·ª£p c√≥ tr·ªçng s·ªë c√°c Item Profile $\\mathbf{v}_i$ c·ªßa c√°c s·∫£n ph·∫©m m√† ng∆∞·ªùi d√πng ƒë√£ t∆∞∆°ng t√°c t√≠ch c·ª±c.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 1.2 (Pruned Interactions) v√† B∆∞·ªõc 1.3 (Feature Encoding)")

            # T·∫°o c√°c tab: Hi·ªán th·ª±c (tr√°i) v√† Thu·∫≠t to√°n (ph·∫£i)
            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_pruned_interactions = 'pruned_interactions' in st.session_state
                has_feature_encoding = 'feature_encoding' in st.session_state

                if not has_pruned_interactions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.2 (Pruning). Vui l√≤ng ch·∫°y B∆∞·ªõc 1.2 tr∆∞·ªõc.")
                if not has_feature_encoding:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.3 (Feature Encoding). Vui l√≤ng ch·∫°y B∆∞·ªõc 1.3 tr∆∞·ªõc.")

                if has_pruned_interactions and has_feature_encoding:
                    if build_weighted_user_profile is None:
                        st.error(f"‚ùå Kh√¥ng th·ªÉ import user_profile module: {_user_profile_import_error}")
                        st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/user_profile.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                    else:
                        # L·∫•y d·ªØ li·ªáu t·ª´ session state
                        pruning_result = st.session_state['pruned_interactions']
                        encoding_result = st.session_state['feature_encoding']
                        
                        pruned_interactions_df = pruning_result['pruned_interactions']
                        encoded_matrix = encoding_result['encoded_matrix']
                        product_ids = encoding_result['product_ids']
                        
                        # Hi·ªÉn th·ªã th√¥ng tin d·ªØ li·ªáu
                        col_info1, col_info2 = st.columns(2)
                        with col_info1:
                            st.info(f"üìã Interactions: {len(pruned_interactions_df)} rows")
                            st.info(f"üìê Feature Matrix: {encoded_matrix.shape[0]} products √ó {encoded_matrix.shape[1]} features")
                        with col_info2:
                            st.info(f"üë• Users: {pruned_interactions_df['user_id'].nunique()}")
                            st.info(f"üì¶ Products: {pruned_interactions_df['product_id'].nunique()}")
                        
                        # Ki·ªÉm tra interaction_type
                        has_interaction_type = 'interaction_type' in pruned_interactions_df.columns
                        if not has_interaction_type:
                            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'interaction_type'. S·∫Ω s·ª≠ d·ª•ng tr·ªçng s·ªë m·∫∑c ƒë·ªãnh = 1.0 cho t·∫•t c·∫£ interactions.")
                        
                        # Hi·ªÉn th·ªã b·∫£ng tr·ªçng s·ªë
                        st.markdown("### ‚öñÔ∏è Tr·ªçng s·ªë t∆∞∆°ng t√°c")
                        weights_df = pd.DataFrame([
                            {'Interaction Type': k, 'Weight': v, 'M√¥ t·∫£': {
                                'purchase': 'Cao nh·∫•t (s·ªü th√≠ch r√µ r√†ng)',
                                'like': 'S·ªü th√≠ch m·∫°nh m·∫Ω',
                                'cart': '√ù ƒë·ªãnh mua s·∫Øm',
                                'view': 'T∆∞∆°ng t√°c th·ª• ƒë·ªông'
                            }.get(k, 'M·∫∑c ƒë·ªãnh')}
                            for k, v in INTERACTION_WEIGHTS.items()
                        ])
                        st.dataframe(weights_df, use_container_width=True)
                        
                        # N√∫t ƒë·ªÉ t√≠nh to√°n
                        process_button = st.button(
                            "üîß X√¢y d·ª±ng Weighted User Profiles",
                            type="primary",
                            use_container_width=True,
                            key="user_profile_process_button"
                        )
                        
                        if process_button:
                            # ƒêo Training Time (B∆∞·ªõc 2.1: x√¢y d·ª±ng P_u)
                            training_start_time = time.time()
                            
                            with st.spinner("ƒêang x√¢y d·ª±ng h·ªì s∆° ng∆∞·ªùi d√πng c√≥ tr·ªçng s·ªë..."):
                                try:
                                    result = build_weighted_user_profile(
                                        pruned_interactions_df,
                                        encoded_matrix,
                                        product_ids,
                                        INTERACTION_WEIGHTS
                                    )
                                    
                                    # K·∫øt th√∫c ƒëo Training Time
                                    training_end_time = time.time()
                                    training_time_measured = training_end_time - training_start_time
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['training_time'] = training_time_measured
                                    
                                    if result['total_users'] == 0:
                                        st.error("‚ùå Kh√¥ng th·ªÉ x√¢y d·ª±ng user profiles. Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
                                    else:
                                        st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ x√¢y d·ª±ng {result['total_users']} user profiles.")
                                        
                                        st.session_state['user_profiles'] = result
                                        # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                        save_intermediate_artifact('user_profiles', result)
                                        
                                        st.markdown("### üìä Th·ªëng k√™ k·∫øt qu·∫£ User Profiles")
                                        
                                        col_stat1, col_stat2, col_stat3 = st.columns(3)
                                        with col_stat1:
                                            st.metric("T·ªïng s·ªë users", result['total_users'])
                                            st.metric("S·ªë chi·ªÅu feature vector", result['feature_dim'])
                                        with col_stat2:
                                            total_interactions = sum(stat['interaction_count'] for stat in result['user_stats'].values())
                                            avg_interactions = total_interactions / result['total_users'] if result['total_users'] > 0 else 0
                                            st.metric("T·ªïng interactions", total_interactions)
                                            st.metric("Trung b√¨nh interactions/user", f"{avg_interactions:.2f}")
                                        with col_stat3:
                                            total_weight = sum(stat['total_weight'] for stat in result['user_stats'].values())
                                            avg_weight = total_weight / result['total_users'] if result['total_users'] > 0 else 0
                                            st.metric("T·ªïng tr·ªçng s·ªë", f"{total_weight:.2f}")
                                            st.metric("Trung b√¨nh tr·ªçng s·ªë/user", f"{avg_weight:.2f}")
                                        
                                        # Hi·ªÉn th·ªã c·∫£nh b√°o v·ªÅ skipped products n·∫øu c√≥
                                        if result.get('skipped_products', 0) > 0:
                                            st.warning(f"‚ö†Ô∏è C√≥ {result['skipped_products']} products trong interactions kh√¥ng t√¨m th·∫•y trong encoded matrix. C√°c products n√†y s·∫Ω b·ªã b·ªè qua khi t√≠nh to√°n user profiles.")
                                            if result.get('skipped_product_ids'):
                                                with st.expander("Xem danh s√°ch products b·ªã skip (10 ƒë·∫ßu ti√™n)", expanded=False):
                                                    st.write(result['skipped_product_ids'])
                                        
                                        # T·∫°o c√°c tab cho c√°c h√¨nh ·∫£nh h√≥a kh√°c nhau
                                        tab1, tab2, tab3, tab4 = st.tabs([
                                            "üìã M·∫´u User Profiles",
                                            "üìä Ph√¢n b·ªë s·ªë l∆∞·ª£ng Interactions",
                                            "üìà Ph√¢n b·ªë Tr·ªçng s·ªë",
                                            "üéì Train Set (Interactions ƒë√£ d√πng)"
                                        ])
                                        
                                        with tab1:
                                            st.markdown("### üìã M·∫´u User Profiles (5 users ƒë·∫ßu ti√™n)")
                                            
                                            # L·∫•y 5 users ƒë·∫ßu ti√™n
                                            sample_users = list(result['user_profiles'].keys())[:5]
                                            
                                            for idx, user_id in enumerate(sample_users, 1):
                                                profile = result['user_profiles'][user_id]
                                                stats = result['user_stats'][user_id]
                                                
                                                with st.expander(f"User {user_id} (Interactions: {stats['interaction_count']}, Total Weight: {stats['total_weight']:.2f})", expanded=False):
                                                    # Hi·ªÉn th·ªã m·ªôt ph·∫ßn vector (20 ƒë·∫∑c tr∆∞ng ƒë·∫ßu)
                                                    max_features_display = min(20, len(profile))
                                                    profile_display = profile[:max_features_display]
                                                    
                                                    profile_df = pd.DataFrame({
                                                        'Feature Index': range(max_features_display),
                                                        'Value': profile_display
                                                    })
                                                    st.dataframe(profile_df, use_container_width=True)
                                                    
                                                    if len(profile) > max_features_display:
                                                        st.info(f"‚ÑπÔ∏è Ch·ªâ hi·ªÉn th·ªã {max_features_display} features ƒë·∫ßu ti√™n. T·ªïng c·ªông c√≥ {len(profile)} features.")
                                                    
                                                    # Th·ªëng k√™ vector
                                                    col_vec1, col_vec2, col_vec3 = st.columns(3)
                                                    with col_vec1:
                                                        st.metric("Min", f"{profile.min():.4f}")
                                                    with col_vec2:
                                                        st.metric("Max", f"{profile.max():.4f}")
                                                    with col_vec3:
                                                        st.metric("Mean", f"{profile.mean():.4f}")
                                        
                                        with tab2:
                                            st.markdown("### üìä Ph√¢n b·ªë s·ªë l∆∞·ª£ng Interactions per User")
                                            
                                            interaction_counts = [stats['interaction_count'] for stats in result['user_stats'].values()]
                                            counts_df = pd.DataFrame({
                                                'User': range(len(interaction_counts)),
                                                'Interaction Count': interaction_counts
                                            })
                                            
                                            fig = px.histogram(
                                                counts_df,
                                                x='Interaction Count',
                                                nbins=20,
                                                title="Ph√¢n b·ªë s·ªë l∆∞·ª£ng interactions c·ªßa m·ªói user",
                                                labels={'Interaction Count': 'S·ªë l∆∞·ª£ng Interactions', 'count': 'S·ªë l∆∞·ª£ng Users'}
                                            )
                                            st.plotly_chart(fig, use_container_width=True)
                                            
                                            # Th·ªëng k√™
                                            col_dist1, col_dist2, col_dist3 = st.columns(3)
                                            with col_dist1:
                                                st.metric("Min interactions", min(interaction_counts))
                                                st.metric("Max interactions", max(interaction_counts))
                                            with col_dist2:
                                                st.metric("Mean", f"{np.mean(interaction_counts):.2f}")
                                                st.metric("Median", f"{np.median(interaction_counts):.2f}")
                                            with col_dist3:
                                                st.metric("Std", f"{np.std(interaction_counts):.2f}")
                                        
                                        with tab3:
                                            st.markdown("### üìà Ph√¢n b·ªë Tr·ªçng s·ªë per User")
                                            
                                            total_weights = [stats['total_weight'] for stats in result['user_stats'].values()]
                                            avg_weights = [stats['avg_weight'] for stats in result['user_stats'].values()]
                                            
                                            weights_df = pd.DataFrame({
                                                'User': range(len(total_weights)),
                                                'Total Weight': total_weights,
                                                'Average Weight': avg_weights
                                            })
                                            
                                            fig = go.Figure()
                                            fig.add_trace(go.Histogram(
                                                x=weights_df['Total Weight'],
                                                name='Total Weight',
                                                nbinsx=20,
                                                opacity=0.7
                                            ))
                                            fig.add_trace(go.Histogram(
                                                x=weights_df['Average Weight'],
                                                name='Average Weight',
                                                nbinsx=20,
                                                opacity=0.7
                                            ))
                                            fig.update_layout(
                                                title="Ph√¢n b·ªë tr·ªçng s·ªë c·ªßa m·ªói user",
                                                xaxis_title="Tr·ªçng s·ªë",
                                                yaxis_title="S·ªë l∆∞·ª£ng Users",
                                                barmode='overlay'
                                            )
                                            st.plotly_chart(fig, use_container_width=True)
                                            
                                            # Th·ªëng k√™
                                            col_weight1, col_weight2 = st.columns(2)
                                            with col_weight1:
                                                st.markdown("#### Total Weight")
                                                st.metric("Min", f"{min(total_weights):.2f}")
                                                st.metric("Max", f"{max(total_weights):.2f}")
                                                st.metric("Mean", f"{np.mean(total_weights):.2f}")
                                            with col_weight2:
                                                st.markdown("#### Average Weight")
                                                st.metric("Min", f"{min(avg_weights):.2f}")
                                                st.metric("Max", f"{max(avg_weights):.2f}")
                                                st.metric("Mean", f"{np.mean(avg_weights):.2f}")
                                        
                                        with tab4:
                                            st.markdown("### üéì Train Set - Interactions ƒë√£ d√πng ƒë·ªÉ x√¢y d·ª±ng User Profiles")
                                            st.info("üí° **Train Set** bao g·ªìm t·∫•t c·∫£ c√°c interactions (purchase, like, cart, view) m√† user ƒë√£ th·ª±c hi·ªán. C√°c interactions n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x√¢y d·ª±ng vector h·ªì s∆° ng∆∞·ªùi d√πng $\\mathbf{P}_u$.")
                                            
                                            # L·∫•y d·ªØ li·ªáu interactions t·ª´ pruning result
                                            train_set_df = pruned_interactions_df.copy()
                                            
                                            # Hi·ªÉn th·ªã th·ªëng k√™ train set
                                            col_train1, col_train2, col_train3 = st.columns(3)
                                            with col_train1:
                                                st.metric("T·ªïng s·ªë interactions", len(train_set_df))
                                                st.metric("S·ªë users", train_set_df['user_id'].nunique())
                                            with col_train2:
                                                st.metric("S·ªë products", train_set_df['product_id'].nunique())
                                                if 'interaction_type' in train_set_df.columns:
                                                    st.metric("S·ªë lo·∫°i t∆∞∆°ng t√°c", train_set_df['interaction_type'].nunique())
                                            with col_train3:
                                                if 'interaction_type' in train_set_df.columns:
                                                    interaction_counts = train_set_df['interaction_type'].value_counts()
                                                    st.markdown("**Ph√¢n b·ªë theo lo·∫°i:**")
                                                    for itype, count in interaction_counts.items():
                                                        st.write(f"- {itype}: {count} ({count/len(train_set_df)*100:.1f}%)")
                                            
                                            # Hi·ªÉn th·ªã m·∫´u train set
                                            st.markdown("#### üìã M·∫´u Train Set (10 interactions ƒë·∫ßu ti√™n)")
                                            sample_train = train_set_df.head(10)
                                            display_cols = ['user_id', 'product_id']
                                            if 'interaction_type' in sample_train.columns:
                                                display_cols.append('interaction_type')
                                            if 'timestamp' in sample_train.columns:
                                                display_cols.append('timestamp')
                                            
                                            st.dataframe(sample_train[display_cols], use_container_width=True)
                                            
                                            # Hi·ªÉn th·ªã train set cho m·ªôt user c·ª• th·ªÉ
                                            st.markdown("#### üîç Train Set cho m·ªôt User c·ª• th·ªÉ")
                                            sample_user_ids = list(result['user_profiles'].keys())[:10]
                                            selected_train_user = st.selectbox(
                                                "Ch·ªçn User ƒë·ªÉ xem train set",
                                                sample_user_ids,
                                                key="train_set_user_selector"
                                            )
                                            
                                            if selected_train_user:
                                                user_train_interactions = train_set_df[
                                                    train_set_df['user_id'].astype(str) == str(selected_train_user)
                                                ]
                                                
                                                if not user_train_interactions.empty:
                                                    col_user_train1, col_user_train2 = st.columns(2)
                                                    with col_user_train1:
                                                        st.metric("S·ªë interactions", len(user_train_interactions))
                                                        if 'interaction_type' in user_train_interactions.columns:
                                                            type_counts = user_train_interactions['interaction_type'].value_counts()
                                                            st.markdown("**Theo lo·∫°i:**")
                                                            for itype, count in type_counts.items():
                                                                weight = INTERACTION_WEIGHTS.get(itype, 1.0)
                                                                st.write(f"- {itype}: {count} (weight={weight})")
                                                    with col_user_train2:
                                                        st.metric("S·ªë products", user_train_interactions['product_id'].nunique())
                                                        stats = result['user_stats'].get(str(selected_train_user), {})
                                                        st.metric("Total Weight", f"{stats.get('total_weight', 0):.2f}")
                                                    
                                                    # Hi·ªÉn th·ªã danh s√°ch interactions
                                                    st.markdown(f"**Danh s√°ch interactions c·ªßa User {selected_train_user}:**")
                                                    display_user_cols = ['product_id']
                                                    if 'interaction_type' in user_train_interactions.columns:
                                                        display_user_cols.append('interaction_type')
                                                    if 'timestamp' in user_train_interactions.columns:
                                                        display_user_cols.append('timestamp')
                                                    
                                                    user_train_display = user_train_interactions[display_user_cols].copy()
                                                    if 'interaction_type' in user_train_display.columns:
                                                        user_train_display['weight'] = user_train_display['interaction_type'].map(
                                                            lambda x: INTERACTION_WEIGHTS.get(x, 1.0)
                                                        )
                                                    
                                                    st.dataframe(user_train_display, use_container_width=True)
                                                else:
                                                    st.warning(f"Kh√¥ng t√¨m th·∫•y interactions cho user {selected_train_user}")
                            
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi x√¢y d·ª±ng user profiles: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
                else:
                    st.info("üí° Vui l√≤ng ho√†n th√†nh B∆∞·ªõc 1.2 (Pruning) v√† B∆∞·ªõc 1.3 (Feature Encoding) tr∆∞·ªõc khi ti·∫øp t·ª•c.")
            
            with tab_algorithm:
                st.markdown("""
                **C√¥ng th·ª©c Vector H·ªì s∆° Ng∆∞·ªùi d√πng:**
                $$\\mathbf{P}_u = \\frac{\\sum_{i \\in I_u^+} w_{ui} \\mathbf{v}_i}{\\sum_{i \\in I_u^+} w_{ui}}$$
                    
                Trong ƒë√≥:
                - $\\mathbf{P}_u$: Vector h·ªì s∆° ng∆∞·ªùi d√πng $u$
                - $I_u^+$: T·∫≠p h·ª£p c√°c s·∫£n ph·∫©m ƒë√£ t∆∞∆°ng t√°c c·ªßa user $u$
                - $w_{ui}$: Tr·ªçng s·ªë t∆∞∆°ng t√°c gi·ªØa user $u$ v√† item $i$
                - $\\mathbf{v}_i$: Item Profile Vector c·ªßa s·∫£n ph·∫©m $i$

                **Tr·ªçng s·ªë t∆∞∆°ng t√°c ($w_{ui}$):**
                | interaction_type | $w_{ui}$ | ƒê·ªô ∆Øu ti√™n |
                |------------------|----------|------------|
                | purchase | 5.0 | Cao nh·∫•t (s·ªü th√≠ch r√µ r√†ng) |
                | like | 3.0 | S·ªü th√≠ch m·∫°nh m·∫Ω |
                | cart | 2.0 | √ù ƒë·ªãnh mua s·∫Øm |
                | view | 1.0 | T∆∞∆°ng t√°c th·ª• ƒë·ªông |

                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - Vector $\\mathbf{P}_u$ (H·ªì s∆° Ng∆∞·ªùi d√πng) ƒë∆∞·ª£c t√≠nh to√°n cho m·ªói ng∆∞·ªùi d√πng
                - ƒê·∫°i di·ªán cho s·ªü th√≠ch trung b√¨nh c√≥ tr·ªçng s·ªë c·ªßa h·ªç trong kh√¥ng gian thu·ªôc t√≠nh s·∫£n ph·∫©m
                - Vector n√†y l√† c∆° s·ªü ƒë·ªÉ t√≠nh to√°n ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng cho CBF (Content-Based Filtering)
                """)
                
                st.markdown("### üßÆ V√≠ d·ª• t√≠nh to√°n")
                st.markdown("""
                **V√≠ d·ª•:** User $u$ t∆∞∆°ng t√°c ba s·∫£n ph·∫©m:
                - Product 1: purchase (weight=5.0), vector $\\mathbf{v}_1 = [1, 1, 1]$
                - Product 2: like (weight=3.0), vector $\\mathbf{v}_2 = [0, 1, 0]$
                - Product 3: view (weight=1.0), vector $\\mathbf{v}_3 = [1, 0, 1]$
                
                **T√≠nh to√°n:**
                - $\\sum w_{ui} \\mathbf{v}_i = 5[1, 1, 1] + 3[0, 1, 0] + 1[1, 0, 1] = [5, 5, 5] + [0, 3, 0] + [1, 0, 1] = [6, 8, 6]$
                - $\\sum w_{ui} = 5 + 3 + 1 = 9$
                - $\\mathbf{P}_u = [6/9, 8/9, 6/9] = [0.67, 0.89, 0.67]$
                """)
                
                st.markdown("""
                **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                - ‚úÖ Vector $\\mathbf{P}_u$ (H·ªì s∆° Ng∆∞·ªùi d√πng) ƒë∆∞·ª£c t√≠nh to√°n cho m·ªói ng∆∞·ªùi d√πng
                - ‚úÖ ƒê·∫°i di·ªán cho s·ªü th√≠ch trung b√¨nh c√≥ tr·ªçng s·ªë c·ªßa h·ªç trong kh√¥ng gian thu·ªôc t√≠nh s·∫£n ph·∫©m
                - ‚úÖ Vector n√†y l√† c∆° s·ªü ƒë·ªÉ t√≠nh to√°n ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng cho CBF (Content-Based Filtering)
                """)

        with st.expander("B∆∞·ªõc 2.2: T√≠nh ƒêi·ªÉm D·ª± ƒëo√°n v√† X·∫øp h·∫°ng", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng Cosine gi·ªØa H·ªì s∆° Ng∆∞·ªùi d√πng $\\mathbf{P}_u$ v√† Item Profile $\\mathbf{v}_i$ ƒë·ªÉ d·ª± ƒëo√°n ƒëi·ªÉm t∆∞∆°ng t√°c $\\hat{r}_{ui}^{\\text{CBF}}$.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 2.1 (User Profiles) v√† B∆∞·ªõc 1.3 (Feature Encoding)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_user_profiles = 'user_profiles' in st.session_state
                has_feature_encoding = 'feature_encoding' in st.session_state

                if not has_user_profiles:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 2.1 (User Profiles). Vui l√≤ng ch·∫°y B∆∞·ªõc 2.1 tr∆∞·ªõc.")
                if not has_feature_encoding:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.3 (Feature Encoding). Vui l√≤ng ch·∫°y B∆∞·ªõc 1.3 tr∆∞·ªõc.")

                if has_user_profiles and has_feature_encoding:
                    if compute_cbf_predictions is None:
                        st.error(f"‚ùå Kh√¥ng th·ªÉ import user_profile module: {_user_profile_import_error}")
                        st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/user_profile.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                    else:
                        # L·∫•y d·ªØ li·ªáu t·ª´ session state
                        user_profiles_result = st.session_state['user_profiles']
                        encoding_result = st.session_state['feature_encoding']
                        
                        user_profiles = user_profiles_result['user_profiles']
                        encoded_matrix = encoding_result['encoded_matrix']
                        product_ids = encoding_result['product_ids']
                        
                        # Hi·ªÉn th·ªã th√¥ng tin d·ªØ li·ªáu
                        col_info1, col_info2 = st.columns(2)
                        with col_info1:
                            st.info(f"üë• Users: {len(user_profiles)}")
                            st.info(f"üìê Feature Matrix: {encoded_matrix.shape[0]} products √ó {encoded_matrix.shape[1]} features")
                        with col_info2:
                            st.info(f"üì¶ Products: {len(product_ids)}")
                            st.info(f"üî¢ Total Predictions: {len(user_profiles) * len(product_ids):,}")
                        
                        # C·∫•u h√¨nh
                        col_config1, col_config2 = st.columns(2)
                        with col_config1:
                            top_k = st.number_input(
                                "S·ªë l∆∞·ª£ng s·∫£n ph·∫©m Top-K ƒë·ªÉ x·∫øp h·∫°ng",
                                min_value=5,
                                max_value=100,
                                value=20,
                                step=5,
                                key="cbf_top_k"
                            )
                        
                        with col_config2:
                            st.write("")  # Kho·∫£ng tr·ªëng
                            process_button = st.button(
                                "üîß T√≠nh ƒêi·ªÉm D·ª± ƒëo√°n v√† X·∫øp h·∫°ng",
                                type="primary",
                                use_container_width=True,
                                key="cbf_predictions_process_button"
                            )
                        
                        if process_button:
                            with st.spinner("ƒêang t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n v√† x·∫øp h·∫°ng..."):
                                try:
                                    result = compute_cbf_predictions(
                                        user_profiles,
                                        encoded_matrix,
                                        product_ids,
                                        top_k=top_k
                                    )
                                    
                                    if result['stats']['total_predictions'] == 0:
                                        st.error("‚ùå Kh√¥ng th·ªÉ t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n. Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
                                    else:
                                        st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n cho {result['stats']['total_users']} users v√† {result['stats']['total_products']} products.")
                                        
                                        # L∆∞u v√†o session state & l∆∞u ra artifacts
                                        st.session_state['cbf_predictions'] = result
                                        save_predictions_artifact("cbf", result)
                                        
                                        # Hi·ªÉn th·ªã th·ªëng k√™
                                        st.markdown("### üìä Th·ªëng k√™ k·∫øt qu·∫£ Predictions")
                                        
                                        col_stat1, col_stat2, col_stat3 = st.columns(3)
                                        with col_stat1:
                                            st.metric("T·ªïng s·ªë predictions", f"{result['stats']['total_predictions']:,}")
                                            st.metric("S·ªë users", result['stats']['total_users'])
                                        with col_stat2:
                                            st.metric("S·ªë products", result['stats']['total_products'])
                                            st.metric("Top-K", top_k)
                                        with col_stat3:
                                            st.metric("Min score", f"{result['stats']['min_score']:.4f}")
                                            st.metric("Max score", f"{result['stats']['max_score']:.4f}")
                                            st.metric("Mean score", f"{result['stats']['mean_score']:.4f}")
                                            st.metric("Std score", f"{result['stats']['std_score']:.4f}")
                                        
                                        # T·∫°o c√°c tab cho c√°c h√¨nh ·∫£nh h√≥a kh√°c nhau
                                        tab1, tab2, tab3, tab4 = st.tabs([
                                            "üìã M·∫´u Rankings (Top-K)",
                                            "üìä Ph√¢n b·ªë ƒêi·ªÉm s·ªë",
                                            "üîç Chi ti·∫øt Predictions",
                                            "üß™ Test Set (S·∫£n ph·∫©m ƒë∆∞·ª£c d·ª± ƒëo√°n)"
                                        ])
                                        
                                        with tab1:
                                            st.markdown(f"### üìã M·∫´u Rankings Top-{top_k} (5 users ƒë·∫ßu ti√™n)")
                                            
                                            # L·∫•y 5 users ƒë·∫ßu ti√™n
                                            sample_users = list(result['rankings'].keys())[:5]
                                            
                                            for idx, user_id in enumerate(sample_users, 1):
                                                ranking = result['rankings'][user_id]
                                                
                                                with st.expander(f"User {user_id} - Top {len(ranking)} s·∫£n ph·∫©m", expanded=False):
                                                    ranking_df = pd.DataFrame([
                                                        {
                                                            'Rank': rank + 1,
                                                            'Product ID': product_id,
                                                            'Score': f"{score:.4f}"
                                                        }
                                                        for rank, (product_id, score) in enumerate(ranking)
                                                    ])
                                                    st.dataframe(ranking_df, use_container_width=True)
                                        
                                        with tab2:
                                            st.markdown("### üìä Ph√¢n b·ªë ƒêi·ªÉm s·ªë Predictions")
                                            
                                            # L·∫•y t·∫•t c·∫£ c√°c ƒëi·ªÉm s·ªë
                                            all_scores = []
                                            for user_preds in result['predictions'].values():
                                                all_scores.extend(user_preds.values())
                                            
                                            scores_df = pd.DataFrame({
                                                'Score': all_scores
                                            })
                                            
                                            # Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t
                                            fig = px.histogram(
                                                scores_df,
                                                x='Score',
                                                nbins=50,
                                                title="Ph√¢n b·ªë ƒëi·ªÉm s·ªë predictions (Cosine Similarity)",
                                                labels={'Score': 'ƒêi·ªÉm s·ªë (Cosine Similarity)', 'count': 'S·ªë l∆∞·ª£ng'}
                                            )
                                            st.plotly_chart(fig, use_container_width=True)
                                            
                                            # Bi·ªÉu ƒë·ªì h·ªôp
                                            fig_box = go.Figure()
                                            fig_box.add_trace(go.Box(
                                                y=all_scores,
                                                name='Predictions Scores',
                                                boxmean='sd'
                                            ))
                                            fig_box.update_layout(
                                                title="Box Plot - Ph√¢n b·ªë ƒëi·ªÉm s·ªë predictions",
                                                yaxis_title="ƒêi·ªÉm s·ªë (Cosine Similarity)"
                                            )
                                            st.plotly_chart(fig_box, use_container_width=True)
                                            
                                            # Th·ªëng k√™ chi ti·∫øt
                                            col_dist1, col_dist2 = st.columns(2)
                                            with col_dist1:
                                                st.markdown("#### Th·ªëng k√™ m√¥ t·∫£")
                                                stats_desc = pd.DataFrame({
                                                    'Metric': ['Min', 'Q1 (25%)', 'Median (50%)', 'Q3 (75%)', 'Max', 'Mean', 'Std'],
                                                    'Value': [
                                                        f"{np.min(all_scores):.4f}",
                                                        f"{np.percentile(all_scores, 25):.4f}",
                                                        f"{np.percentile(all_scores, 50):.4f}",
                                                        f"{np.percentile(all_scores, 75):.4f}",
                                                        f"{np.max(all_scores):.4f}",
                                                        f"{np.mean(all_scores):.4f}",
                                                        f"{np.std(all_scores):.4f}"
                                                    ]
                                                })
                                                st.dataframe(stats_desc, use_container_width=True)
                                            
                                            with col_dist2:
                                                st.markdown("#### Ph√¢n b·ªë theo kho·∫£ng")
                                                # Ph√¢n chia th√†nh c√°c kho·∫£ng
                                                bins = np.linspace(-1, 1, 21)  # 20 bins t·ª´ -1 ƒë·∫øn 1
                                                hist, bin_edges = np.histogram(all_scores, bins=bins)
                                                bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                                                
                                                dist_df = pd.DataFrame({
                                                    'Kho·∫£ng': [f"[{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})" for i in range(len(hist))],
                                                    'S·ªë l∆∞·ª£ng': hist,
                                                    'T·ªâ l·ªá (%)': (hist / len(all_scores) * 100).round(2)
                                                })
                                                st.dataframe(dist_df, use_container_width=True)
                                        
                                        with tab3:
                                            st.markdown("### üîç Chi ti·∫øt Predictions cho m·ªôt User")
                                            
                                            # Ch·ªçn user
                                            selected_user = st.selectbox(
                                                "Ch·ªçn User ƒë·ªÉ xem chi ti·∫øt",
                                                list(result['predictions'].keys()),
                                                key="cbf_user_selector"
                                            )
                                            
                                            if selected_user:
                                                user_predictions = result['predictions'][selected_user]
                                                user_ranking = result['rankings'][selected_user]
                                                
                                                col_detail1, col_detail2 = st.columns(2)
                                                with col_detail1:
                                                    st.metric("T·ªïng s·ªë predictions", len(user_predictions))
                                                    st.metric("Top score", f"{user_ranking[0][1]:.4f}" if user_ranking else "N/A")
                                                    st.metric("Min score", f"{min(user_predictions.values()):.4f}")
                                                with col_detail2:
                                                    st.metric("Max score", f"{max(user_predictions.values()):.4f}")
                                                    st.metric("Mean score", f"{np.mean(list(user_predictions.values())):.4f}")
                                                    st.metric("Median score", f"{np.median(list(user_predictions.values())):.4f}")
                                                
                                                # Hi·ªÉn th·ªã top-K
                                                st.markdown(f"#### Top-{top_k} Recommendations cho User {selected_user}")
                                                top_k_df = pd.DataFrame([
                                                    {
                                                        'Rank': rank + 1,
                                                        'Product ID': product_id,
                                                        'Score': score,
                                                        'Score (Rounded)': f"{score:.4f}"
                                                    }
                                                    for rank, (product_id, score) in enumerate(user_ranking)
                                                ])
                                                st.dataframe(top_k_df, use_container_width=True)
                                                
                                                # Bi·ªÉu ƒë·ªì ƒëi·ªÉm s·ªë top-K
                                                fig = px.bar(
                                                    top_k_df,
                                                    x='Rank',
                                                    y='Score',
                                                    title=f"Top-{top_k} Scores cho User {selected_user}",
                                                    labels={'Rank': 'X·∫øp h·∫°ng', 'Score': 'ƒêi·ªÉm s·ªë (Cosine Similarity)'}
                                                )
                                                st.plotly_chart(fig, use_container_width=True)
                                        
                                        with tab4:
                                            st.markdown("### üß™ Test Set - S·∫£n ph·∫©m ƒë∆∞·ª£c d·ª± ƒëo√°n (ch∆∞a t∆∞∆°ng t√°c)")
                                            st.info("üí° **Test Set** bao g·ªìm t·∫•t c·∫£ c√°c s·∫£n ph·∫©m ch∆∞a ƒë∆∞·ª£c user t∆∞∆°ng t√°c. C√°c s·∫£n ph·∫©m n√†y ƒë∆∞·ª£c bi·∫øn ƒë·ªïi th√†nh vector $\\mathbf{v}_i$ v√† t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi vector h·ªì s∆° ng∆∞·ªùi d√πng $\\mathbf{P}_u$ ƒë·ªÉ d·ª± ƒëo√°n ƒëi·ªÉm t∆∞∆°ng t√°c.")
                                            
                                            # L·∫•y train set t·ª´ pruning result ƒë·ªÉ x√°c ƒë·ªãnh test set
                                            train_interactions_df = pd.DataFrame()
                                            if 'pruned_interactions' in st.session_state:
                                                pruning_result = st.session_state['pruned_interactions']
                                                train_interactions_df = pruning_result.get('pruned_interactions', pd.DataFrame())
                                                
                                                # T√≠nh test set: t·∫•t c·∫£ products - products ƒë√£ t∆∞∆°ng t√°c
                                                all_products_set = set(product_ids)
                                                
                                                # Hi·ªÉn th·ªã th·ªëng k√™ test set
                                                col_test1, col_test2, col_test3 = st.columns(3)
                                                with col_test1:
                                                    st.metric("T·ªïng s·ªë products", len(all_products_set))
                                                    if not train_interactions_df.empty:
                                                        interacted_products = set(train_interactions_df['product_id'].astype(str).unique())
                                                        st.metric("Products ƒë√£ t∆∞∆°ng t√°c (Train)", len(interacted_products))
                                                with col_test2:
                                                    if not train_interactions_df.empty:
                                                        interacted_products = set(train_interactions_df['product_id'].astype(str).unique())
                                                        test_products = all_products_set - interacted_products
                                                        st.metric("Products ch∆∞a t∆∞∆°ng t√°c (Test)", len(test_products))
                                                        st.metric("T·ª∑ l·ªá Test/T·ªïng", f"{len(test_products)/len(all_products_set)*100:.1f}%")
                                                with col_test3:
                                                    st.metric("S·ªë users", len(user_profiles))
                                                    st.metric("T·ªïng predictions", result['stats']['total_predictions'])
                                                
                                                # Hi·ªÉn th·ªã m·∫´u test set (products)
                                                st.markdown("#### üìã M·∫´u Test Set - Products (10 ƒë·∫ßu ti√™n)")
                                                if not train_interactions_df.empty:
                                                    interacted_products = set(train_interactions_df['product_id'].astype(str).unique())
                                                    test_products_list = list(all_products_set - interacted_products)[:10]
                                                else:
                                                    test_products_list = list(all_products_set)[:10]
                                                
                                                test_sample_df = pd.DataFrame({
                                                    'Product ID': test_products_list,
                                                    'Status': 'Ch∆∞a t∆∞∆°ng t√°c (Test Set)'
                                                })
                                                st.dataframe(test_sample_df, use_container_width=True)
                                                
                                                # Hi·ªÉn th·ªã test set cho m·ªôt user c·ª• th·ªÉ
                                                st.markdown("#### üîç Test Set cho m·ªôt User c·ª• th·ªÉ")
                                                sample_test_users = list(result['predictions'].keys())[:10]
                                                selected_test_user = st.selectbox(
                                                    "Ch·ªçn User ƒë·ªÉ xem test set",
                                                    sample_test_users,
                                                    key="test_set_user_selector"
                                                )
                                                
                                                if selected_test_user:
                                                    # L·∫•y products ƒë√£ t∆∞∆°ng t√°c c·ªßa user n√†y (train set)
                                                    user_train_products = set()
                                                    if not train_interactions_df.empty:
                                                        user_train_interactions = train_interactions_df[
                                                            train_interactions_df['user_id'].astype(str) == str(selected_test_user)
                                                        ]
                                                        user_train_products = set(user_train_interactions['product_id'].astype(str).unique())
                                                    
                                                    # Test set = t·∫•t c·∫£ products - products ƒë√£ t∆∞∆°ng t√°c
                                                    user_test_products = all_products_set - user_train_products
                                                    
                                                    col_user_test1, col_user_test2 = st.columns(2)
                                                    with col_user_test1:
                                                        st.metric("Train Set (ƒë√£ t∆∞∆°ng t√°c)", len(user_train_products))
                                                        if user_train_products:
                                                            st.markdown("**M·∫´u products ƒë√£ t∆∞∆°ng t√°c (5 ƒë·∫ßu):**")
                                                            sample_train_products = list(user_train_products)[:5]
                                                            for pid in sample_train_products:
                                                                st.write(f"- {pid}")
                                                    with col_user_test2:
                                                        st.metric("Test Set (ch∆∞a t∆∞∆°ng t√°c)", len(user_test_products))
                                                        st.metric("T·ª∑ l·ªá Test/T·ªïng", f"{len(user_test_products)/len(all_products_set)*100:.1f}%")
                                                    
                                                    # Hi·ªÉn th·ªã top predictions t·ª´ test set
                                                    if selected_test_user in result['rankings']:
                                                        user_ranking = result['rankings'][selected_test_user]
                                                        st.markdown(f"**Top-{min(10, len(user_ranking))} Predictions t·ª´ Test Set:**")
                                                        
                                                        test_ranking_df = pd.DataFrame([
                                                            {
                                                                'Rank': rank + 1,
                                                                'Product ID': product_id,
                                                                'Score': f"{score:.4f}",
                                                                'In Test Set': '‚úÖ' if product_id in user_test_products else '‚ùå'
                                                            }
                                                            for rank, (product_id, score) in enumerate(user_ranking[:10])
                                                        ])
                                                        st.dataframe(test_ranking_df, use_container_width=True)
                                                        
                                                        # Th·ªëng k√™ v·ªÅ test set trong predictions
                                                        test_in_topk = sum(1 for pid, _ in user_ranking[:top_k] if pid in user_test_products)
                                                        st.info(f"üìä Trong Top-{top_k} predictions, c√≥ {test_in_topk} s·∫£n ph·∫©m t·ª´ Test Set ({test_in_topk/top_k*100:.1f}%)")
                                    
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
                else:
                    st.info("üí° Vui l√≤ng ho√†n th√†nh B∆∞·ªõc 2.1 (User Profiles) v√† B∆∞·ªõc 1.3 (Feature Encoding) tr∆∞·ªõc khi ti·∫øp t·ª•c.")
            
            with tab_algorithm:
                st.markdown("""
                **C√¥ng th·ª©c T√≠nh ƒëi·ªÉm (T∆∞∆°ng ƒë·ªìng Cosine):**
                $$\\hat{r}_{ui}^{\\text{CBF}} = \\text{cos}(\\mathbf{P}_u, \\mathbf{v}_i) = \\frac{\\mathbf{P}_u \\cdot \\mathbf{v}_i}{\\|\\mathbf{P}_u\\| \\|\\mathbf{v}_i\\|}$$
                    
                Trong ƒë√≥:
                - $\\hat{r}_{ui}^{\\text{CBF}}$: ƒêi·ªÉm d·ª± ƒëo√°n CBF cho user $u$ v√† item $i$
                - $\\mathbf{P}_u$: Vector h·ªì s∆° ng∆∞·ªùi d√πng $u$
                - $\\mathbf{v}_i$: Item Profile Vector c·ªßa s·∫£n ph·∫©m $i$
                - $\\mathbf{P}_u \\cdot \\mathbf{v}_i$: T√≠ch v√¥ h∆∞·ªõng c·ªßa hai vectors
                - $\\|\\mathbf{P}_u\\|$, $\\|\\mathbf{v}_i\\|$: Chu·∫©n L2 c·ªßa c√°c vectors
                - K·∫øt qu·∫£: ƒêi·ªÉm s·ªë trong kho·∫£ng $[-1, 1]$ (1 = ho√†n to√†n t∆∞∆°ng ƒë·ªìng, -1 = ho√†n to√†n ƒë·ªëi l·∫≠p)

                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - M·ªôt danh s√°ch c√°c s·∫£n ph·∫©m ti·ªÅm nƒÉng ƒë∆∞·ª£c g√°n ƒëi·ªÉm $\\hat{r}_{ui}^{\\text{CBF}} \\in [-1, 1]$
                - ƒêi·ªÉm s·ªë n√†y ph·∫£n √°nh m·ª©c ƒë·ªô ph√π h·ª£p v·ªÅ m·∫∑t thu·ªôc t√≠nh n·ªôi dung gi·ªØa s·∫£n ph·∫©m v√† s·ªü th√≠ch l·ªãch s·ª≠ c·ªßa ng∆∞·ªùi d√πng
                """)
                
                st.markdown("### üßÆ V√≠ d·ª• t√≠nh to√°n")
                st.markdown("""
                **V√≠ d·ª•:** User $u$ c√≥ $\\mathbf{P}_u \\approx [0.89, 1.0, 0.67]$ v√† s·∫£n ph·∫©m $i_{\\text{cand}}$ c√≥ $\\mathbf{v}_{\\text{cand}} = [1, 1, 0]$ (Red, Casual, Women):
                
                **T√≠nh to√°n:**
                - T√≠ch v√¥ h∆∞·ªõng: $\\mathbf{P}_u \\cdot \\mathbf{v}_{\\text{cand}} = (0.89 \\times 1) + (1.0 \\times 1) + (0.67 \\times 0) = 0.89 + 1.0 + 0 = 1.89$
                - Chu·∫©n L2: $\\|\\mathbf{P}_u\\| = \\sqrt{0.89^2 + 1.0^2 + 0.67^2} = \\sqrt{0.7921 + 1.0 + 0.4489} \\approx 1.57$
                - Chu·∫©n L2: $\\|\\mathbf{v}_{\\text{cand}}\\| = \\sqrt{1^2 + 1^2 + 0^2} = \\sqrt{2} \\approx 1.41$
                - ƒêi·ªÉm D·ª± ƒëo√°n: $\\hat{r}_{ui}^{\\text{CBF}} = \\frac{1.89}{1.57 \\times 1.41} \\approx \\frac{1.89}{2.21} \\approx 0.85$
                
                **K·∫øt qu·∫£:** ƒêi·ªÉm s·ªë $0.85$ cho th·∫•y s·∫£n ph·∫©m n√†y c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao v·ªõi s·ªü th√≠ch c·ªßa user (g·∫ßn 1.0 = ho√†n to√†n t∆∞∆°ng ƒë·ªìng).
                """)
                
                st.markdown("""
                **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                - ‚úÖ M·ªôt danh s√°ch c√°c s·∫£n ph·∫©m ti·ªÅm nƒÉng ƒë∆∞·ª£c g√°n ƒëi·ªÉm $\\hat{r}_{ui}^{\\text{CBF}} \\in [-1, 1]$
                - ‚úÖ ƒêi·ªÉm s·ªë n√†y ph·∫£n √°nh m·ª©c ƒë·ªô ph√π h·ª£p v·ªÅ m·∫∑t thu·ªôc t√≠nh n·ªôi dung gi·ªØa s·∫£n ph·∫©m v√† s·ªü th√≠ch l·ªãch s·ª≠ c·ªßa ng∆∞·ªùi d√πng
                - ‚úÖ Top-K rankings cho m·ªói user, s·∫µn s√†ng cho recommendation
                """)

        with st.expander("B∆∞·ªõc 2.3: T·∫°o Danh s√°ch g·ª£i √Ω c√° nh√¢n h√≥a", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Quy tr√¨nh t·∫°o ra danh s√°ch Top-K Personalized d·ª±a tr√™n hai c·∫•p ƒë·ªô l·ªçc c·ª©ng (strict filtering) v√† sau ƒë√≥ l√† ∆∞u ti√™n (prioritization) b·∫±ng ƒëi·ªÉm m√¥ h√¨nh.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 2.2 (CBF Predictions) v√† d·ªØ li·ªáu Products/Users")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_cbf_predictions = 'cbf_predictions' in st.session_state
                has_feature_encoding = 'feature_encoding' in st.session_state

                if not has_cbf_predictions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 2.2 (CBF Predictions). Vui l√≤ng ch·∫°y B∆∞·ªõc 2.2 tr∆∞·ªõc.")
                
                if has_cbf_predictions and apply_personalized_filters is not None:
                    # Load products and users data
                    products_path = os.path.join(current_dir, 'apps', 'exports', 'products.csv')
                    users_path = os.path.join(current_dir, 'apps', 'exports', 'users.csv')
                    
                    products_df = None
                    users_df = None
                    
                    if os.path.exists(products_path):
                        products_df = pd.read_csv(products_path)
                        if 'id' in products_df.columns:
                            products_df['id'] = products_df['id'].astype(str)
                            products_df.set_index('id', inplace=True)
                    else:
                        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file products.csv. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i trong apps/exports/")
                    
                    if os.path.exists(users_path):
                        users_df = pd.read_csv(users_path)
                        if 'id' in users_df.columns:
                            users_df['id'] = users_df['id'].astype(str)
                    else:
                        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file users.csv. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i trong apps/exports/")
                    
                    if products_df is not None:
                        cbf_predictions = st.session_state['cbf_predictions']
                        
                        # C·∫•u h√¨nh
                        col_config1, col_config2 = st.columns(2)
                        with col_config1:
                            selected_user_id = st.selectbox(
                                "Ch·ªçn User ID ƒë·ªÉ √°p d·ª•ng l·ªçc",
                                list(cbf_predictions['predictions'].keys()) if cbf_predictions else [],
                                key="filter_user_id"
                            )
                        
                        with col_config2:
                            payload_articletype = st.selectbox(
                                "Ch·ªçn articleType c·ªßa s·∫£n ph·∫©m ƒë·∫ßu v√†o (payload)",
                                products_df['articleType'].unique().tolist() if 'articleType' in products_df.columns else [],
                                key="payload_articletype"
                            )
                        
                        # Get user info
                        user_age = None
                        user_gender = None
                        if users_df is not None and selected_user_id:
                            user_row = users_df[users_df['id'] == selected_user_id]
                            if not user_row.empty:
                                user_age = user_row.iloc[0].get('age', None)
                                user_gender = user_row.iloc[0].get('gender', None)
                        
                        if selected_user_id and payload_articletype:
                            col_info1, col_info2 = st.columns(2)
                            with col_info1:
                                if user_age is not None:
                                    st.info(f"üë§ User Age: {user_age}")
                                if user_gender is not None:
                                    st.info(f"üë§ User Gender: {user_gender}")
                            with col_info2:
                                st.info(f"üì¶ Payload articleType: {payload_articletype}")
                                if user_age is not None and user_gender is not None:
                                    allowed_genders = get_allowed_genders(user_age, user_gender) if get_allowed_genders else []
                                    st.info(f"‚úÖ Allowed Genders: {', '.join(allowed_genders)}")
                            
                            # Top-K configuration
                            top_k_personalized = st.number_input(
                                "S·ªë l∆∞·ª£ng s·∫£n ph·∫©m Top-K Personalized",
                                min_value=5,
                                max_value=100,
                                value=20,
                                step=5,
                                key="top_k_personalized"
                            )
                            
                            process_button = st.button(
                                "üîß √Åp d·ª•ng Personalized Filters v√† X·∫øp h·∫°ng Top-K",
                                type="primary",
                                use_container_width=True,
                                key="personalized_filter_button"
                            )
                            
                            if process_button:
                                # ƒêo Inference Time (t·ª´ khi nh·∫≠n user ƒë·∫øn khi t·∫°o L(u) - B∆∞·ªõc 2.3)
                                inference_start_time = time.time()
                                
                                with st.spinner("ƒêang √°p d·ª•ng c√°c b·ªô l·ªçc c√° nh√¢n h√≥a v√† x·∫øp h·∫°ng..."):
                                    try:
                                        # L·∫•y danh s√°ch s·∫£n ph·∫©m ·ª©ng vi√™n t·ª´ CBF predictions
                                        user_predictions = cbf_predictions['predictions'][selected_user_id]
                                        candidate_products = list(user_predictions.keys())
                                        
                                        # √Åp d·ª•ng filters v√† x·∫øp h·∫°ng Top-K
                                        result = apply_personalized_filters(
                                            candidate_products,
                                            products_df,
                                            payload_articletype=payload_articletype,
                                            user_age=user_age,
                                            user_gender=user_gender,
                                            cbf_scores=user_predictions,
                                            top_k=top_k_personalized
                                        )
                                        
                                        # K·∫øt th√∫c ƒëo Inference Time
                                        inference_end_time = time.time()
                                        inference_time_measured = inference_end_time - inference_start_time
                                        
                                        st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ l·ªçc danh s√°ch ·ª©ng vi√™n.")
                                        
                                        # L∆∞u v√†o session state
                                        if 'personalized_filters' not in st.session_state:
                                            st.session_state['personalized_filters'] = {}
                                        st.session_state['personalized_filters'][selected_user_id] = result
                                        # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                        save_intermediate_artifact('personalized_filters', st.session_state['personalized_filters'])
                                        
                                        # L∆∞u Inference Time v√†o session state (l·∫•y trung b√¨nh n·∫øu c√≥ nhi·ªÅu users)
                                        if 'inference_times' not in st.session_state:
                                            st.session_state['inference_times'] = []
                                        st.session_state['inference_times'].append(inference_time_measured)
                                        st.session_state['inference_time'] = np.mean(st.session_state['inference_times'])
                                        
                                        # Hi·ªÉn th·ªã th·ªëng k√™
                                        st.markdown("### üìä Th·ªëng k√™ qu√° tr√¨nh l·ªçc")
                                        
                                        stats = result['stats']
                                        col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                                        with col_stat1:
                                            st.metric("Danh s√°ch ban ƒë·∫ßu", f"{stats['initial_count']:,}")
                                        with col_stat2:
                                            st.metric("Sau l·ªçc articleType", f"{stats['after_articletype']:,}")
                                        with col_stat3:
                                            st.metric("Sau l·ªçc Age/Gender", f"{stats['after_age_gender']:,}")
                                        with col_stat4:
                                            st.metric(f"Top-K Personalized ({top_k_personalized})", f"{stats['final_count']:,}")
                                        
                                        # Hi·ªÉn th·ªã Top-K Personalized Rankings
                                        if result.get('ranked_products'):
                                            st.markdown(f"### üìã Danh s√°ch Top-{top_k_personalized} Personalized")
                                            ranked_df = pd.DataFrame([
                                                {
                                                    'Rank': rank + 1,
                                                    'Product ID': product_id,
                                                    'CBF Score': f"{score:.4f}"
                                                }
                                                for rank, (product_id, score) in enumerate(result['ranked_products'])
                                            ])
                                            st.dataframe(ranked_df, use_container_width=True)
                                            
                                            # Bi·ªÉu ƒë·ªì Top-K scores
                                            fig_scores = px.bar(
                                                ranked_df,
                                                x='Rank',
                                                y='CBF Score',
                                                title=f"Top-{top_k_personalized} Personalized Scores",
                                                labels={'Rank': 'X·∫øp h·∫°ng', 'CBF Score': 'ƒêi·ªÉm CBF'}
                                            )
                                            st.plotly_chart(fig_scores, use_container_width=True)
                                        
                                        # Reduction visualization
                                        st.markdown("### üìâ Bi·ªÉu ƒë·ªì gi·∫£m k√≠ch th∆∞·ªõc danh s√°ch")
                                        reduction_df = pd.DataFrame({
                                            'B∆∞·ªõc': ['Ban ƒë·∫ßu', 'Sau articleType', 'Sau Age/Gender', f'Top-{top_k_personalized}'],
                                            'S·ªë l∆∞·ª£ng': [
                                                stats['initial_count'],
                                                stats['after_articletype'],
                                                stats['after_age_gender'],
                                                stats['final_count']
                                            ]
                                        })
                                        
                                        fig = px.bar(
                                            reduction_df,
                                            x='B∆∞·ªõc',
                                            y='S·ªë l∆∞·ª£ng',
                                            title="Qu√° tr√¨nh gi·∫£m k√≠ch th∆∞·ªõc danh s√°ch ·ª©ng vi√™n",
                                            labels={'S·ªë l∆∞·ª£ng': 'S·ªë l∆∞·ª£ng s·∫£n ph·∫©m', 'B∆∞·ªõc': 'B∆∞·ªõc l·ªçc'}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                    except Exception as e:
                                        st.error(f"‚ùå L·ªói khi √°p d·ª•ng personalized filters: {str(e)}")
                                        import traceback
                                        st.code(traceback.format_exc())
                        else:
                            st.info("üí° Vui l√≤ng ch·ªçn User ID v√† articleType ƒë·ªÉ ti·∫øp t·ª•c.")
                    else:
                        st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu products. Vui l√≤ng ki·ªÉm tra l·∫°i.")
                elif apply_personalized_filters is None:
                    st.error(f"‚ùå Kh√¥ng th·ªÉ import cbf_utils module: {_cbf_utils_import_error}")
            
            with tab_algorithm:
                st.markdown("""
                **Quy tr√¨nh l·ªçc v√† x·∫øp h·∫°ng:**
                
                1. **L·ªçc C·ª©ng theo articleType (STRICT):**
                   - Logic: $i_{\\text{cand}} \\in I_{\\text{valid}}$ n·∫øu v√† ch·ªâ n·∫øu $i_{\\text{cand}}.\\text{articleType} = i_{\\text{payload}}.\\text{articleType}$
                   - K·∫øt qu·∫£: Lo·∫°i b·ªè t·∫•t c·∫£ c√°c s·∫£n ph·∫©m kh√¥ng c√πng lo·∫°i v·ªõi s·∫£n ph·∫©m ƒë·∫ßu v√†o
                
                2. **L·ªçc v√† ∆Øu ti√™n theo Gi·ªõi t√≠nh/ƒê·ªô tu·ªïi (Age/Gender Priority):**
                   - **Logic √Åp d·ª•ng (Strict Filtering):**
                     - N·∫øu $u.\\text{age} < 13$ v√† $u.\\text{gender} = \\text{'male'}$: $i_{\\text{cand}}.\\text{gender}$ ph·∫£i l√† $\\text{'Boys'}$
                     - N·∫øu $u.\\text{age} \\ge 13$ v√† $u.\\text{gender} = \\text{'female'}$: $i_{\\text{cand}}.\\text{gender}$ ph·∫£i l√† $\\text{'Women'}$ ho·∫∑c $\\text{'Unisex'}$
                   - **Ph√¢n t√≠ch ∆Øu ti√™n/X·∫øp h·∫°ng:** C√°c s·∫£n ph·∫©m c√≤n l·∫°i sau khi l·ªçc c·ª©ng ƒë∆∞·ª£c x·∫øp h·∫°ng tr·ª±c ti·∫øp b·∫±ng ƒëi·ªÉm m√¥ h√¨nh ($\\hat{r}_{ui}^{\\text{CBF}}$)
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:** Danh s√°ch ·ª©ng vi√™n ƒë∆∞·ª£c l·ªçc ch·ªâ ch·ª©a c√°c s·∫£n ph·∫©m h·ª£p l·ªá v·ªÅ articleType, age, v√† gender. Danh s√°ch n√†y sau ƒë√≥ ƒë∆∞·ª£c x·∫øp h·∫°ng theo ƒëi·ªÉm $\\hat{r}_{ui}^{\\text{CBF}}$ ƒë·ªÉ t·∫°o ra danh s√°ch Top-K Personalized cu·ªëi c√πng.
                """)
                
                st.markdown("### üßÆ V√≠ d·ª• t√≠nh to√°n")
                st.markdown("""
                **V√≠ d·ª•:** User $u$ v·ªõi danh s√°ch ·ª©ng vi√™n ban ƒë·∫ßu:
                
                - **Danh s√°ch ban ƒë·∫ßu:** $N$ s·∫£n ph·∫©m t·ª´ CBF Predictions
                - **Sau L·ªçc C·ª©ng 1 (articleType):** Ch·ªâ gi·ªØ l·∫°i c√°c s·∫£n ph·∫©m c√≥ c√πng articleType v·ªõi payload product
                - **Sau L·ªçc C·ª©ng 2 (Age/Gender):** √Åp d·ª•ng c√°c quy t·∫Øc l·ªçc theo ƒë·ªô tu·ªïi v√† gi·ªõi t√≠nh c·ªßa user
                - **Sau X·∫øp h·∫°ng Top-K:** Ch·ªçn Top-K s·∫£n ph·∫©m c√≥ ƒëi·ªÉm $\\hat{r}_{ui}^{\\text{CBF}}$ cao nh·∫•t
                
                **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                - ‚úÖ Danh s√°ch ·ª©ng vi√™n ƒë∆∞·ª£c l·ªçc ch·ªâ ch·ª©a c√°c s·∫£n ph·∫©m h·ª£p l·ªá v·ªÅ articleType, age, v√† gender
                - ‚úÖ Danh s√°ch ƒë∆∞·ª£c x·∫øp h·∫°ng theo ƒëi·ªÉm $\\hat{r}_{ui}^{\\text{CBF}}$ ƒë·ªÉ t·∫°o ra danh s√°ch Top-K Personalized cu·ªëi c√πng
                - ‚úÖ ƒê·∫£m b·∫£o t√≠nh h·ª£p l·ªá c∆° b·∫£n v√† ƒë·ªô ∆∞u ti√™n c·ªßa c√°c ƒë·ªÅ xu·∫•t
                """)

        with st.expander("B∆∞·ªõc 2.4: T√≠nh to√°n S·ªë li·ªáu (ƒê√°nh gi√° M√¥ h√¨nh)", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** T√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë so s√°nh (Recall@K, NDCG@K,...) tr√™n danh s√°ch Top-K t·ª´ CBF Predictions (B∆∞·ªõc 2.2).")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 2.2 (CBF Predictions) v√† d·ªØ li·ªáu Ground Truth (interactions)")
            st.info("üí° **L∆∞u √Ω:** Metrics ƒë∆∞·ª£c t√≠nh tr√™n CBF Predictions (B∆∞·ªõc 2.2), kh√¥ng ph·∫£i Top-K Personalized (B∆∞·ªõc 2.3) v√¨ ground truth n√™n so s√°nh v·ªõi to√†n b·ªô recommendations, kh√¥ng ch·ªâ ph·∫ßn ƒë√£ l·ªçc.")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_cbf_predictions = 'cbf_predictions' in st.session_state
                has_feature_encoding = 'feature_encoding' in st.session_state
                has_user_profiles = 'user_profiles' in st.session_state

                if not has_cbf_predictions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 2.2 (CBF Predictions). Vui l√≤ng ch·∫°y B∆∞·ªõc 2.2 tr∆∞·ªõc.")
                if not has_feature_encoding:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.3 (Feature Encoding). Vui l√≤ng ch·∫°y B∆∞·ªõc 1.3 tr∆∞·ªõc.")
                
                if has_cbf_predictions and has_feature_encoding and compute_cbf_metrics is not None:
                    cbf_predictions = st.session_state['cbf_predictions']
                    encoding_result = st.session_state.get('feature_encoding', {})
                    
                    encoded_matrix = encoding_result.get('encoded_matrix', None)
                    product_ids = encoding_result.get('product_ids', [])
                    
                    # Load interactions for ground truth
                    interactions_path = os.path.join(current_dir, 'apps', 'exports', 'interactions.csv')
                    interactions_df = None
                    if os.path.exists(interactions_path):
                        interactions_df = pd.read_csv(interactions_path)
                        if 'user_id' in interactions_df.columns:
                            interactions_df['user_id'] = interactions_df['user_id'].astype(str)
                        if 'product_id' in interactions_df.columns:
                            interactions_df['product_id'] = interactions_df['product_id'].astype(str)
                    
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        k_values_input = st.text_input(
                            "C√°c gi√° tr·ªã K (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)",
                            value="10,20",
                            key="k_values_input"
                        )
                        try:
                            k_values = [int(k.strip()) for k in k_values_input.split(',')]
                        except:
                            k_values = [10, 20]
                            st.warning("‚ö†Ô∏è ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh: [10, 20]")
                    
                    with col_config2:
                        # Training Time v√† Inference Time ƒë∆∞·ª£c ƒëo t·ª± ƒë·ªông t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                        # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ th·ªùi gian ƒë√£ ƒëo
                        training_time_auto = st.session_state.get('training_time', None)
                        inference_time_auto = st.session_state.get('inference_time', None)
                        
                        if training_time_auto is not None:
                            st.info(f"‚è±Ô∏è **Training Time (t·ª± ƒë·ªông):** {training_time_auto:.3f}s (ƒëo t·ª´ B∆∞·ªõc 2.1)")
                        else:
                            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ Training Time. Vui l√≤ng ch·∫°y B∆∞·ªõc 2.1 tr∆∞·ªõc.")
                        
                        if inference_time_auto is not None:
                            st.info(f"‚è±Ô∏è **Inference Time (t·ª± ƒë·ªông):** {inference_time_auto:.3f}s (ƒëo t·ª´ B∆∞·ªõc 2.3)")
                        else:
                            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ Inference Time. Vui l√≤ng ch·∫°y B∆∞·ªõc 2.3 tr∆∞·ªõc.")
                        
                        # Cho ph√©p override th·ªß c√¥ng n·∫øu c·∫ßn
                        st.markdown("**Ho·∫∑c nh·∫≠p th·ªß c√¥ng (n·∫øu c·∫ßn):**")
                        training_time_manual = st.number_input(
                            "Training Time (gi√¢y) - Th·ªß c√¥ng",
                            min_value=0.0,
                            value=training_time_auto if training_time_auto is not None else 0.0,
                            step=0.1,
                            key="training_time_input"
                        )
                        
                        inference_time_manual = st.number_input(
                            "Inference Time (gi√¢y) - Th·ªß c√¥ng",
                            min_value=0.0,
                            value=inference_time_auto if inference_time_auto is not None else 0.0,
                            step=0.1,
                            key="inference_time_input"
                        )
                    
                    process_button = st.button(
                        "üîß T√≠nh to√°n Evaluation Metrics",
                        type="primary",
                        use_container_width=True,
                        key="evaluation_metrics_button"
                    )
                    
                    if process_button:
                        with st.spinner("ƒêang t√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°..."):
                            try:
                                # L·∫•y d·ªØ li·ªáu t·ª´ B∆∞·ªõc 2.2 (CBF Predictions) - TR∆Ø·ªöC KHI l·ªçc
                                cbf_predictions = st.session_state['cbf_predictions']
                                encoding_result = st.session_state['feature_encoding']
                                
                                encoded_matrix = encoding_result['encoded_matrix']
                                product_ids = encoding_result['product_ids']
                                predictions_dict = {}
                                for user_id, user_ranking in cbf_predictions['rankings'].items():
                                    user_id_str = str(user_id)
                                    
                                    ranked_products = [(str(pid), score) for pid, score in user_ranking]
                                    predictions_dict[user_id_str] = ranked_products
                                final_training_time = training_time_manual if training_time_manual > 0 else training_time_auto
                                final_inference_time = inference_time_manual if inference_time_manual > 0 else inference_time_auto
                                ground_truth_dict = {}
                                
                                # T·∫£i products ƒë·ªÉ ki·ªÉm tra articleType c·ªßa c√°c items li√™n quan
                                products_path = os.path.join(current_dir, 'apps', 'exports', 'products.csv')
                                products_df_for_gt = None
                                if os.path.exists(products_path):
                                    products_df_for_gt = pd.read_csv(products_path)
                                    if 'id' in products_df_for_gt.columns:
                                        products_df_for_gt['id'] = products_df_for_gt['id'].astype(str)
                                        products_df_for_gt.set_index('id', inplace=True)
                                
                                if interactions_df is not None and 'user_id' in interactions_df.columns and 'product_id' in interactions_df.columns:
                                    # Chu·∫©n h√≥a user_id v√† product_id v·ªÅ string
                                    interactions_df['user_id'] = interactions_df['user_id'].astype(str)
                                    interactions_df['product_id'] = interactions_df['product_id'].astype(str)
                                    
                                    # Consider only positive interactions (purchase, like, cart)
                                    positive_interactions = interactions_df[
                                        interactions_df['interaction_type'].isin(['purchase', 'like', 'cart'])
                                    ] if 'interaction_type' in interactions_df.columns else interactions_df
                                    
                                    for user_id in predictions_dict.keys():
                                        # ƒê·∫£m b·∫£o user_id l√† string
                                        user_id_str = str(user_id)
                                        
                                        user_interactions = positive_interactions[
                                            positive_interactions['user_id'] == user_id_str
                                        ]
                                        if not user_interactions.empty:
                                            # L·∫•y t·∫•t c·∫£ c√°c items li√™n quan t·ª´ interactions g·ªëc
                                            relevant_items_all = set(user_interactions['product_id'].astype(str).unique())
                                            ground_truth_dict[user_id_str] = relevant_items_all
                                        else:
                                            ground_truth_dict[user_id_str] = set()
                                else:
                                    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu interactions ƒë·ªÉ l√†m ground truth. S·ª≠ d·ª•ng empty sets.")
                                    for user_id in predictions_dict.keys():
                                        ground_truth_dict[str(user_id)] = set()
                                
                                # Get all items for coverage
                                all_items = set(product_ids) if product_ids else set()
                                
                                # Compute metrics
                                result = compute_cbf_metrics(
                                    predictions_dict,
                                    ground_truth_dict,
                                    k_values=k_values,
                                    item_features=encoded_matrix,
                                    item_ids=product_ids,
                                    all_items=all_items,
                                    training_time=final_training_time,
                                    inference_time=final_inference_time,
                                    use_ild=True  # S·ª≠ d·ª•ng ILD@K cho Diversity
                                )
                                
                                st.success("‚úÖ **Ho√†n th√†nh!** ƒê√£ t√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë ƒë√°nh gi√°.")
                                
                                # Store in session state
                                st.session_state['cbf_evaluation_metrics'] = result
                                # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                save_intermediate_artifact('cbf_evaluation_metrics', result)
                                # L∆∞u timing metrics
                                if 'training_time' in st.session_state:
                                    save_intermediate_artifact('training_time', st.session_state['training_time'])
                                if 'inference_time' in st.session_state:
                                    save_intermediate_artifact('inference_time', st.session_state['inference_time'])
                                
                                # Display results
                                st.markdown("### üìä K·∫øt qu·∫£ Evaluation Metrics")
                                
                                # Hi·ªÉn th·ªã th√¥ng tin Train/Test Split
                                st.markdown("### üéì Train/Test Set Split")
                                col_split1, col_split2, col_split3 = st.columns(3)
                                
                                # T√≠nh train set v√† test set
                                if interactions_df is not None and 'user_id' in interactions_df.columns and 'product_id' in interactions_df.columns:
                                    positive_interactions = interactions_df[
                                        interactions_df['interaction_type'].isin(['purchase', 'like', 'cart'])
                                    ] if 'interaction_type' in interactions_df.columns else interactions_df
                                    
                                    total_interactions = len(positive_interactions)
                                    total_users = positive_interactions['user_id'].nunique()
                                    total_products = positive_interactions['product_id'].nunique()
                                    
                                    # Train set: interactions ƒë√£ d√πng ƒë·ªÉ x√¢y d·ª±ng user profiles
                                    train_interactions_count = 0
                                    if 'pruned_interactions' in st.session_state:
                                        train_interactions_count = len(st.session_state['pruned_interactions'].get('pruned_interactions', pd.DataFrame()))
                                    
                                    # Test set: c√°c products ƒë∆∞·ª£c d·ª± ƒëo√°n (ground truth)
                                    test_products_count = len(ground_truth_dict)
                                    total_test_items = sum(len(items) for items in ground_truth_dict.values())
                                    
                                    with col_split1:
                                        st.markdown("#### üéì Train Set")
                                        st.metric("Interactions", f"{train_interactions_count:,}")
                                        st.metric("Users", total_users)
                                        st.caption("D√πng ƒë·ªÉ x√¢y d·ª±ng User Profiles")
                                    
                                    with col_split2:
                                        st.markdown("#### üß™ Test Set")
                                        st.metric("Users c√≥ ground truth", test_products_count)
                                        st.metric("T·ªïng relevant items", f"{total_test_items:,}")
                                        st.caption("D√πng ƒë·ªÉ ƒë√°nh gi√° predictions")
                                    
                                    with col_split3:
                                        st.markdown("#### üìä T·ªïng quan")
                                        st.metric("T·ªïng interactions", f"{total_interactions:,}")
                                        st.metric("T·ªïng products", total_products)
                                        if train_interactions_count > 0:
                                            test_ratio = (total_test_items / train_interactions_count * 100) if train_interactions_count > 0 else 0
                                            st.metric("Test/Train ratio", f"{test_ratio:.1f}%")
                                
                                # Create metrics table
                                metrics_data = []
                                for k in k_values:
                                    metrics_data.append({
                                        'K': k,
                                        'Recall@K': f"{result['recall'].get(k, 0.0):.4f}",
                                        'Precision@K': f"{result['precision'].get(k, 0.0):.4f}",
                                        'NDCG@K': f"{result['ndcg'].get(k, 0.0):.4f}"
                                    })
                                
                                metrics_df = pd.DataFrame(metrics_data)
                                st.dataframe(metrics_df, use_container_width=True)
                                
                                # Other metrics
                                col_other1, col_other2, col_other3, col_other4 = st.columns(4)
                                with col_other1:
                                    st.metric("Diversity (ILD@K)", f"{result['diversity']:.4f}" if result['diversity'] is not None else "N/A")
                                    st.caption("Intra-List Diversity")
                                with col_other2:
                                    st.metric("Coverage", f"{result['coverage']:.4f}" if result['coverage'] is not None else "N/A")
                                    st.caption("T·ª∑ l·ªá items ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t")
                                with col_other3:
                                    st.metric("Training Time", f"{result['training_time']:.2f}s" if result['training_time'] is not None else "N/A")
                                    st.caption("B∆∞·ªõc 2.1 ‚Üí 2.2")
                                with col_other4:
                                    st.metric("Inference Time", f"{result['inference_time']:.2f}s" if result['inference_time'] is not None else "N/A")
                                    st.caption("User ‚Üí L(u) (B∆∞·ªõc 2.3)")
                                
                                # Visualization
                                st.markdown("### üìà Bi·ªÉu ƒë·ªì Metrics theo K")
                                
                                fig = go.Figure()
                                fig.add_trace(go.Scatter(
                                    x=k_values,
                                    y=[result['recall'].get(k, 0.0) for k in k_values],
                                    mode='lines+markers',
                                    name='Recall@K',
                                    line=dict(color='blue', width=2)
                                ))
                                fig.add_trace(go.Scatter(
                                    x=k_values,
                                    y=[result['precision'].get(k, 0.0) for k in k_values],
                                    mode='lines+markers',
                                    name='Precision@K',
                                    line=dict(color='green', width=2)
                                ))
                                fig.add_trace(go.Scatter(
                                    x=k_values,
                                    y=[result['ndcg'].get(k, 0.0) for k in k_values],
                                    mode='lines+markers',
                                    name='NDCG@K',
                                    line=dict(color='red', width=2)
                                ))
                                fig.update_layout(
                                    title="Metrics theo K",
                                    xaxis_title="K",
                                    yaxis_title="Score",
                                    hovermode='x unified'
                                )
                                st.plotly_chart(fig, use_container_width=True)
                                
                                # Summary table for export
                                st.markdown("### üìã B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë (Export)")
                                summary_data = {
                                    'Model': ['CBF']
                                }
                                
                                # Th√™m c√°c metrics theo K values
                                for k in k_values:
                                    summary_data[f'Recall@{k}'] = [f"{result['recall'].get(k, 0.0):.4f}"]
                                    summary_data[f'Precision@{k}'] = [f"{result['precision'].get(k, 0.0):.4f}"]
                                    summary_data[f'NDCG@{k}'] = [f"{result['ndcg'].get(k, 0.0):.4f}"]
                                
                                # Th√™m c√°c metrics kh√°c
                                summary_data['Diversity (ILD@K)'] = [f"{result['diversity']:.4f}" if result['diversity'] is not None else "N/A"]
                                summary_data['Coverage'] = [f"{result['coverage']:.4f}" if result['coverage'] is not None else "N/A"]
                                summary_data['Training Time (s)'] = [f"{result['training_time']:.3f}" if result['training_time'] is not None else "N/A"]
                                summary_data['Inference Time (s)'] = [f"{result['inference_time']:.3f}" if result['inference_time'] is not None else "N/A"]
                                summary_df = pd.DataFrame(summary_data)
                                st.dataframe(summary_df, use_container_width=True)
                                
                                st.markdown("""
                                **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                - ‚úÖ M·ªôt h√†ng d·ªØ li·ªáu ho√†n ch·ªânh trong B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë
                                - ‚úÖ Th·ªÉ hi·ªán hi·ªáu su·∫•t c∆° s·ªü c·ªßa m√¥ h√¨nh Content-based Filtering
                                - ‚úÖ S·∫µn s√†ng ƒë·ªÉ so s√°nh v·ªõi c√°c m√¥ h√¨nh kh√°c (GNN, Hybrid)
                                """)
                            
                            except Exception as e:
                                st.error(f"‚ùå L·ªói khi t√≠nh to√°n evaluation metrics: {str(e)}")
                                import traceback
                                st.code(traceback.format_exc())
                elif compute_cbf_metrics is None:
                    st.error(f"‚ùå Kh√¥ng th·ªÉ import evaluation_metrics module: {_evaluation_import_error}")
            
            with tab_algorithm:
                st.markdown("**B·∫£ng ch·ªâ s·ªë ƒë√°nh gi√°:**")

                st.markdown("- **Training Time (s)**: ƒêo th·ªùi gian t·ª´ B∆∞·ªõc 2.1 ƒë·∫øn 2.2 (x√¢y d·ª±ng $\\mathbf{P}_u$).")
                st.markdown("- **Inference Time (s)**: ƒêo th·ªùi gian t·ª´ khi nh·∫≠n $u$ ƒë·∫øn khi t·∫°o $L(u)$ cu·ªëi c√πng (B∆∞·ªõc 2.3).")

                st.markdown("- **Recall@K** (K = 5, 10, 20) ‚Äì C√¥ng th·ª©c:")
                st.latex(r"\text{Recall}@K = \frac{|\text{Relevant}(u) \cap L(u)|}{|\text{Relevant}(u)|}")

                st.markdown("- **Precision@K** (K = 5, 10, 20) ‚Äì C√¥ng th·ª©c:")
                st.latex(r"\text{Precision}@K = \frac{|\text{Relevant}(u) \cap L(u)|}{K}")

                st.markdown("- **NDCG@K** (K = 5, 10, 20) ‚Äì C√¥ng th·ª©c:")
                st.latex(r"\text{NDCG}@K = \frac{\text{DCG}@K}{\text{IDCG}@K}")
                st.latex(r"\text{DCG}@K = \sum_{i=1}^{K} \frac{2^{\text{rel}(i)} - 1}{\log_2(i+1)}")

                st.markdown("- **Diversity (ILD@K)** ‚Äì C√¥ng th·ª©c:")
                st.latex(r"\text{ILD}@K = \frac{2}{K(K-1)} \sum_{i \in L(u)} \sum_{j \in L(u),\, j>i} \left(1 - \text{cos}(\mathbf{v}_i, \mathbf{v}_j)\right)")

                st.markdown("- **Coverage** ‚Äì C√¥ng th·ª©c:")
                st.latex(r"\text{Coverage} = \frac{|\{i \in I \mid i \in L(u) \text{ cho √≠t nh·∫•t m·ªôt user } u\}|}{|I|}")

                st.markdown("**K·∫øt qu·∫£ mong ƒë·ª£i:** M·ªôt h√†ng d·ªØ li·ªáu ho√†n ch·ªânh (cho CBF) trong B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë, th·ªÉ hi·ªán hi·ªáu su·∫•t c∆° s·ªü c·ªßa m√¥ h√¨nh Content-based Filtering.")

        # PH·∫¶N III: M√î H√åNH M·∫†NG NEURAL ƒê·ªí TH·ªä (GNN)
        st.markdown('<div class="sub-header">üìö PH·∫¶N III: M√î H√åNH M·∫†NG NEURAL ƒê·ªí TH·ªä (GNN)</div>', unsafe_allow_html=True)
        st.markdown("")

        with st.expander("B∆∞·ªõc 3.1: X√¢y d·ª±ng ƒê·ªì th·ªã v√† Kh·ªüi t·∫°o Nh√∫ng", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** X√¢y d·ª±ng ƒë·ªì th·ªã hai ph√≠a $G=(U, I, \\mathcal{E})$ v√† kh·ªüi t·∫°o ng·∫´u nhi√™n c√°c vector nh√∫ng $\\mathbf{e}_u^{(0)}$ v√† $\\mathbf{e}_i^{(0)}$.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 1.2 (Pruned Interactions)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_pruned_interactions = 'pruned_interactions' in st.session_state

                if not has_pruned_interactions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.2 (Pruning). Vui l√≤ng ch·∫°y B∆∞·ªõc 1.2 tr∆∞·ªõc.")
                else:
                    pruning_result = st.session_state['pruned_interactions']
                    pruned_interactions_df = pruning_result['pruned_interactions']
                    
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        embedding_dim = st.number_input(
                            "K√≠ch th∆∞·ªõc nh√∫ng (embedding_dim)",
                            min_value=16,
                            max_value=256,
                            value=64,
                            step=16,
                            key="gnn_embedding_dim"
                        )
                    
                    with col_config2:
                        st.write("")  # Kho·∫£ng tr·ªëng
                        process_button = st.button(
                            "üîß X√¢y d·ª±ng ƒê·ªì th·ªã v√† Kh·ªüi t·∫°o Nh√∫ng",
                            type="primary",
                            use_container_width=True,
                            key="gnn_graph_construction_button"
                        )
                    
                    if process_button:
                        if build_graph is None:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ import gnn_utils module: {_gnn_utils_import_error}")
                            st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/gnn_utils.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                        else:
                            with st.spinner("ƒêang x√¢y d·ª±ng ƒë·ªì th·ªã v√† kh·ªüi t·∫°o nh√∫ng..."):
                                try:
                                    # X√¢y d·ª±ng ƒë·ªì th·ªã
                                    graph_result = build_graph(pruned_interactions_df, embedding_dim)
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['gnn_graph'] = graph_result
                                    # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                    save_intermediate_artifact('gnn_graph', graph_result)
                                    
                                    st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ x√¢y d·ª±ng ƒë·ªì th·ªã v·ªõi {graph_result['num_users']} users v√† {graph_result['num_products']} products.")
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    st.markdown("### üìä Th·ªëng k√™ ƒê·ªì th·ªã")
                                    
                                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                                    with col_stat1:
                                        st.metric("S·ªë l∆∞·ª£ng Users", graph_result['num_users'])
                                        st.metric("S·ªë l∆∞·ª£ng Products", graph_result['num_products'])
                                    with col_stat2:
                                        st.metric("S·ªë l∆∞·ª£ng Edges", graph_result['num_edges'])
                                        st.metric("K√≠ch th∆∞·ªõc nh√∫ng", f"{embedding_dim}D")
                                    with col_stat3:
                                        density = (2 * graph_result['num_edges']) / (graph_result['num_users'] * graph_result['num_products']) if (graph_result['num_users'] * graph_result['num_products']) > 0 else 0
                                        st.metric("M·∫≠t ƒë·ªô ƒë·ªì th·ªã", f"{density:.6f}")
                                    
                                    # Hi·ªÉn th·ªã c√°c nh√∫ng m·∫´u
                                    st.markdown("### üî¢ M·∫´u Vector Nh√∫ng Ban ƒë·∫ßu")
                                    
                                    if 'user_embeddings' in graph_result and 'product_embeddings' in graph_result:
                                        sample_user_emb = graph_result['user_embeddings'][:3] if len(graph_result['user_embeddings']) >= 3 else graph_result['user_embeddings']
                                        sample_product_emb = graph_result['product_embeddings'][:3] if len(graph_result['product_embeddings']) >= 3 else graph_result['product_embeddings']
                                        
                                        col_emb1, col_emb2 = st.columns(2)
                                        with col_emb1:
                                            st.write("**Sample User Embeddings (3 users ƒë·∫ßu ti√™n):**")
                                            user_emb_df = pd.DataFrame(
                                                sample_user_emb,
                                                index=[f"User {i+1}" for i in range(len(sample_user_emb))],
                                                columns=[f"Dim {j+1}" for j in range(embedding_dim)]
                                            )
                                            st.dataframe(user_emb_df, use_container_width=True)
                                        
                                        with col_emb2:
                                            st.write("**Sample Product Embeddings (3 products ƒë·∫ßu ti√™n):**")
                                            product_emb_df = pd.DataFrame(
                                                sample_product_emb,
                                                index=[f"Product {i+1}" for i in range(len(sample_product_emb))],
                                                columns=[f"Dim {j+1}" for j in range(embedding_dim)]
                                            )
                                            st.dataframe(product_emb_df, use_container_width=True)
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ ƒê·ªì th·ªã $G=(U, I, \\mathcal{E})$ ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ interactions ƒë√£ l√†m s·∫°ch
                                    - ‚úÖ C√°c vector nh√∫ng ban ƒë·∫ßu $\\mathbf{e}_u^{(0)}$ v√† $\\mathbf{e}_i^{(0)}$ ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n
                                    - ‚úÖ S·∫µn s√†ng cho qu√° tr√¨nh lan truy·ªÅn th√¥ng ƒëi·ªáp (Message Propagation)
                                    """)
                                
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi x√¢y d·ª±ng ƒë·ªì th·ªã: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **C·∫•u tr√∫c ƒë·ªì th·ªã:**
                - **ƒê·ªì th·ªã hai ph√≠a (Bipartite Graph):** $G=(U, I, \\mathcal{E})$
                  - $U$: T·∫≠p h·ª£p c√°c nodes ng∆∞·ªùi d√πng
                  - $I$: T·∫≠p h·ª£p c√°c nodes s·∫£n ph·∫©m
                  - $\\mathcal{E}$: T·∫≠p h·ª£p c√°c c·∫°nh (edges) bi·ªÉu di·ªÖn t∆∞∆°ng t√°c gi·ªØa users v√† products
                
                **Kh·ªüi t·∫°o nh√∫ng:**
                - **User Embeddings:** $\\mathbf{e}_u^{(0)} \\in \\mathbb{R}^d$ - Vector nh√∫ng ban ƒë·∫ßu cho m·ªói user $u$
                - **Item Embeddings:** $\\mathbf{e}_i^{(0)} \\in \\mathbb{R}^d$ - Vector nh√∫ng ban ƒë·∫ßu cho m·ªói item $i$
                - **Ph∆∞∆°ng ph√°p kh·ªüi t·∫°o:** Xavier Uniform Initialization
                - **K√≠ch th∆∞·ªõc nh√∫ng:** $d$ (embedding_dim, m·∫∑c ƒë·ªãnh: 64)
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - ƒê·ªì th·ªã $G$ ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ interactions ƒë√£ l√†m s·∫°ch
                - C√°c vector nh√∫ng ban ƒë·∫ßu $\\mathbf{e}_u^{(0)}$ v√† $\\mathbf{e}_i^{(0)}$ ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n
                - S·∫µn s√†ng cho qu√° tr√¨nh lan truy·ªÅn th√¥ng ƒëi·ªáp (Message Propagation)
                """)

        with st.expander("B∆∞·ªõc 3.2: C∆° ch·∫ø Lan truy·ªÅn Th√¥ng ƒëi·ªáp (Message Propagation)", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Lan truy·ªÅn th√¥ng ƒëi·ªáp qua $L$ l·ªõp ƒë·ªÉ c·∫≠p nh·∫≠t nh√∫ng $\\mathbf{e}_u^{(l)}$ v√† $\\mathbf{e}_i^{(l)}$.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 3.1 (Graph Construction)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_gnn_graph = 'gnn_graph' in st.session_state

                if not has_gnn_graph:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 3.1 (Graph Construction). Vui l√≤ng ch·∫°y B∆∞·ªõc 3.1 tr∆∞·ªõc.")
                else:
                    graph_result = st.session_state['gnn_graph']
                    
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        num_layers = st.number_input(
                            "S·ªë l·ªõp lan truy·ªÅn (num_layers)",
                            min_value=1,
                            max_value=10,
                            value=3,
                            step=1,
                            key="gnn_num_layers"
                        )
                    
                    with col_config2:
                        st.write("")  # Kho·∫£ng tr·ªëng
                        process_button = st.button(
                            "üîß Th·ª±c hi·ªán Message Propagation",
                            type="primary",
                            use_container_width=True,
                            key="gnn_message_propagation_button"
                        )
                    
                    if process_button:
                        if message_propagation is None:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ import gnn_utils module: {_gnn_utils_import_error}")
                            st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/gnn_utils.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                        else:
                            with st.spinner("ƒêang th·ª±c hi·ªán lan truy·ªÅn th√¥ng ƒëi·ªáp..."):
                                try:
                                    # Th·ª±c hi·ªán lan truy·ªÅn th√¥ng ƒëi·ªáp
                                    propagation_result = message_propagation(graph_result, num_layers)
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['gnn_propagation'] = propagation_result
                                    # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                    save_intermediate_artifact('gnn_propagation', propagation_result)
                                    
                                    st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ th·ª±c hi·ªán lan truy·ªÅn qua {num_layers} l·ªõp.")
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    st.markdown("### üìä Th·ªëng k√™ Message Propagation")
                                    
                                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                                    with col_stat1:
                                        st.metric("S·ªë l·ªõp", num_layers)
                                        st.metric("K√≠ch th∆∞·ªõc nh√∫ng", f"{graph_result['embedding_dim']}D")
                                    with col_stat2:
                                        if 'final_user_embeddings' in propagation_result:
                                            st.metric("User Embeddings Shape", f"{propagation_result['final_user_embeddings'].shape}")
                                        if 'final_product_embeddings' in propagation_result:
                                            st.metric("Product Embeddings Shape", f"{propagation_result['final_product_embeddings'].shape}")
                                    with col_stat3:
                                        if 'layer_stats' in propagation_result:
                                            st.metric("L·ªõp ƒë√£ x·ª≠ l√Ω", len(propagation_result['layer_stats']))
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™ theo t·ª´ng l·ªõp
                                    if 'layer_stats' in propagation_result:
                                        st.markdown("### üìà Th·ªëng k√™ theo t·ª´ng l·ªõp")
                                        layer_stats_df = pd.DataFrame(propagation_result['layer_stats'])
                                        st.dataframe(layer_stats_df, use_container_width=True)
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ C√°c vector nh√∫ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t qua $L$ l·ªõp
                                    - ‚úÖ Nh√∫ng cu·ªëi c√πng $\\mathbf{e}_u^{(L)}$ v√† $\\mathbf{e}_i^{(L)}$ ph·∫£n √°nh c·∫•u tr√∫c ƒë·ªì th·ªã v√† t∆∞∆°ng t√°c
                                    - ‚úÖ S·∫µn s√†ng cho qu√° tr√¨nh d·ª± ƒëo√°n v√† x·∫øp h·∫°ng
                                    """)
                                
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi th·ª±c hi·ªán message propagation: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **C√¥ng th·ª©c C·∫≠p nh·∫≠t Nh√∫ng:**
                $$\\mathbf{e}_u^{(l)} = \\text{LeakyReLU} \\left( \\mathbf{W}_1 \\mathbf{e}_u^{(l-1)} + \\sum_{i \\in N_u} M_{u \\leftarrow i} \\right)$$
                
                Trong ƒë√≥:
                - $\\mathbf{e}_u^{(l)}$: Vector nh√∫ng c·ªßa user $u$ ·ªü l·ªõp $l$
                - $\\mathbf{W}_1$: Ma tr·∫≠n tr·ªçng s·ªë h·ªçc ƒë∆∞·ª£c
                - $N_u$: T·∫≠p h·ª£p c√°c items m√† user $u$ ƒë√£ t∆∞∆°ng t√°c (neighbors)
                - $M_{u \\leftarrow i}$: Th√¥ng ƒëi·ªáp t·ª´ item $i$ ƒë·∫øn user $u$
                - $\\text{LeakyReLU}$: H√†m k√≠ch ho·∫°t
                
                **Qu√° tr√¨nh lan truy·ªÅn:**
                1. **L·ªõp 0:** S·ª≠ d·ª•ng nh√∫ng ban ƒë·∫ßu $\\mathbf{e}_u^{(0)}$ v√† $\\mathbf{e}_i^{(0)}$
                2. **L·ªõp 1 ƒë·∫øn L:** C·∫≠p nh·∫≠t nh√∫ng d·ª±a tr√™n th√¥ng ƒëi·ªáp t·ª´ neighbors
                3. **Normalization:** Chu·∫©n h√≥a theo degree c·ªßa nodes ƒë·ªÉ ·ªïn ƒë·ªãnh training
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - C√°c vector nh√∫ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t qua $L$ l·ªõp
                - Nh√∫ng cu·ªëi c√πng $\\mathbf{e}_u^{(L)}$ v√† $\\mathbf{e}_i^{(L)}$ ph·∫£n √°nh c·∫•u tr√∫c ƒë·ªì th·ªã v√† t∆∞∆°ng t√°c
                """)

        with st.expander("B∆∞·ªõc 3.3: D·ª± ƒëo√°n v√† X·∫øp h·∫°ng", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** T·ªïng h·ª£p nh√∫ng cu·ªëi c√πng $\\mathbf{E}_u^*, \\mathbf{E}_i^*$. T√≠nh ƒëi·ªÉm d·ª± ƒëo√°n $\\hat{r}_{ui}^{\\text{GNN}} = \\mathbf{E}_u^* \\cdot \\mathbf{E}_i^*$.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 3.2 (Message Propagation)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_gnn_propagation = 'gnn_propagation' in st.session_state

                if not has_gnn_propagation:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 3.2 (Message Propagation). Vui l√≤ng ch·∫°y B∆∞·ªõc 3.2 tr∆∞·ªõc.")
                else:
                    propagation_result = st.session_state['gnn_propagation']
                    
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        top_k = st.number_input(
                            "S·ªë l∆∞·ª£ng s·∫£n ph·∫©m Top-K ƒë·ªÉ x·∫øp h·∫°ng",
                            min_value=5,
                            max_value=100,
                            value=20,
                            step=5,
                            key="gnn_top_k"
                        )
                    
                    with col_config2:
                        st.write("")  # Kho·∫£ng tr·ªëng
                        process_button = st.button(
                            "üîß T√≠nh ƒêi·ªÉm D·ª± ƒëo√°n v√† X·∫øp h·∫°ng",
                            type="primary",
                            use_container_width=True,
                            key="gnn_predictions_button"
                        )
                    
                    if process_button:
                        if compute_gnn_predictions is None:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ import gnn_utils module: {_gnn_utils_import_error}")
                            st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/gnn_utils.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                        else:
                            with st.spinner("ƒêang t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n v√† x·∫øp h·∫°ng..."):
                                try:
                                    # T√≠nh to√°n d·ª± ƒëo√°n
                                    predictions_result = compute_gnn_predictions(propagation_result, top_k)
                                    
                                    # L∆∞u v√†o session state & l∆∞u ra artifacts
                                    st.session_state['gnn_predictions'] = predictions_result
                                    save_predictions_artifact("gnn", predictions_result)
                                    
                                    st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n cho {predictions_result['stats']['total_users']} users.")
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    st.markdown("### üìä Th·ªëng k√™ Predictions")
                                    
                                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                                    with col_stat1:
                                        st.metric("T·ªïng s·ªë predictions", f"{predictions_result['stats']['total_predictions']:,}")
                                        st.metric("S·ªë users", predictions_result['stats']['total_users'])
                                    with col_stat2:
                                        st.metric("S·ªë products", predictions_result['stats']['total_products'])
                                        st.metric("Top-K", top_k)
                                    with col_stat3:
                                        st.metric("Min score", f"{predictions_result['stats']['min_score']:.4f}")
                                        st.metric("Max score", f"{predictions_result['stats']['max_score']:.4f}")
                                        st.metric("Mean score", f"{predictions_result['stats']['mean_score']:.4f}")
                                    
                                    # Display sample rankings
                                    st.markdown(f"### üìã M·∫´u Rankings Top-{top_k} (5 users ƒë·∫ßu ti√™n)")
                                    
                                    if 'rankings' in predictions_result:
                                        sample_users = list(predictions_result['rankings'].keys())[:5]
                                        
                                        for idx, user_id in enumerate(sample_users, 1):
                                            ranking = predictions_result['rankings'][user_id]
                                            
                                            with st.expander(f"User {user_id} - Top {len(ranking)} s·∫£n ph·∫©m", expanded=False):
                                                ranking_df = pd.DataFrame([
                                                    {
                                                        'Rank': rank + 1,
                                                        'Product ID': product_id,
                                                        'Score': f"{score:.4f}"
                                                    }
                                                    for rank, (product_id, score) in enumerate(ranking)
                                                ])
                                                st.dataframe(ranking_df, use_container_width=True)
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ ƒêi·ªÉm d·ª± ƒëo√°n $\\hat{r}_{ui}^{\\text{GNN}}$ cho t·∫•t c·∫£ user-item pairs
                                    - ‚úÖ Top-K rankings cho m·ªói user
                                    - ‚úÖ S·∫µn s√†ng cho qu√° tr√¨nh hu·∫•n luy·ªán ho·∫∑c ƒë√°nh gi√°
                                    """)
                                
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **C√¥ng th·ª©c D·ª± ƒëo√°n:**
                $$\\hat{r}_{ui}^{\\text{GNN}} = \\mathbf{E}_u^* \\cdot \\mathbf{E}_i^*$$
                
                Trong ƒë√≥:
                - $\\mathbf{E}_u^*$: Vector nh√∫ng cu·ªëi c√πng c·ªßa user $u$ sau $L$ l·ªõp lan truy·ªÅn
                - $\\mathbf{E}_i^*$: Vector nh√∫ng cu·ªëi c√πng c·ªßa item $i$ sau $L$ l·ªõp lan truy·ªÅn
                - $\\hat{r}_{ui}^{\\text{GNN}}$: ƒêi·ªÉm d·ª± ƒëo√°n GNN cho user $u$ v√† item $i$
                
                **Qu√° tr√¨nh:**
                1. L·∫•y nh√∫ng cu·ªëi c√πng t·ª´ B∆∞·ªõc 3.2
                2. T√≠nh t√≠ch v√¥ h∆∞·ªõng gi·ªØa user embedding v√† product embedding
                3. X·∫øp h·∫°ng c√°c s·∫£n ph·∫©m theo ƒëi·ªÉm d·ª± ƒëo√°n gi·∫£m d·∫ßn
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - ƒêi·ªÉm d·ª± ƒëo√°n $\\hat{r}_{ui}^{\\text{GNN}}$ cho t·∫•t c·∫£ user-item pairs
                - Top-K rankings cho m·ªói user
                """)

        with st.expander("B∆∞·ªõc 3.4: Hu·∫•n luy·ªán M√¥ h√¨nh: T·ªëi ∆∞u h√≥a b·∫±ng BPR Loss", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng c√°ch t·ªëi ∆∞u h√≥a tr·ª±c ti·∫øp th·ª© h·∫°ng.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 3.2 (Message Propagation) v√† B∆∞·ªõc 1.2 (Pruned Interactions)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                has_gnn_propagation = 'gnn_propagation' in st.session_state
                has_pruned_interactions = 'pruned_interactions' in st.session_state

                if not has_gnn_propagation:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 3.2 (Message Propagation). Vui l√≤ng ch·∫°y B∆∞·ªõc 3.2 tr∆∞·ªõc.")
                if not has_pruned_interactions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.2 (Pruning). Vui l√≤ng ch·∫°y B∆∞·ªõc 1.2 tr∆∞·ªõc.")
                
                if has_gnn_propagation and has_pruned_interactions:
                    propagation_result = st.session_state['gnn_propagation']
                    pruning_result = st.session_state['pruned_interactions']
                    pruned_interactions_df = pruning_result['pruned_interactions']
                    
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        num_epochs = st.number_input(
                            "S·ªë epochs",
                            min_value=1,
                            max_value=100,
                            value=10,
                            step=1,
                            key="gnn_num_epochs"
                        )
                        
                        learning_rate = st.number_input(
                            "Learning Rate",
                            min_value=0.0001,
                            max_value=0.1,
                            value=0.001,
                            step=0.0001,
                            format="%.4f",
                            key="gnn_learning_rate"
                        )
                    
                    with col_config2:
                        reg_weight = st.number_input(
                            "Regularization Weight (Œª)",
                            min_value=0.0,
                            max_value=0.01,
                            value=0.0001,
                            step=0.0001,
                            format="%.4f",
                            key="gnn_reg_weight"
                        )
                        
                        batch_size = st.number_input(
                            "Batch Size",
                            min_value=32,
                            max_value=1024,
                            value=256,
                            step=32,
                            key="gnn_batch_size"
                        )
                    
                    process_button = st.button(
                        "üîß Hu·∫•n luy·ªán M√¥ h√¨nh (BPR Loss)",
                        type="primary",
                        use_container_width=True,
                        key="gnn_training_button"
                    )

                    if process_button:
                        if train_gnn_model is None:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ import gnn_utils module: {_gnn_utils_import_error}")
                            st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/gnn_utils.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                        else:
                            # ƒêo Training Time (B∆∞·ªõc 3.2 ƒë·∫øn 3.4)
                            training_start_time = time.time()
                            
                            with st.spinner(f"ƒêang hu·∫•n luy·ªán m√¥ h√¨nh qua {num_epochs} epochs (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)..."):
                                try:
                                    # Hu·∫•n luy·ªán m√¥ h√¨nh
                                    training_result = train_gnn_model(
                                        propagation_result,
                                        pruned_interactions_df,
                                        num_epochs=num_epochs,
                                        learning_rate=learning_rate,
                                        reg_weight=reg_weight,
                                        batch_size=batch_size
                                    )
                                    
                                    # K·∫øt th√∫c ƒëo Training Time
                                    training_end_time = time.time()
                                    training_time_measured = training_end_time - training_start_time
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['gnn_training'] = training_result
                                    st.session_state['gnn_training_time'] = training_time_measured
                                    # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                    save_intermediate_artifact('gnn_training', training_result)
                                    
                                    st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ hu·∫•n luy·ªán m√¥ h√¨nh qua {num_epochs} epochs.")
                                    
                                    # Debug th√™m ƒë·ªÉ ki·ªÉm tra nguy√™n nh√¢n BPR Loss lu√¥n l√† 0.0000
                                    with st.expander("üîç Debug Training Result (GNN BPR Loss)", expanded=False):
                                        st.markdown("**Raw `training_result` t·ª´ `train_gnn_model`:**")
                                        try:
                                            st.json(training_result)
                                        except Exception:
                                            st.write(training_result)
                                        
                                        if isinstance(training_result, dict):
                                            initial_loss_val = training_result.get('initial_loss', None)
                                            final_loss_val = training_result.get('final_loss', None)
                                            loss_history_val = training_result.get('loss_history', None)
                                            
                                            if (initial_loss_val in [0, 0.0, None]) and (final_loss_val in [0, 0.0, None]):
                                                st.warning(
                                                    "‚ö†Ô∏è `initial_loss` v√†/ho·∫∑c `final_loss` ƒëang l√† 0.\n\n"
                                                    "- N·∫øu ƒë·ªìng th·ªùi `loss_history` r·ªóng v√† trong k·∫øt qu·∫£ c√≥ kh√≥a "
                                                    "`warning` gi·ªëng nh∆∞: **\"No positive pairs found for training. "
                                                    "Using embeddings from propagation only.\"** th√¨ m√¥ h√¨nh **kh√¥ng "
                                                    "th·ª±c s·ª± train**, m√† ch·ªâ d√πng embeddings t·ª´ b∆∞·ªõc message propagation.\n"
                                                    "- Nguy√™n nh√¢n th∆∞·ªùng l√† **kh√¥ng t·∫°o ƒë∆∞·ª£c positive pair (u, i, j)** "
                                                    "t·ª´ `pruned_interactions_df` trong `train_gnn_model` "
                                                    "(v√≠ d·ª• do d·ªØ li·ªáu qu√° √≠t, ho·∫∑c logic l·ªçc triplet qu√° ch·∫∑t).\n"
                                                    "- Khi ƒë√≥ c√°c th·ªëng k√™ BPR Loss ·ªü UI s·∫Ω hi·ªÉn th·ªã 0.0000 l√† ƒë√∫ng v·ªõi "
                                                    "k·∫øt qu·∫£ hi·ªán t·∫°i (kh√¥ng c√≥ b∆∞·ªõc t·ªëi ∆∞u h√≥a)."
                                                )
                                            
                                            if isinstance(loss_history_val, (list, tuple)) and loss_history_val:
                                                st.write("**Sample `loss_history` (5 gi√° tr·ªã ƒë·∫ßu ti√™n):**", loss_history_val[:5])
                                            else:
                                                st.warning("‚ö†Ô∏è `loss_history` r·ªóng ho·∫∑c kh√¥ng t·ªìn t·∫°i ‚Äì ƒë√¢y c≈©ng c√≥ th·ªÉ l√† nguy√™n nh√¢n c√°c s·ªë li·ªáu hi·ªÉn th·ªã l√† 0.0000.")

                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    st.markdown("### üìä Th·ªëng k√™ Hu·∫•n luy·ªán")
                                    
                                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                                    with col_stat1:
                                        st.metric("S·ªë epochs", num_epochs)
                                        st.metric("Training Time", f"{training_time_measured:.2f}s")
                                    with col_stat2:
                                        warning_msg = training_result.get('warning') if isinstance(training_result, dict) else None
                                        if warning_msg and "No positive pairs found for training" in str(warning_msg):
                                            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m ƒë∆∞·ª£c positive pairs ƒë·ªÉ train BPR. "
                                                       "M√¥ h√¨nh ch·ªâ d√πng embeddings t·ª´ propagation, kh√¥ng c√≥ b∆∞·ªõc t·ªëi ∆∞u h√≥a BPR.")
                                            st.metric("Final BPR Loss", "N/A")
                                            st.metric("Initial BPR Loss", "N/A")
                                        else:
                                            if 'final_loss' in training_result:
                                                st.metric("Final BPR Loss", f"{training_result['final_loss']:.4f}")
                                            if 'initial_loss' in training_result:
                                                st.metric("Initial BPR Loss", f"{training_result['initial_loss']:.4f}")
                                    with col_stat3:
                                        if warning_msg and "No positive pairs found for training" in str(warning_msg):
                                            st.metric("Loss Reduction", "N/A")
                                        elif 'final_loss' in training_result and 'initial_loss' in training_result:
                                            loss_reduction = training_result['initial_loss'] - training_result['final_loss']
                                            st.metric("Loss Reduction", f"{loss_reduction:.4f}")
                                    
                                    # Hi·ªÉn th·ªã l·ªãch s·ª≠ hu·∫•n luy·ªán
                                    if 'loss_history' in training_result:
                                        st.markdown("### üìà L·ªãch s·ª≠ BPR Loss qua c√°c Epochs")
                                        
                                        loss_history_df = pd.DataFrame({
                                            'Epoch': range(1, len(training_result['loss_history']) + 1),
                                            'BPR Loss': training_result['loss_history']
                                        })
                                        
                                        fig = px.line(
                                            loss_history_df,
                                            x='Epoch',
                                            y='BPR Loss',
                                            title="BPR Loss qua c√°c Epochs",
                                            labels={'BPR Loss': 'BPR Loss', 'Epoch': 'Epoch'}
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ Gi√° tr·ªã $L_{BPR}$ gi·∫£m d·∫ßn v√† h·ªôi t·ª•
                                    - ‚úÖ T·ªëi ∆∞u h√≥a c√°c vector nh√∫ng ($\\Theta$)
                                    - ‚úÖ M√¥ h√¨nh h·ªçc ƒë∆∞·ª£c patterns t·ª´ ƒë·ªì th·ªã t∆∞∆°ng t√°c
                                    """)
                                
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **C√¥ng th·ª©c BPR Loss:**
                $$L_{BPR} = - \\sum_{(u, i, j) \\in D_S} \\ln \\sigma(\\hat{r}_{ui} - \\hat{r}_{uj}) + \\lambda ||\\Theta||^2$$
                
                Trong ƒë√≥:
                - $D_S$: T·∫≠p h·ª£p c√°c triplets $(u, i, j)$ v·ªõi $i$ l√† positive item v√† $j$ l√† negative item
                - $\\hat{r}_{ui}$: ƒêi·ªÉm d·ª± ƒëo√°n cho positive pair $(u, i)$
                - $\\hat{r}_{uj}$: ƒêi·ªÉm d·ª± ƒëo√°n cho negative pair $(u, j)$
                - $\\sigma$: H√†m sigmoid
                - $\\lambda$: H·ªá s·ªë regularization
                - $||\\Theta||^2$: L2 regularization c·ªßa c√°c tham s·ªë m√¥ h√¨nh
                
                **Qu√° tr√¨nh hu·∫•n luy·ªán:**
                1. **Sampling:** T·∫°o c√°c triplets $(u, i, j)$ t·ª´ interactions
                2. **Forward Pass:** T√≠nh $\\hat{r}_{ui}$ v√† $\\hat{r}_{uj}$
                3. **Loss Calculation:** T√≠nh $L_{BPR}$
                4. **Backward Pass:** C·∫≠p nh·∫≠t tham s·ªë $\\Theta$ b·∫±ng gradient descent
                5. **L·∫∑p l·∫°i** qua c√°c epochs cho ƒë·∫øn khi h·ªôi t·ª•
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - Gi√° tr·ªã $L_{BPR}$ gi·∫£m d·∫ßn v√† h·ªôi t·ª•
                - T·ªëi ∆∞u h√≥a c√°c vector nh√∫ng ($\\Theta$)
                - M√¥ h√¨nh h·ªçc ƒë∆∞·ª£c patterns t·ª´ ƒë·ªì th·ªã t∆∞∆°ng t√°c
                """)

        with st.expander("B∆∞·ªõc 3.5: T·∫°o Danh s√°ch g·ª£i √Ω c√° nh√¢n h√≥a v√† T√≠nh to√°n S·ªë li·ªáu (ƒê√°nh gi√° M√¥ h√¨nh)", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:**")
            st.write("1. **G·ª£i √Ω C√° nh√¢n h√≥a:** √Åp d·ª•ng Logic L·ªçc v√† ∆Øu ti√™n (B∆∞·ªõc 2.3) l√™n danh s√°ch ·ª©ng vi√™n ƒë∆∞·ª£c x·∫øp h·∫°ng b·ªüi $\\hat{r}_{ui}^{\\text{GNN}}$.")
            st.write("2. **T√≠nh to√°n S·ªë li·ªáu:** T√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë (Recall@K, NDCG@K,...) t∆∞∆°ng t·ª± nh∆∞ B∆∞·ªõc 2.4, s·ª≠ d·ª•ng $L(u)$ v√† c√°c tham s·ªë th·ªùi gian t∆∞∆°ng ·ª©ng c·ªßa GNN.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 3.3 (GNN Predictions) ho·∫∑c B∆∞·ªõc 3.4 (Trained Model)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_gnn_predictions = 'gnn_predictions' in st.session_state
                has_gnn_training = 'gnn_training' in st.session_state

                if not has_gnn_predictions and not has_gnn_training:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 3.3 (GNN Predictions) ho·∫∑c B∆∞·ªõc 3.4 (Trained Model). Vui l√≤ng ch·∫°y m·ªôt trong hai b∆∞·ªõc tr∆∞·ªõc.")
                else:
                    if has_gnn_training:
                        gnn_predictions = st.session_state['gnn_training']
                    elif has_gnn_predictions:
                        gnn_predictions = st.session_state['gnn_predictions']
                    else:
                        gnn_predictions = None
                    
                    # Ki·ªÉm tra format
                    if gnn_predictions is not None:
                        if not isinstance(gnn_predictions, dict):
                            st.error(f"‚ùå **L·ªói:** gnn_predictions kh√¥ng ph·∫£i l√† dictionary. Type: {type(gnn_predictions)}")
                            st.write(f"Value: {gnn_predictions}")
                            gnn_predictions = None
                        elif len(gnn_predictions) == 0:
                            st.warning("‚ö†Ô∏è **C·∫£nh b√°o:** gnn_predictions l√† dictionary r·ªóng. Vui l√≤ng ch·∫°y l·∫°i B∆∞·ªõc 3.3 ho·∫∑c 3.4.")
                            gnn_predictions = None
                    
                    has_feature_encoding = 'feature_encoding' in st.session_state
                    if not has_feature_encoding:
                        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.3 (Feature Encoding). C·∫ßn cho t√≠nh to√°n Diversity.")
                    
                    if gnn_predictions is not None:
                        encoding_result = st.session_state.get('feature_encoding', {})
                        encoded_matrix = encoding_result.get('encoded_matrix', None)
                        product_ids = encoding_result.get('product_ids', [])
                        
                        # Load interactions for ground truth
                        interactions_path = os.path.join(current_dir, 'apps', 'exports', 'interactions.csv')
                        interactions_df = None
                        if os.path.exists(interactions_path):
                            interactions_df = pd.read_csv(interactions_path)
                            if 'user_id' in interactions_df.columns:
                                interactions_df['user_id'] = interactions_df['user_id'].astype(str)
                            if 'product_id' in interactions_df.columns:
                                interactions_df['product_id'] = interactions_df['product_id'].astype(str)
                        
                        # C·∫•u h√¨nh
                        col_config1, col_config2 = st.columns(2)
                        with col_config1:
                            k_values_input = st.text_input(
                                "C√°c gi√° tr·ªã K (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)",
                                value="10,20",
                                key="gnn_k_values_input"
                            )
                            try:
                                k_values = [int(k.strip()) for k in k_values_input.split(',')]
                            except:
                                k_values = [10, 20]
                                st.warning("‚ö†Ô∏è ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh: [10, 20]")
                        
                        with col_config2:
                            # Training Time v√† Inference Time ƒë∆∞·ª£c ƒëo t·ª± ƒë·ªông t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                            training_time_auto = st.session_state.get('gnn_training_time', None)
                            inference_time_auto = st.session_state.get('gnn_inference_time', None)
                            
                            if training_time_auto is not None:
                                st.info(f"‚è±Ô∏è **Training Time (t·ª± ƒë·ªông):** {training_time_auto:.3f}s (ƒëo t·ª´ B∆∞·ªõc 3.2-3.4)")
                            else:
                                st.warning("‚ö†Ô∏è Ch∆∞a c√≥ Training Time. Vui l√≤ng ch·∫°y B∆∞·ªõc 3.4 tr∆∞·ªõc.")
                            
                            # Cho ph√©p override th·ªß c√¥ng n·∫øu c·∫ßn
                            st.markdown("**Ho·∫∑c nh·∫≠p th·ªß c√¥ng (n·∫øu c·∫ßn):**")
                            training_time_manual = st.number_input(
                                "Training Time (gi√¢y) - Th·ªß c√¥ng",
                                min_value=0.0,
                                value=training_time_auto if training_time_auto is not None else 0.0,
                                step=0.1,
                                key="gnn_training_time_input"
                            )
                            
                            inference_time_manual = st.number_input(
                                "Inference Time (gi√¢y) - Th·ªß c√¥ng",
                                min_value=0.0,
                                value=inference_time_auto if inference_time_auto is not None else 0.0,
                                step=0.1,
                                key="gnn_inference_time_input"
                            )
                        
                        process_button = st.button(
                            "üîß T√≠nh to√°n Evaluation Metrics",
                            type="primary",
                            use_container_width=True,
                            key="gnn_evaluation_metrics_button"
                        )
                        
                        if process_button:
                            # ƒêo Inference Time
                            inference_start_time = time.time()
                            
                            with st.spinner("ƒêang t√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°..."):
                                try:
                                    # Chu·∫©n b·ªã ƒë·ªãnh d·∫°ng d·ª± ƒëo√°n t·ª´ GNN Predictions
                                    predictions_dict = {}
                                    
                                    if 'rankings' in gnn_predictions:
                                        for user_id, user_ranking in gnn_predictions['rankings'].items():
                                            user_id_str = str(user_id)
                                            # X·ª≠ l√Ω c·∫£ ƒë·ªãnh d·∫°ng tuple v√† kh√¥ng ph·∫£i tuple
                                            if user_ranking and len(user_ranking) > 0:
                                                if isinstance(user_ranking[0], tuple):
                                                    ranked_products = [(str(pid), score) for pid, score in user_ranking]
                                                else:
                                                    # N·∫øu l√† dict, chuy·ªÉn ƒë·ªïi th√†nh danh s√°ch c√°c tuple
                                                    if isinstance(user_ranking, dict):
                                                        ranked_products = [(str(pid), score) for pid, score in user_ranking.items()]
                                                    else:
                                                        ranked_products = [(str(item), 0.0) for item in user_ranking]
                                                predictions_dict[user_id_str] = ranked_products
                                    elif 'predictions' in gnn_predictions:
                                        # Chuy·ªÉn ƒë·ªïi dict d·ª± ƒëo√°n sang ƒë·ªãnh d·∫°ng x·∫øp h·∫°ng
                                        user_predictions_dict = gnn_predictions['predictions']
                                        if isinstance(user_predictions_dict, dict) and len(user_predictions_dict) > 0:
                                            # L·∫•y top_k t·ª´ k_values (s·ª≠ d·ª•ng k l·ªõn nh·∫•t)
                                            max_k = max(k_values) if k_values else 20
                                            
                                            for user_id, user_preds in user_predictions_dict.items():
                                                user_id_str = str(user_id)
                                                if isinstance(user_preds, dict) and len(user_preds) > 0:
                                                    ranked_products = sorted(
                                                        [(str(pid), score) for pid, score in user_preds.items()],
                                                        key=lambda x: x[1],
                                                        reverse=True
                                                    )[:max_k]  # Gi·ªõi h·∫°n ƒë·∫øn max_k
                                                    predictions_dict[user_id_str] = ranked_products
                                        else:
                                            st.warning(f"‚ö†Ô∏è 'predictions' key t·ªìn t·∫°i nh∆∞ng kh√¥ng ph·∫£i dict ho·∫∑c r·ªóng. Type: {type(user_predictions_dict)}, Length: {len(user_predictions_dict) if isinstance(user_predictions_dict, dict) else 'N/A'}")
                                    else:
                                        st.error("‚ùå GNN predictions kh√¥ng c√≥ c·∫£ 'rankings' v√† 'predictions' keys!")
                                        st.write(f"Available keys: {list(gnn_predictions.keys()) if isinstance(gnn_predictions, dict) else 'N/A'}")
                                    
                                    final_training_time = training_time_manual if training_time_manual > 0 else training_time_auto
                                    
                                    ground_truth_dict = {}
                                    
                                    if interactions_df is not None and 'user_id' in interactions_df.columns and 'product_id' in interactions_df.columns:
                                        positive_interactions = interactions_df[
                                            interactions_df['interaction_type'].isin(['purchase', 'like', 'cart'])
                                        ] if 'interaction_type' in interactions_df.columns else interactions_df
                                        
                                        for user_id in predictions_dict.keys():
                                            user_id_str = str(user_id)
                                            user_interactions = positive_interactions[
                                                positive_interactions['user_id'] == user_id_str
                                            ]
                                            if not user_interactions.empty:
                                                relevant_items = set(user_interactions['product_id'].astype(str).unique())
                                                ground_truth_dict[user_id_str] = relevant_items
                                            else:
                                                ground_truth_dict[user_id_str] = set()
                                    else:
                                        st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu interactions ƒë·ªÉ l√†m ground truth. S·ª≠ d·ª•ng empty sets.")
                                        for user_id in predictions_dict.keys():
                                            ground_truth_dict[str(user_id)] = set()
                                    
                                    # Get all items for coverage
                                    all_items = set(product_ids) if product_ids else set()
                                    
                                    # K·∫øt th√∫c ƒëo Inference Time
                                    inference_end_time = time.time()
                                    inference_time_measured = inference_end_time - inference_start_time
                                    # S·ª≠ d·ª•ng inference time ƒë√£ ƒëo ho·∫∑c th·ªß c√¥ng
                                    final_inference_time = inference_time_manual if inference_time_manual > 0 else inference_time_measured
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['gnn_inference_time'] = inference_time_measured
                                    
                                    # Compute metrics
                                    if compute_cbf_metrics is not None:
                                        result = compute_cbf_metrics(
                                            predictions_dict,
                                            ground_truth_dict,
                                            k_values=k_values,
                                            item_features=encoded_matrix,
                                            item_ids=product_ids,
                                            all_items=all_items,
                                            training_time=final_training_time,
                                            inference_time=final_inference_time,
                                            use_ild=True
                                        )
                                        
                                        st.success("‚úÖ **Ho√†n th√†nh!** ƒê√£ t√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë ƒë√°nh gi√°.")
                                        
                                        # L∆∞u v√†o session state
                                        st.session_state['gnn_evaluation_metrics'] = result
                                        # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                        save_intermediate_artifact('gnn_evaluation_metrics', result)
                                        # L∆∞u timing metrics
                                        if 'gnn_training_time' in st.session_state:
                                            save_intermediate_artifact('gnn_training_time', st.session_state['gnn_training_time'])
                                        if 'gnn_inference_time' in st.session_state:
                                            save_intermediate_artifact('gnn_inference_time', st.session_state['gnn_inference_time'])
                                        
                                        # Display results (similar to Step 2.5)
                                        st.markdown("### üìä K·∫øt qu·∫£ Evaluation Metrics")
                                        
                                        # Create metrics table
                                        metrics_data = []
                                        for k in k_values:
                                            metrics_data.append({
                                                'K': k,
                                                'Recall@K': f"{result['recall'].get(k, 0.0):.4f}",
                                                'Precision@K': f"{result['precision'].get(k, 0.0):.4f}",
                                                'NDCG@K': f"{result['ndcg'].get(k, 0.0):.4f}"
                                            })
                                        
                                        metrics_df = pd.DataFrame(metrics_data)
                                        st.dataframe(metrics_df, use_container_width=True)
                                        
                                        # Other metrics
                                        col_other1, col_other2, col_other3, col_other4 = st.columns(4)
                                        with col_other1:
                                            st.metric("Diversity (ILD@K)", f"{result['diversity']:.4f}" if result['diversity'] is not None else "N/A")
                                        with col_other2:
                                            st.metric("Coverage", f"{result['coverage']:.4f}" if result['coverage'] is not None else "N/A")
                                        with col_other3:
                                            st.metric("Training Time", f"{result['training_time']:.2f}s" if result['training_time'] is not None else "N/A")
                                        with col_other4:
                                            st.metric("Inference Time", f"{result['inference_time']:.2f}s" if result['inference_time'] is not None else "N/A")
                                        
                                        # Visualization
                                        st.markdown("### üìà Bi·ªÉu ƒë·ªì Metrics theo K")
                                        
                                        fig = go.Figure()
                                        fig.add_trace(go.Scatter(
                                            x=k_values,
                                            y=[result['recall'].get(k, 0.0) for k in k_values],
                                            mode='lines+markers',
                                            name='Recall@K',
                                            line=dict(color='blue', width=2)
                                        ))
                                        fig.add_trace(go.Scatter(
                                            x=k_values,
                                            y=[result['precision'].get(k, 0.0) for k in k_values],
                                            mode='lines+markers',
                                            name='Precision@K',
                                            line=dict(color='green', width=2)
                                        ))
                                        fig.add_trace(go.Scatter(
                                            x=k_values,
                                            y=[result['ndcg'].get(k, 0.0) for k in k_values],
                                            mode='lines+markers',
                                            name='NDCG@K',
                                            line=dict(color='red', width=2)
                                        ))
                                        fig.update_layout(
                                            title="Metrics theo K (GNN)",
                                            xaxis_title="K",
                                            yaxis_title="Score",
                                            hovermode='x unified'
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                        
                                        # Summary table for export
                                        st.markdown("### üìã B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë (Export)")
                                        summary_data = {
                                            'Model': ['GNN']
                                        }
                                        
                                        # Th√™m c√°c metrics theo K values
                                        for k in k_values:
                                            summary_data[f'Recall@{k}'] = [f"{result['recall'].get(k, 0.0):.4f}"]
                                            summary_data[f'Precision@{k}'] = [f"{result['precision'].get(k, 0.0):.4f}"]
                                            summary_data[f'NDCG@{k}'] = [f"{result['ndcg'].get(k, 0.0):.4f}"]
                                        
                                        # Th√™m c√°c metrics kh√°c
                                        summary_data['Diversity (ILD@K)'] = [f"{result['diversity']:.4f}" if result['diversity'] is not None else "N/A"]
                                        summary_data['Coverage'] = [f"{result['coverage']:.4f}" if result['coverage'] is not None else "N/A"]
                                        summary_data['Training Time (s)'] = [f"{result['training_time']:.3f}" if result['training_time'] is not None else "N/A"]
                                        summary_data['Inference Time (s)'] = [f"{result['inference_time']:.3f}" if result['inference_time'] is not None else "N/A"]
                                        summary_df = pd.DataFrame(summary_data)
                                        st.dataframe(summary_df, use_container_width=True)
                                        
                                        st.markdown("""
                                        **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                        - ‚úÖ M·ªôt h√†ng d·ªØ li·ªáu ho√†n ch·ªânh trong B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë cho GNN
                                        - ‚úÖ Th·ªÉ hi·ªán hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh GNN
                                        - ‚úÖ S·∫µn s√†ng ƒë·ªÉ so s√°nh v·ªõi c√°c m√¥ h√¨nh kh√°c (CBF, Hybrid)
                                        """)
                                    else:
                                        st.error("‚ùå Kh√¥ng th·ªÉ import evaluation_metrics module.")
                                
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi t√≠nh to√°n evaluation metrics: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **D·ªØ li·ªáu ƒê·∫ßu v√†o (ƒê∆∞·ª£c l·∫•y t·ª´):**
                - **Training Time (s):** ƒêo th·ªùi gian t·ª´ B∆∞·ªõc 3.2 ƒë·∫øn 3.4 (qu√° tr√¨nh l·∫∑p l·∫°i BPR Loss qua c√°c epoch).
                - **Inference Time (s):** ƒêo th·ªùi gian cho qu√° tr√¨nh t√≠nh to√°n $\\hat{r}_{ui}^{\\text{GNN}}$ v√† h·∫≠u x·ª≠ l√Ω (B∆∞·ªõc 3.5).
                - **ILD, NDCG, Recall, Precision:** D·ªØ li·ªáu t∆∞∆°ng t·ª± B∆∞·ªõc 2.4, nh∆∞ng s·ª≠ d·ª•ng $L(u)$ ƒë∆∞·ª£c t·∫°o t·ª´ $\\hat{r}_{ui}^{\\text{GNN}}$.
                
                **C√°c ch·ªâ s·ªë ƒë√°nh gi√°:** T∆∞∆°ng t·ª± nh∆∞ B∆∞·ªõc 2.4 v·ªõi c√°c c√¥ng th·ª©c:
                - **Recall@K**, **Precision@K**, **NDCG@K**
                - **Diversity (ILD@K)**
                - **Coverage**
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:** M·ªôt h√†ng d·ªØ li·ªáu ho√†n ch·ªânh trong B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë cho GNN, th·ªÉ hi·ªán hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh GNN v√† s·∫µn s√†ng ƒë·ªÉ so s√°nh v·ªõi c√°c m√¥ h√¨nh kh√°c (CBF, Hybrid).
                """)
        st.markdown('<div class="sub-header">üìö PH·∫¶N IV: M√î H√åNH K·∫æT H·ª¢P (HYBRID GNN + CONTENT-BASED)</div>', unsafe_allow_html=True)
        st.markdown("")

        with st.expander("B∆∞·ªõc 4.1 & 4.2: H·ª£p nh·∫•t ƒêi·ªÉm s·ªë Tuy·∫øn t√≠nh", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** K·∫øt h·ª£p tuy·∫øn t√≠nh ƒëi·ªÉm d·ª± ƒëo√°n ƒë√£ chu·∫©n h√≥a c·ªßa GNN ($\\hat{r}_{ui}^{\\text{GNN}}$ t·ª´ Ph·∫ßn III) v√† CBF ($\\hat{r}_{ui}^{\\text{CBF}}$ t·ª´ Ph·∫ßn II).")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 2.2 (CBF Predictions) v√† B∆∞·ªõc 3.3/3.4 (GNN Predictions)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_cbf_predictions = 'cbf_predictions' in st.session_state
                has_gnn_predictions = 'gnn_predictions' in st.session_state
                has_gnn_training = 'gnn_training' in st.session_state

                if not has_cbf_predictions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 2.2 (CBF Predictions). Vui l√≤ng ch·∫°y B∆∞·ªõc 2.2 tr∆∞·ªõc.")
                if not has_gnn_predictions and not has_gnn_training:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 3.3 (GNN Predictions) ho·∫∑c B∆∞·ªõc 3.4 (Trained Model). Vui l√≤ng ch·∫°y m·ªôt trong hai b∆∞·ªõc tr∆∞·ªõc.")
                
                if has_cbf_predictions and (has_gnn_predictions or has_gnn_training):
                    # Get GNN predictions
                    if has_gnn_training:
                        gnn_predictions = st.session_state['gnn_training']
                    elif has_gnn_predictions:
                        gnn_predictions = st.session_state['gnn_predictions']
                    
                    cbf_predictions = st.session_state['cbf_predictions']
                    
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        alpha = st.slider(
                            "Tr·ªçng s·ªë k·∫øt h·ª£p (Œ±)",
                            min_value=0.0,
                            max_value=1.0,
                            value=0.5,
                            step=0.1,
                            key="hybrid_alpha"
                        )
                        st.info(f"**Œ± = {alpha}:** {alpha*100:.0f}% GNN + {(1-alpha)*100:.0f}% CBF")
                    
                    with col_config2:
                        top_k = st.number_input(
                            "S·ªë l∆∞·ª£ng s·∫£n ph·∫©m Top-K ƒë·ªÉ x·∫øp h·∫°ng",
                            min_value=5,
                            max_value=100,
                            value=20,
                            step=5,
                            key="hybrid_top_k"
                        )
                    
                    process_button = st.button(
                        "üîß H·ª£p nh·∫•t ƒêi·ªÉm s·ªë Hybrid",
                        type="primary",
                        use_container_width=True,
                        key="hybrid_combine_button"
                    )
                    
                    if process_button:
                        if combine_hybrid_scores is None:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ import hybrid_utils module: {_hybrid_utils_import_error}")
                            st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/hybrid_utils.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
                        else:
                            with st.spinner("ƒêang h·ª£p nh·∫•t ƒëi·ªÉm s·ªë GNN v√† CBF..."):
                                try:
                                    # Combine scores
                                    hybrid_result = combine_hybrid_scores(cbf_predictions, gnn_predictions, alpha, top_k)
                                    
                                    # L∆∞u v√†o session state & l∆∞u ra artifacts
                                    st.session_state['hybrid_predictions'] = hybrid_result
                                    save_predictions_artifact("hybrid", hybrid_result)
                                    
                                    st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ h·ª£p nh·∫•t ƒëi·ªÉm s·ªë cho {hybrid_result['stats']['total_users']} users.")
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    st.markdown("### üìä Th·ªëng k√™ Hybrid Predictions")
                                    
                                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                                    with col_stat1:
                                        st.metric("T·ªïng s·ªë users", hybrid_result['stats']['total_users'])
                                        st.metric("Tr·ªçng s·ªë Œ±", f"{alpha:.2f}")
                                    with col_stat2:
                                        st.metric("CBF Score Range", f"[{hybrid_result['stats']['cbf_min']:.4f}, {hybrid_result['stats']['cbf_max']:.4f}]")
                                    with col_stat3:
                                        st.metric("GNN Score Range", f"[{hybrid_result['stats']['gnn_min']:.4f}, {hybrid_result['stats']['gnn_max']:.4f}]")
                                    
                                    # Display sample rankings
                                    st.markdown(f"### üìã M·∫´u Rankings Top-{top_k} (5 users ƒë·∫ßu ti√™n)")
                                    
                                    if 'rankings' in hybrid_result:
                                        sample_users = list(hybrid_result['rankings'].keys())[:5]
                                        
                                        for idx, user_id in enumerate(sample_users, 1):
                                            ranking = hybrid_result['rankings'][user_id]
                                            
                                            with st.expander(f"User {user_id} - Top {len(ranking)} s·∫£n ph·∫©m", expanded=False):
                                                ranking_df = pd.DataFrame([
                                                    {
                                                        'Rank': rank + 1,
                                                        'Product ID': product_id,
                                                        'Hybrid Score': f"{score:.4f}"
                                                    }
                                                    for rank, (product_id, score) in enumerate(ranking)
                                                ])
                                                st.dataframe(ranking_df, use_container_width=True)
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ ƒêi·ªÉm $Score_{Hybrid}(u, i)$ k·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ GNN v√† CBF
                                    - ‚úÖ Top-K rankings cho m·ªói user
                                    - ‚úÖ S·∫µn s√†ng cho qu√° tr√¨nh g·ª£i √Ω c√° nh√¢n h√≥a v√† ƒë√°nh gi√°
                                    """)
                                
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói khi h·ª£p nh·∫•t ƒëi·ªÉm s·ªë: {str(e)}")
                                    import traceback
                                    st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **C√¥ng th·ª©c T√≠nh ƒëi·ªÉm Hybrid:**
                $$Score_{Hybrid}(u, i) = \\alpha \\cdot \\hat{r}_{ui}^{\\text{GNN}} + (1 - \\alpha) \\cdot \\hat{r}_{ui}^{\\text{CBF}}$$
                
                Trong ƒë√≥:
                - $\\hat{r}_{ui}^{\\text{GNN}}$: ƒêi·ªÉm d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh GNN (ƒë√£ chu·∫©n h√≥a v·ªÅ [0, 1])
                - $\\hat{r}_{ui}^{\\text{CBF}}$: ƒêi·ªÉm d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh CBF (ƒë√£ chu·∫©n h√≥a v·ªÅ [0, 1])
                - $\\alpha$: Tr·ªçng s·ªë k·∫øt h·ª£p (0 ‚â§ Œ± ‚â§ 1)
                  - $\\alpha = 0$: Ch·ªâ s·ª≠ d·ª•ng CBF
                  - $\\alpha = 0.5$: C√¢n b·∫±ng gi·ªØa GNN v√† CBF
                  - $\\alpha = 1$: Ch·ªâ s·ª≠ d·ª•ng GNN
                
                **Qu√° tr√¨nh chu·∫©n h√≥a:**
                1. Chu·∫©n h√≥a ƒëi·ªÉm GNN v·ªÅ [0, 1]: $\\hat{r}_{ui}^{\\text{GNN}} = \\frac{\\hat{r}_{ui}^{\\text{GNN}} - \\min(\\hat{r}^{\\text{GNN}})}{\\max(\\hat{r}^{\\text{GNN}}) - \\min(\\hat{r}^{\\text{GNN}})}$
                2. Chu·∫©n h√≥a ƒëi·ªÉm CBF v·ªÅ [0, 1]: $\\hat{r}_{ui}^{\\text{CBF}} = \\frac{\\hat{r}_{ui}^{\\text{CBF}} - \\min(\\hat{r}^{\\text{CBF}})}{\\max(\\hat{r}^{\\text{CBF}}) - \\min(\\hat{r}^{\\text{CBF}})}$
                3. K·∫øt h·ª£p tuy·∫øn t√≠nh v·ªõi tr·ªçng s·ªë $\\alpha$
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:** ƒêi·ªÉm $Score_{Hybrid}(u, i)$ c√≥ ƒë·ªô ch√≠nh x√°c d·ª± ƒëo√°n cao nh·∫•t, k·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ GNN (collaborative filtering) v√† CBF (content-based filtering).
                """)

        with st.expander("B∆∞·ªõc 4.3: T·∫°o Danh s√°ch g·ª£i √Ω c√° nh√¢n h√≥a v·ªõi Hybrid", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:**")
            st.write("1. **G·ª£i √Ω C√° nh√¢n h√≥a:** √Åp d·ª•ng Logic L·ªçc v√† ∆Øu ti√™n (B∆∞·ªõc 2.3) l√™n danh s√°ch ·ª©ng vi√™n ƒë∆∞·ª£c x·∫øp h·∫°ng b·ªüi $Score_{Hybrid}(u, i)$.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 4.1 & 4.2 (Hybrid Predictions)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_hybrid_predictions = 'hybrid_predictions' in st.session_state

                if not has_hybrid_predictions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 4.1 & 4.2 (Hybrid Predictions). Vui l√≤ng ch·∫°y B∆∞·ªõc 4.1 & 4.2 tr∆∞·ªõc.")
                else:
                    hybrid_predictions = st.session_state['hybrid_predictions']
                    
                    # Ki·ªÉm tra xem c√≥ h√†m apply_personalized_filters kh√¥ng
                    if apply_personalized_filters is not None:
                        # Load products and users data
                        products_path = os.path.join(current_dir, 'apps', 'exports', 'products.csv')
                        users_path = os.path.join(current_dir, 'apps', 'exports', 'users.csv')
                        
                        products_df = None
                        users_df = None
                        
                        if os.path.exists(products_path):
                            products_df = pd.read_csv(products_path)
                            if 'id' in products_df.columns:
                                products_df['id'] = products_df['id'].astype(str)
                                products_df.set_index('id', inplace=True)
                        else:
                            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file products.csv. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i trong apps/exports/")
                        
                        if os.path.exists(users_path):
                            users_df = pd.read_csv(users_path)
                            if 'id' in users_df.columns:
                                users_df['id'] = users_df['id'].astype(str)
                        else:
                            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file users.csv. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i trong apps/exports/")
                        
                        if products_df is not None:
                            # Ki·ªÉm tra format c·ªßa hybrid_predictions
                            if 'predictions' in hybrid_predictions:
                                predictions_dict = hybrid_predictions['predictions']
                            elif 'rankings' in hybrid_predictions:
                                # Convert rankings to predictions format
                                predictions_dict = {}
                                for user_id, ranking in hybrid_predictions['rankings'].items():
                                    user_id_str = str(user_id)
                                    predictions_dict[user_id_str] = {str(pid): score for pid, score in ranking}
                            else:
                                st.error("‚ùå Kh√¥ng t√¨m th·∫•y 'predictions' ho·∫∑c 'rankings' trong hybrid_predictions")
                                predictions_dict = {}
                            
                            if predictions_dict:
                                # C·∫•u h√¨nh
                                col_config1, col_config2 = st.columns(2)
                                with col_config1:
                                    selected_user_id = st.selectbox(
                                        "Ch·ªçn User ID ƒë·ªÉ √°p d·ª•ng l·ªçc",
                                        list(predictions_dict.keys()),
                                        key="hybrid_filter_user_id"
                                    )
                                
                                with col_config2:
                                    payload_articletype = st.selectbox(
                                        "Ch·ªçn articleType c·ªßa s·∫£n ph·∫©m ƒë·∫ßu v√†o (payload)",
                                        products_df['articleType'].unique().tolist() if 'articleType' in products_df.columns else [],
                                        key="hybrid_payload_articletype"
                                    )
                                
                                # Get user info
                                user_age = None
                                user_gender = None
                                if users_df is not None and selected_user_id:
                                    user_row = users_df[users_df['id'] == selected_user_id]
                                    if not user_row.empty:
                                        user_age = user_row.iloc[0].get('age', None)
                                        user_gender = user_row.iloc[0].get('gender', None)
                                
                                if selected_user_id and payload_articletype:
                                    col_info1, col_info2 = st.columns(2)
                                    with col_info1:
                                        if user_age is not None:
                                            st.info(f"üë§ User Age: {user_age}")
                                        if user_gender is not None:
                                            st.info(f"üë§ User Gender: {user_gender}")
                                    with col_info2:
                                        st.info(f"üì¶ Payload articleType: {payload_articletype}")
                                        if user_age is not None and user_gender is not None:
                                            allowed_genders = get_allowed_genders(user_age, user_gender) if get_allowed_genders else []
                                            st.info(f"‚úÖ Allowed Genders: {', '.join(allowed_genders)}")
                                    
                                    # Top-K configuration
                                    top_k_personalized = st.number_input(
                                        "S·ªë l∆∞·ª£ng s·∫£n ph·∫©m Top-K Personalized",
                                        min_value=5,
                                        max_value=100,
                                        value=20,
                                        step=5,
                                        key="hybrid_top_k_personalized"
                                    )
                                    
                                    process_button = st.button(
                                        "üîß √Åp d·ª•ng Personalized Filters v√† X·∫øp h·∫°ng Top-K v·ªõi Hybrid",
                                        type="primary",
                                        use_container_width=True,
                                        key="hybrid_personalized_filter_button"
                                    )
                                    
                                    if process_button:
                                        # ƒêo Inference Time (t·ª´ khi nh·∫≠n user ƒë·∫øn khi t·∫°o L(u) - B∆∞·ªõc 4.3)
                                        inference_start_time = time.time()
                                        
                                        with st.spinner("ƒêang √°p d·ª•ng c√°c b·ªô l·ªçc c√° nh√¢n h√≥a v√† x·∫øp h·∫°ng v·ªõi Hybrid scores..."):
                                            try:
                                                # L·∫•y danh s√°ch candidate products t·ª´ Hybrid predictions
                                                user_predictions = predictions_dict[selected_user_id]
                                                candidate_products = list(user_predictions.keys())
                                                
                                                # √Åp d·ª•ng filters v√† x·∫øp h·∫°ng Top-K v·ªõi Hybrid scores
                                                result = apply_personalized_filters(
                                                    candidate_products,
                                                    products_df,
                                                    payload_articletype=payload_articletype,
                                                    user_age=user_age,
                                                    user_gender=user_gender,
                                                    cbf_scores=user_predictions,  # S·ª≠ d·ª•ng hybrid scores nh∆∞ cbf_scores
                                                    top_k=top_k_personalized
                                                )
                                                
                                                # K·∫øt th√∫c ƒëo Inference Time
                                                inference_end_time = time.time()
                                                inference_time_measured = inference_end_time - inference_start_time
                                                
                                                st.success(f"‚úÖ **Ho√†n th√†nh!** ƒê√£ l·ªçc danh s√°ch ·ª©ng vi√™n v·ªõi Hybrid scores.")
                                                
                                                # L∆∞u v√†o session state
                                                if 'hybrid_personalized_filters' not in st.session_state:
                                                    st.session_state['hybrid_personalized_filters'] = {}
                                                st.session_state['hybrid_personalized_filters'][selected_user_id] = result
                                                # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                                save_intermediate_artifact('hybrid_personalized_filters', st.session_state['hybrid_personalized_filters'])
                                                
                                                # L∆∞u Inference Time v√†o session state (l·∫•y trung b√¨nh n·∫øu c√≥ nhi·ªÅu users)
                                                if 'hybrid_inference_times' not in st.session_state:
                                                    st.session_state['hybrid_inference_times'] = []
                                                st.session_state['hybrid_inference_times'].append(inference_time_measured)
                                                st.session_state['hybrid_inference_time'] = np.mean(st.session_state['hybrid_inference_times'])
                                                
                                                # Hi·ªÉn th·ªã th·ªëng k√™
                                                st.markdown("### üìä Th·ªëng k√™ qu√° tr√¨nh l·ªçc")
                                                
                                                stats = result['stats']
                                                col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                                                with col_stat1:
                                                    st.metric("Danh s√°ch ban ƒë·∫ßu", f"{stats['initial_count']:,}")
                                                with col_stat2:
                                                    st.metric("Sau l·ªçc articleType", f"{stats['after_articletype']:,}")
                                                with col_stat3:
                                                    st.metric("Sau l·ªçc Age/Gender", f"{stats['after_age_gender']:,}")
                                                with col_stat4:
                                                    st.metric(f"Top-K Personalized ({top_k_personalized})", f"{stats['final_count']:,}")
                                                
                                                # Hi·ªÉn th·ªã Top-K Personalized Rankings
                                                if result.get('ranked_products'):
                                                    st.markdown(f"### üìã Danh s√°ch Top-{top_k_personalized} Personalized (Hybrid)")
                                                    ranked_df = pd.DataFrame([
                                                        {
                                                            'Rank': rank + 1,
                                                            'Product ID': product_id,
                                                            'Hybrid Score': f"{score:.4f}"
                                                        }
                                                        for rank, (product_id, score) in enumerate(result['ranked_products'])
                                                    ])
                                                    st.dataframe(ranked_df, use_container_width=True)
                                                    
                                                    # Bi·ªÉu ƒë·ªì Top-K scores
                                                    fig_scores = px.bar(
                                                        ranked_df,
                                                        x='Rank',
                                                        y='Hybrid Score',
                                                        title=f"Top-{top_k_personalized} Personalized Hybrid Scores",
                                                        labels={'Rank': 'X·∫øp h·∫°ng', 'Hybrid Score': 'ƒêi·ªÉm Hybrid'}
                                                    )
                                                    st.plotly_chart(fig_scores, use_container_width=True)
                                                
                                                # Reduction visualization
                                                st.markdown("### üìâ Bi·ªÉu ƒë·ªì gi·∫£m k√≠ch th∆∞·ªõc danh s√°ch")
                                                reduction_df = pd.DataFrame({
                                                    'B∆∞·ªõc': ['Ban ƒë·∫ßu', 'Sau articleType', 'Sau Age/Gender', f'Top-{top_k_personalized}'],
                                                    'S·ªë l∆∞·ª£ng': [
                                                        stats['initial_count'],
                                                        stats['after_articletype'],
                                                        stats['after_age_gender'],
                                                        stats['final_count']
                                                    ]
                                                })
                                                
                                                fig = px.bar(
                                                    reduction_df,
                                                    x='B∆∞·ªõc',
                                                    y='S·ªë l∆∞·ª£ng',
                                                    title="Qu√° tr√¨nh gi·∫£m k√≠ch th∆∞·ªõc danh s√°ch ·ª©ng vi√™n (Hybrid)",
                                                    labels={'S·ªë l∆∞·ª£ng': 'S·ªë l∆∞·ª£ng s·∫£n ph·∫©m', 'B∆∞·ªõc': 'B∆∞·ªõc l·ªçc'}
                                                )
                                                st.plotly_chart(fig, use_container_width=True)
                                            
                                            except Exception as e:
                                                st.error(f"‚ùå L·ªói khi √°p d·ª•ng personalized filters v·ªõi Hybrid: {str(e)}")
                                                import traceback
                                                st.code(traceback.format_exc())
                                else:
                                    st.info("üí° Vui l√≤ng ch·ªçn User ID v√† articleType ƒë·ªÉ ti·∫øp t·ª•c.")
                            else:
                                st.warning("‚ö†Ô∏è Kh√¥ng c√≥ predictions trong hybrid_predictions. Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
                        else:
                            st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu products. Vui l√≤ng ki·ªÉm tra l·∫°i.")
                    elif apply_personalized_filters is None:
                        st.error(f"‚ùå Kh√¥ng th·ªÉ import cbf_utils module: {_cbf_utils_import_error}")
            
            with tab_algorithm:
                st.markdown("""
                **Quy tr√¨nh l·ªçc v√† x·∫øp h·∫°ng v·ªõi Hybrid Scores:**
                
                B∆∞·ªõc 4.3 √°p d·ª•ng c√πng logic l·ªçc c√° nh√¢n h√≥a nh∆∞ B∆∞·ªõc 2.3, nh∆∞ng s·ª≠ d·ª•ng ƒëi·ªÉm s·ªë Hybrid ($Score_{Hybrid}(u, i)$) thay v√¨ ƒëi·ªÉm CBF ($\\hat{r}_{ui}^{\\text{CBF}}$). 
                ƒêi·ªÉm Hybrid k·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ GNN v√† CBF, mang l·∫°i ƒë·ªô ch√≠nh x√°c v√† t√≠nh ƒëa d·∫°ng cao h∆°n.
                
                **1. L·ªçc C·ª©ng theo articleType (STRICT):**
                   - **Logic:** $i_{\\text{cand}} \\in I_{\\text{valid}}$ n·∫øu v√† ch·ªâ n·∫øu $i_{\\text{cand}}.\\text{articleType} = i_{\\text{payload}}.\\text{articleType}$
                   - **M·ª•c ƒë√≠ch:** ƒê·∫£m b·∫£o c√°c s·∫£n ph·∫©m g·ª£i √Ω c√πng lo·∫°i v·ªõi s·∫£n ph·∫©m ƒë·∫ßu v√†o (payload)
                   - **K·∫øt qu·∫£:** Lo·∫°i b·ªè t·∫•t c·∫£ c√°c s·∫£n ph·∫©m kh√¥ng c√πng lo·∫°i v·ªõi s·∫£n ph·∫©m ƒë·∫ßu v√†o
                   - **V√≠ d·ª•:** N·∫øu payload l√† "Trousers", ch·ªâ c√°c s·∫£n ph·∫©m "Trousers" m·ªõi ƒë∆∞·ª£c gi·ªØ l·∫°i
                
                **2. L·ªçc v√† ∆Øu ti√™n theo Gi·ªõi t√≠nh/ƒê·ªô tu·ªïi (Age/Gender Priority):**
                   - **Logic √Åp d·ª•ng (Strict Filtering):**
                     - N·∫øu $u.\\text{age} < 13$ v√† $u.\\text{gender} = \\text{'male'}$: $i_{\\text{cand}}.\\text{gender}$ ph·∫£i l√† $\\text{'Boys'}$
                     - N·∫øu $u.\\text{age} \\ge 13$ v√† $u.\\text{gender} = \\text{'female'}$: $i_{\\text{cand}}.\\text{gender}$ ph·∫£i l√† $\\text{'Women'}$ ho·∫∑c $\\text{'Unisex'}$
                   - **M·ª•c ƒë√≠ch:** ƒê·∫£m b·∫£o c√°c s·∫£n ph·∫©m ph√π h·ª£p v·ªõi ƒë·∫∑c ƒëi·ªÉm nh√¢n kh·∫©u h·ªçc c·ªßa ng∆∞·ªùi d√πng
                   - **Ph√¢n t√≠ch ∆Øu ti√™n/X·∫øp h·∫°ng:** C√°c s·∫£n ph·∫©m c√≤n l·∫°i sau khi l·ªçc c·ª©ng ƒë∆∞·ª£c x·∫øp h·∫°ng tr·ª±c ti·∫øp b·∫±ng ƒëi·ªÉm Hybrid ($Score_{Hybrid}(u, i)$)
                
                **3. X·∫øp h·∫°ng theo Hybrid Score:**
                   - **C√¥ng th·ª©c:** $Score_{Hybrid}(u, i) = \\alpha \\cdot \\hat{r}_{ui}^{\\text{GNN}} + (1 - \\alpha) \\cdot \\hat{r}_{ui}^{\\text{CBF}}$
                   - **∆Øu ƒëi·ªÉm:** K·∫øt h·ª£p s·ª©c m·∫°nh c·ªßa Graph Neural Network (h·ªçc t·ª´ c·∫•u tr√∫c ƒë·ªì th·ªã t∆∞∆°ng t√°c) v√† Content-Based Filtering (d·ª±a tr√™n ƒë·∫∑c tr∆∞ng s·∫£n ph·∫©m)
                   - **K·∫øt qu·∫£:** Danh s√°ch Top-K ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒëi·ªÉm Hybrid gi·∫£m d·∫ßn
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:**
                - ‚úÖ Danh s√°ch ·ª©ng vi√™n ƒë∆∞·ª£c l·ªçc ch·ªâ ch·ª©a c√°c s·∫£n ph·∫©m h·ª£p l·ªá v·ªÅ articleType, age, v√† gender
                - ‚úÖ Danh s√°ch ƒë∆∞·ª£c x·∫øp h·∫°ng theo ƒëi·ªÉm $Score_{Hybrid}(u, i)$ ƒë·ªÉ t·∫°o ra danh s√°ch Top-K Personalized cu·ªëi c√πng
                - ‚úÖ ƒê·∫£m b·∫£o t√≠nh h·ª£p l·ªá c∆° b·∫£n v√† ƒë·ªô ∆∞u ti√™n c·ªßa c√°c ƒë·ªÅ xu·∫•t
                - ‚úÖ Ch·∫•t l∆∞·ª£ng g·ª£i √Ω cao h∆°n nh·ªù k·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ GNN v√† CBF
                
                **So s√°nh v·ªõi B∆∞·ªõc 2.3:**
                - **B∆∞·ªõc 2.3:** S·ª≠ d·ª•ng $\\hat{r}_{ui}^{\\text{CBF}}$ (ch·ªâ d·ª±a tr√™n ƒë·∫∑c tr∆∞ng n·ªôi dung)
                - **B∆∞·ªõc 4.3:** S·ª≠ d·ª•ng $Score_{Hybrid}(u, i)$ (k·∫øt h·ª£p GNN + CBF)
                - **L·ª£i √≠ch:** Hybrid score mang l·∫°i ƒë·ªô ch√≠nh x√°c cao h∆°n v√† kh·∫£ nƒÉng ph√°t hi·ªán c√°c m·∫´u ph·ª©c t·∫°p t·ª´ ƒë·ªì th·ªã t∆∞∆°ng t√°c
                """)

        with st.expander("B∆∞·ªõc 4.4: T√≠nh to√°n S·ªë li·ªáu (ƒê√°nh gi√° M√¥ h√¨nh)", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** T√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë (Recall@K, NDCG@K,...) t∆∞∆°ng t·ª± nh∆∞ B∆∞·ªõc 2.4, s·ª≠ d·ª•ng $L(u)$ v√† c√°c tham s·ªë th·ªùi gian t∆∞∆°ng ·ª©ng c·ªßa Hybrid.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 4.1 & 4.2 (Hybrid Predictions)")

            tab_implementation, tab_algorithm = st.tabs(["Hi·ªán th·ª±c", "Thu·∫≠t to√°n"])
            
            with tab_implementation:
                # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc
                has_hybrid_predictions = 'hybrid_predictions' in st.session_state
                has_feature_encoding = 'feature_encoding' in st.session_state

                if not has_hybrid_predictions:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 4.1 & 4.2 (Hybrid Predictions). Vui l√≤ng ch·∫°y B∆∞·ªõc 4.1 & 4.2 tr∆∞·ªõc.")
                if not has_feature_encoding:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ B∆∞·ªõc 1.3 (Feature Encoding). C·∫ßn cho t√≠nh to√°n Diversity.")
                
                if has_hybrid_predictions and has_feature_encoding:
                    hybrid_predictions = st.session_state['hybrid_predictions']
                    encoding_result = st.session_state.get('feature_encoding', {})
                    encoded_matrix = encoding_result.get('encoded_matrix', None)
                    product_ids = encoding_result.get('product_ids', [])
                    
                    # Load interactions for ground truth
                    interactions_path = os.path.join(current_dir, 'apps', 'exports', 'interactions.csv')
                    interactions_df = None
                    if os.path.exists(interactions_path):
                        interactions_df = pd.read_csv(interactions_path)
                        if 'user_id' in interactions_df.columns:
                            interactions_df['user_id'] = interactions_df['user_id'].astype(str)
                        if 'product_id' in interactions_df.columns:
                            interactions_df['product_id'] = interactions_df['product_id'].astype(str)
                    
                    # C·∫•u h√¨nh
                    col_config1, col_config2 = st.columns(2)
                    with col_config1:
                        k_values_input = st.text_input(
                            "C√°c gi√° tr·ªã K (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)",
                            value="10,20",
                            key="hybrid_k_values_input"
                        )
                        try:
                            k_values = [int(k.strip()) for k in k_values_input.split(',')]
                        except:
                            k_values = [10, 20]
                            st.warning("‚ö†Ô∏è ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh: [10, 20]")
                    
                    with col_config2:
                        # Training Time = GNN Training Time + CBF Training Time
                        gnn_training_time = st.session_state.get('gnn_training_time', None)
                        cbf_training_time = st.session_state.get('training_time', None)
                        
                        training_time_auto = None
                        if gnn_training_time is not None and cbf_training_time is not None:
                            training_time_auto = gnn_training_time + cbf_training_time
                            st.info(f"‚è±Ô∏è **Training Time (t·ª± ƒë·ªông):** {training_time_auto:.3f}s (GNN: {gnn_training_time:.3f}s + CBF: {cbf_training_time:.3f}s)")
                        elif gnn_training_time is not None:
                            st.warning(f"‚ö†Ô∏è Ch·ªâ c√≥ GNN Training Time: {gnn_training_time:.3f}s. Thi·∫øu CBF Training Time.")
                            training_time_auto = gnn_training_time
                        elif cbf_training_time is not None:
                            st.warning(f"‚ö†Ô∏è Ch·ªâ c√≥ CBF Training Time: {cbf_training_time:.3f}s. Thi·∫øu GNN Training Time.")
                            training_time_auto = cbf_training_time
                        else:
                            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ Training Time. Vui l√≤ng ch·∫°y B∆∞·ªõc 2.1 v√† B∆∞·ªõc 3.4 tr∆∞·ªõc.")
                        
                        # Inference Time = GNN Inference + CBF Inference + Combination Time
                        gnn_inference_time = st.session_state.get('gnn_inference_time', None)
                        cbf_inference_time = st.session_state.get('inference_time', None)
                        
                        inference_time_auto = None
                        if gnn_inference_time is not None and cbf_inference_time is not None:
                            # Estimate combination time (usually very small, ~0.001s)
                            combination_time = 0.001
                            inference_time_auto = gnn_inference_time + cbf_inference_time + combination_time
                            st.info(f"‚è±Ô∏è **Inference Time (t·ª± ƒë·ªông):** {inference_time_auto:.3f}s (GNN: {gnn_inference_time:.3f}s + CBF: {cbf_inference_time:.3f}s + Combine: {combination_time:.3f}s)")
                        elif gnn_inference_time is not None:
                            st.warning(f"‚ö†Ô∏è Ch·ªâ c√≥ GNN Inference Time: {gnn_inference_time:.3f}s. Thi·∫øu CBF Inference Time.")
                            inference_time_auto = gnn_inference_time + 0.001
                        elif cbf_inference_time is not None:
                            st.warning(f"‚ö†Ô∏è Ch·ªâ c√≥ CBF Inference Time: {cbf_inference_time:.3f}s. Thi·∫øu GNN Inference Time.")
                            inference_time_auto = cbf_inference_time + 0.001
                        else:
                            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ Inference Time. Vui l√≤ng ch·∫°y B∆∞·ªõc 2.3 v√† B∆∞·ªõc 3.5 tr∆∞·ªõc.")
                        
                        # Cho ph√©p override th·ªß c√¥ng n·∫øu c·∫ßn
                        st.markdown("**Ho·∫∑c nh·∫≠p th·ªß c√¥ng (n·∫øu c·∫ßn):**")
                        training_time_manual = st.number_input(
                            "Training Time (gi√¢y) - Th·ªß c√¥ng",
                            min_value=0.0,
                            value=training_time_auto if training_time_auto is not None else 0.0,
                            step=0.1,
                            key="hybrid_training_time_input"
                        )
                        
                        inference_time_manual = st.number_input(
                            "Inference Time (gi√¢y) - Th·ªß c√¥ng",
                            min_value=0.0,
                            value=inference_time_auto if inference_time_auto is not None else 0.0,
                            step=0.1,
                            key="hybrid_inference_time_input"
                        )
                    
                    process_button = st.button(
                        "üîß T√≠nh to√°n Evaluation Metrics",
                        type="primary",
                        use_container_width=True,
                        key="hybrid_evaluation_metrics_button"
                    )
                    
                    if process_button:
                        # ƒêo Inference Time
                        inference_start_time = time.time()
                        
                        with st.spinner("ƒêang t√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°..."):
                            try:
                                # Prepare predictions format t·ª´ Hybrid Predictions
                                predictions_dict = {}
                                
                                if 'rankings' in hybrid_predictions:
                                    for user_id, user_ranking in hybrid_predictions['rankings'].items():
                                        user_id_str = str(user_id)
                                        ranked_products = [(str(pid), score) for pid, score in user_ranking]
                                        predictions_dict[user_id_str] = ranked_products
                                
                                # S·ª≠ d·ª•ng th·ªùi gian ƒë√£ ƒëo t·ª± ƒë·ªông ho·∫∑c th·ªùi gian nh·∫≠p th·ªß c√¥ng
                                final_training_time = training_time_manual if training_time_manual > 0 else training_time_auto
                                
                                # Prepare ground truth from interactions
                                ground_truth_dict = {}
                                
                                if interactions_df is not None and 'user_id' in interactions_df.columns and 'product_id' in interactions_df.columns:
                                    # Consider only positive interactions (purchase, like, cart)
                                    positive_interactions = interactions_df[
                                        interactions_df['interaction_type'].isin(['purchase', 'like', 'cart'])
                                    ] if 'interaction_type' in interactions_df.columns else interactions_df
                                    
                                    for user_id in predictions_dict.keys():
                                        user_id_str = str(user_id)
                                        user_interactions = positive_interactions[
                                            positive_interactions['user_id'] == user_id_str
                                        ]
                                        if not user_interactions.empty:
                                            relevant_items = set(user_interactions['product_id'].astype(str).unique())
                                            ground_truth_dict[user_id_str] = relevant_items
                                        else:
                                            ground_truth_dict[user_id_str] = set()
                                
                                # Get all items for coverage
                                all_items = set(product_ids) if product_ids else set()
                                
                                # K·∫øt th√∫c ƒëo Inference Time
                                inference_end_time = time.time()
                                inference_time_measured = inference_end_time - inference_start_time
                                
                                # S·ª≠ d·ª•ng inference time ƒë√£ ƒëo ho·∫∑c th·ªß c√¥ng
                                final_inference_time = inference_time_manual if inference_time_manual > 0 else inference_time_measured
                                
                                # Compute metrics
                                if compute_cbf_metrics is not None:
                                    result = compute_cbf_metrics(
                                        predictions_dict,
                                        ground_truth_dict,
                                        k_values=k_values,
                                        item_features=encoded_matrix,
                                        item_ids=product_ids,
                                        all_items=all_items,
                                        training_time=final_training_time,
                                        inference_time=final_inference_time,
                                        use_ild=True
                                    )
                                    
                                    st.success("‚úÖ **Ho√†n th√†nh!** ƒê√£ t√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë ƒë√°nh gi√°.")
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state['hybrid_evaluation_metrics'] = result
                                    # L∆∞u v√†o artifacts ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi ch·∫°y b∆∞·ªõc kh√°c
                                    save_intermediate_artifact('hybrid_evaluation_metrics', result)
                                    
                                    # Display results (similar to Step 2.5 and 3.5)
                                    st.markdown("### üìä K·∫øt qu·∫£ Evaluation Metrics")
                                    
                                    # Create metrics table
                                    metrics_data = []
                                    for k in k_values:
                                        metrics_data.append({
                                            'K': k,
                                            'Recall@K': f"{result['recall'].get(k, 0.0):.4f}",
                                            'Precision@K': f"{result['precision'].get(k, 0.0):.4f}",
                                            'NDCG@K': f"{result['ndcg'].get(k, 0.0):.4f}"
                                        })
                                    
                                    metrics_df = pd.DataFrame(metrics_data)
                                    st.dataframe(metrics_df, use_container_width=True)
                                    
                                    # Other metrics
                                    col_other1, col_other2, col_other3, col_other4 = st.columns(4)
                                    with col_other1:
                                        st.metric("Diversity (ILD@K)", f"{result['diversity']:.4f}" if result['diversity'] is not None else "N/A")
                                    with col_other2:
                                        st.metric("Coverage", f"{result['coverage']:.4f}" if result['coverage'] is not None else "N/A")
                                    with col_other3:
                                        st.metric("Training Time", f"{result['training_time']:.2f}s" if result['training_time'] is not None else "N/A")
                                    with col_other4:
                                        st.metric("Inference Time", f"{result['inference_time']:.2f}s" if result['inference_time'] is not None else "N/A")
                                    
                                    # Visualization
                                    st.markdown("### üìà Bi·ªÉu ƒë·ªì Metrics theo K")
                                    
                                    fig = go.Figure()
                                    fig.add_trace(go.Scatter(
                                        x=k_values,
                                        y=[result['recall'].get(k, 0.0) for k in k_values],
                                        mode='lines+markers',
                                        name='Recall@K',
                                        line=dict(color='blue', width=2)
                                    ))
                                    fig.add_trace(go.Scatter(
                                        x=k_values,
                                        y=[result['precision'].get(k, 0.0) for k in k_values],
                                        mode='lines+markers',
                                        name='Precision@K',
                                        line=dict(color='green', width=2)
                                    ))
                                    fig.add_trace(go.Scatter(
                                        x=k_values,
                                        y=[result['ndcg'].get(k, 0.0) for k in k_values],
                                        mode='lines+markers',
                                        name='NDCG@K',
                                        line=dict(color='red', width=2)
                                    ))
                                    fig.update_layout(
                                        title="Metrics theo K (Hybrid)",
                                        xaxis_title="K",
                                        yaxis_title="Score",
                                        hovermode='x unified'
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                                    
                                    # Summary table for export
                                    st.markdown("### üìã B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë (Export)")
                                    summary_data = {
                                        'Model': ['Hybrid']
                                    }
                                    
                                    # Th√™m c√°c metrics theo K values
                                    for k in k_values:
                                        summary_data[f'Recall@{k}'] = [f"{result['recall'].get(k, 0.0):.4f}"]
                                        summary_data[f'Precision@{k}'] = [f"{result['precision'].get(k, 0.0):.4f}"]
                                        summary_data[f'NDCG@{k}'] = [f"{result['ndcg'].get(k, 0.0):.4f}"]
                                    
                                    # Th√™m c√°c metrics kh√°c
                                    summary_data['Diversity (ILD@K)'] = [f"{result['diversity']:.4f}" if result['diversity'] is not None else "N/A"]
                                    summary_data['Coverage'] = [f"{result['coverage']:.4f}" if result['coverage'] is not None else "N/A"]
                                    summary_data['Training Time (s)'] = [f"{result['training_time']:.3f}" if result['training_time'] is not None else "N/A"]
                                    summary_data['Inference Time (s)'] = [f"{result['inference_time']:.3f}" if result['inference_time'] is not None else "N/A"]
                                    summary_df = pd.DataFrame(summary_data)
                                    st.dataframe(summary_df, use_container_width=True)
                                    
                                    st.markdown("""
                                    **‚úÖ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:**
                                    - ‚úÖ M·ªôt h√†ng d·ªØ li·ªáu ho√†n ch·ªânh trong B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë cho Hybrid
                                    - ‚úÖ Th·ªÉ hi·ªán hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh Hybrid (k·∫øt h·ª£p GNN + CBF)
                                    - ‚úÖ S·∫µn s√†ng ƒë·ªÉ so s√°nh v·ªõi c√°c m√¥ h√¨nh kh√°c (CBF, GNN)
                                    """)
                                else:
                                    st.error("‚ùå Kh√¥ng th·ªÉ import evaluation_metrics module.")
                            
                            except Exception as e:
                                st.error(f"‚ùå L·ªói khi t√≠nh to√°n evaluation metrics: {str(e)}")
                                import traceback
                                st.code(traceback.format_exc())
            
            with tab_algorithm:
                st.markdown("""
                **D·ªØ li·ªáu ƒê·∫ßu v√†o (ƒê∆∞·ª£c l·∫•y t·ª´):**
                - **Training Time (s):** T·ªïng th·ªùi gian hu·∫•n luy·ªán c·ªßa GNN v√† CBF ($\\text{Time}_{\\text{GNN}} + \\text{Time}_{\\text{CBF}}$).
                - **Inference Time (s):** T·ªïng th·ªùi gian t√≠nh to√°n $\\hat{r}_{ui}^{\\text{GNN}}$, $\\hat{r}_{ui}^{\\text{CBF}}$ v√† b∆∞·ªõc h·ª£p nh·∫•t ƒëi·ªÉm s·ªë.
                - **ILD, NDCG, Recall, Precision:** D·ªØ li·ªáu t∆∞∆°ng t·ª± B∆∞·ªõc 2.4, nh∆∞ng s·ª≠ d·ª•ng $L(u)$ ƒë∆∞·ª£c t·∫°o t·ª´ $Score_{Hybrid}(u, i)$.
                
                **C√°c ch·ªâ s·ªë ƒë√°nh gi√°:** T∆∞∆°ng t·ª± nh∆∞ B∆∞·ªõc 2.4 v·ªõi c√°c c√¥ng th·ª©c:
                - **Recall@K**, **Precision@K**, **NDCG@K**
                - **Diversity (ILD@K)**
                - **Coverage**
                
                **K·∫øt qu·∫£ mong ƒë·ª£i:** M·ªôt h√†ng d·ªØ li·ªáu ho√†n ch·ªânh trong B·∫£ng T·ªïng h·ª£p Ch·ªâ s·ªë cho Hybrid, th·ªÉ hi·ªán hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh Hybrid v√† s·∫µn s√†ng ƒë·ªÉ so s√°nh v·ªõi c√°c m√¥ h√¨nh kh√°c (CBF, GNN).
                """)

        st.markdown('<div class="sub-header">üìö PH·∫¶N V: B·∫¢NG T·ªîNG K·∫æT V√Ä SO S√ÅNH CH·ªà S·ªê</div>', unsafe_allow_html=True)
        st.markdown("")
        
        with st.expander("B∆∞·ªõc 5: B·∫£ng T·ªïng k·∫øt v√† So s√°nh Ch·ªâ s·ªë", expanded=True):
            # T·ª± ƒë·ªông restore artifacts tr∆∞·ªõc khi ki·ªÉm tra d·ªØ li·ªáu
            restore_all_artifacts()
            
            st.write("**N·ªôi dung th·ª±c hi·ªán:** T·ªïng h·ª£p v√† so s√°nh t·∫•t c·∫£ c√°c ch·ªâ s·ªë ƒë√°nh gi√° t·ª´ 3 m√¥ h√¨nh: CBF, GNN, v√† Hybrid.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** K·∫øt qu·∫£ t·ª´ B∆∞·ªõc 2.4 (CBF Metrics), B∆∞·ªõc 3.5 (GNN Metrics), v√† B∆∞·ªõc 4.4 (Hybrid Metrics)")
            
            st.markdown("""
            **M·ª•c ƒë√≠ch:**
            - So s√°nh hi·ªáu su·∫•t c·ªßa 3 m√¥ h√¨nh tr√™n c√πng m·ªôt b·ªô metrics
            - X√°c ƒë·ªãnh m√¥ h√¨nh t·ªëi ∆∞u d·ª±a tr√™n c√°c ti√™u ch√≠ ƒë√°nh gi√°
            - Ph√¢n t√≠ch ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu c·ªßa t·ª´ng m√¥ h√¨nh
            
            **C√°c ch·ªâ s·ªë ƒë∆∞·ª£c so s√°nh:**
            - **Recall@K** (K=10, 20): T·ª∑ l·ªá relevant items ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t
            - **Precision@K** (K=10, 20): T·ª∑ l·ªá items ƒë·ªÅ xu·∫•t l√† relevant
            - **NDCG@K** (K=10, 20): Ch·∫•t l∆∞·ª£ng x·∫øp h·∫°ng (ch·ªâ s·ªë ∆∞u ti√™n)
            - **Training Time (s):** Th·ªùi gian hu·∫•n luy·ªán m√¥ h√¨nh
            - **Inference Time (s):** Th·ªùi gian t√≠nh to√°n recommendations
            - **Coverage:** T·ª∑ l·ªá items ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t √≠t nh·∫•t m·ªôt l·∫ßn
            - **Diversity (ILD@K):** ƒê·ªô ƒëa d·∫°ng trong danh s√°ch ƒë·ªÅ xu·∫•t
            """)

            # Ki·ªÉm tra d·ªØ li·ªáu t·ª´ c√°c b∆∞·ªõc evaluation
            has_cbf_metrics = 'cbf_evaluation_metrics' in st.session_state
            has_gnn_metrics = 'gnn_evaluation_metrics' in st.session_state
            has_hybrid_metrics = 'hybrid_evaluation_metrics' in st.session_state

            col_check1, col_check2, col_check3 = st.columns(3)
            with col_check1:
                if has_cbf_metrics:
                    st.success("‚úÖ CBF Metrics")
                else:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ CBF Metrics")
            with col_check2:
                if has_gnn_metrics:
                    st.success("‚úÖ GNN Metrics")
                else:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ GNN Metrics")
            with col_check3:
                if has_hybrid_metrics:
                    st.success("‚úÖ Hybrid Metrics")
                else:
                    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ Hybrid Metrics")

            if has_cbf_metrics or has_gnn_metrics or has_hybrid_metrics:
                # Configuration for K values
                k_values_input = st.text_input(
                    "C√°c gi√° tr·ªã K ƒë·ªÉ hi·ªÉn th·ªã (ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)",
                    value="10,20",
                    key="comparison_k_values"
                )
                try:
                    k_values = [int(k.strip()) for k in k_values_input.split(',')]
                except:
                    k_values = [10, 20]
                    st.warning("‚ö†Ô∏è ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh: [10, 20]")

                # Collect metrics from all models
                comparison_data = []

                # CBF Metrics
                if has_cbf_metrics:
                    cbf_metrics = st.session_state['cbf_evaluation_metrics']
                    cbf_row = {'Model': 'CBF (Content-based)'}
                    
                    for k in k_values:
                        cbf_row[f'Recall@{k}'] = f"{cbf_metrics['recall'].get(k, 0.0):.4f}"
                        cbf_row[f'Precision@{k}'] = f"{cbf_metrics['precision'].get(k, 0.0):.4f}"
                        cbf_row[f'NDCG@{k}'] = f"{cbf_metrics['ndcg'].get(k, 0.0):.4f}"
                    
                    cbf_row['Training Time (s)'] = f"{cbf_metrics.get('training_time', 0.0):.3f}" if cbf_metrics.get('training_time') is not None else "N/A"
                    cbf_row['Inference Time (s)'] = f"{cbf_metrics.get('inference_time', 0.0):.3f}" if cbf_metrics.get('inference_time') is not None else "N/A"
                    cbf_row['Coverage'] = f"{cbf_metrics.get('coverage', 0.0):.4f}" if cbf_metrics.get('coverage') is not None else "N/A"
                    cbf_row['Diversity (ILD@K)'] = f"{cbf_metrics.get('diversity', 0.0):.4f}" if cbf_metrics.get('diversity') is not None else "N/A"
                    
                    comparison_data.append(cbf_row)

                # GNN Metrics
                if has_gnn_metrics:
                    gnn_metrics = st.session_state['gnn_evaluation_metrics']
                    gnn_row = {'Model': 'GNN'}
                    
                    for k in k_values:
                        gnn_row[f'Recall@{k}'] = f"{gnn_metrics['recall'].get(k, 0.0):.4f}"
                        gnn_row[f'Precision@{k}'] = f"{gnn_metrics['precision'].get(k, 0.0):.4f}"
                        gnn_row[f'NDCG@{k}'] = f"{gnn_metrics['ndcg'].get(k, 0.0):.4f}"
                    
                    gnn_row['Training Time (s)'] = f"{gnn_metrics.get('training_time', 0.0):.3f}" if gnn_metrics.get('training_time') is not None else "N/A"
                    gnn_row['Inference Time (s)'] = f"{gnn_metrics.get('inference_time', 0.0):.3f}" if gnn_metrics.get('inference_time') is not None else "N/A"
                    gnn_row['Coverage'] = f"{gnn_metrics.get('coverage', 0.0):.4f}" if gnn_metrics.get('coverage') is not None else "N/A"
                    gnn_row['Diversity (ILD@K)'] = f"{gnn_metrics.get('diversity', 0.0):.4f}" if gnn_metrics.get('diversity') is not None else "N/A"
                    
                    comparison_data.append(gnn_row)

                if has_hybrid_metrics:
                    hybrid_metrics = st.session_state['hybrid_evaluation_metrics']
                    hybrid_row = {'Model': 'Hybrid (GNN+CBF)'}
                    
                    for k in k_values:
                        hybrid_row[f'Recall@{k}'] = f"{hybrid_metrics['recall'].get(k, 0.0):.4f}"
                        hybrid_row[f'Precision@{k}'] = f"{hybrid_metrics['precision'].get(k, 0.0):.4f}"
                        hybrid_row[f'NDCG@{k}'] = f"{hybrid_metrics['ndcg'].get(k, 0.0):.4f}"
                    
                    hybrid_row['Training Time (s)'] = f"{hybrid_metrics.get('training_time', 0.0):.3f}" if hybrid_metrics.get('training_time') is not None else "N/A"
                    hybrid_row['Inference Time (s)'] = f"{hybrid_metrics.get('inference_time', 0.0):.3f}" if hybrid_metrics.get('inference_time') is not None else "N/A"
                    hybrid_row['Coverage'] = f"{hybrid_metrics.get('coverage', 0.0):.4f}" if hybrid_metrics.get('coverage') is not None else "N/A"
                    hybrid_row['Diversity (ILD@K)'] = f"{hybrid_metrics.get('diversity', 0.0):.4f}" if hybrid_metrics.get('diversity') is not None else "N/A"
                    
                    comparison_data.append(hybrid_row)

                if comparison_data:
                    comparison_df = pd.DataFrame(comparison_data)
                    
                    st.markdown("### üìä B·∫£ng T·ªïng k·∫øt v√† So s√°nh Ch·ªâ s·ªë")
                    st.dataframe(comparison_df, use_container_width=True, hide_index=True)
                    
                    csv = comparison_df.to_csv(index=False)
                    st.download_button(
                        "‚¨áÔ∏è T·∫£i xu·ªëng B·∫£ng So s√°nh (CSV)",
                        csv,
                        file_name="model_comparison.csv",
                        mime="text/csv",
                        key="comparison_download"
                    )
                    
                    # Visualization
                    st.markdown("### üìà Bi·ªÉu ƒë·ªì So s√°nh Metrics")
                    
                    # Select metrics to visualize
                    metric_types = st.multiselect(
                        "Ch·ªçn metrics ƒë·ªÉ so s√°nh",
                        ['Recall', 'Precision', 'NDCG'],
                        default=['Recall', 'Precision', 'NDCG'],
                        key="comparison_metrics"
                    )
                    
                    if metric_types:
                        for metric_type in metric_types:
                            fig = go.Figure()
                            
                            for idx, row in comparison_df.iterrows():
                                model_name = row['Model']
                                metric_values = []
                                
                                for k in k_values:
                                    value_str = row.get(f'{metric_type}@{k}', '0.0000')
                                    try:
                                        value = float(value_str)
                                    except:
                                        value = 0.0
                                    metric_values.append(value)
                                
                                fig.add_trace(go.Scatter(
                                    x=k_values,
                                    y=metric_values,
                                    mode='lines+markers',
                                    name=model_name,
                                    line=dict(width=2)
                                ))
                            
                            fig.update_layout(
                                title=f"{metric_type}@K Comparison",
                                xaxis_title="K",
                                yaxis_title=f"{metric_type}@K",
                                hovermode='x unified',
                                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                            )
                            st.plotly_chart(fig, use_container_width=True)
                    
                    # Comparison of other metrics
                    st.markdown("### üìä So s√°nh Training Time, Inference Time, Coverage, v√† Diversity")
                    
                    other_metrics = ['Training Time (s)', 'Inference Time (s)', 'Coverage', 'Diversity (ILD@K)']
                    other_data = []
                    
                    for metric in other_metrics:
                        metric_row = {'Metric': metric}
                        for idx, row in comparison_df.iterrows():
                            model_name = row['Model']
                            value_str = row.get(metric, 'N/A')
                            if value_str != 'N/A':
                                try:
                                    value = float(value_str)
                                    metric_row[model_name] = value
                                except:
                                    metric_row[model_name] = 0.0
                            else:
                                metric_row[model_name] = None
                        other_data.append(metric_row)
                    
                    other_df = pd.DataFrame(other_data)
                    
                    # Create bar chart for each metric
                    for metric in other_metrics:
                        metric_data = other_df[other_df['Metric'] == metric]
                        if not metric_data.empty:
                            fig = go.Figure()
                            
                            for col in comparison_df['Model'].values:
                                value = metric_data[col].iloc[0] if col in metric_data.columns else None
                                if value is not None:
                                    fig.add_trace(go.Bar(
                                        name=col,
                                        x=[metric],
                                        y=[value],
                                        text=f"{value:.4f}" if isinstance(value, float) else str(value),
                                        textposition='auto'
                                    ))
                            
                            fig.update_layout(
                                title=f"{metric} Comparison",
                                xaxis_title="Metric",
                                yaxis_title="Value",
                                barmode='group'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                    
                    # H∆∞·ªõng d·∫´n So s√°nh v√† L·ª±a ch·ªçn M√¥ h√¨nh T·ªëi ∆∞u
                    st.markdown("### üìñ H∆∞·ªõng d·∫´n So s√°nh v√† L·ª±a ch·ªçn M√¥ h√¨nh T·ªëi ∆∞u")
                    
                    st.markdown("""
                    **Ph√¢n t√≠ch t·∫≠p trung v√†o vi·ªác x√°c ƒë·ªãnh M√¥ h√¨nh Hybrid c√≥ ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng t·ªëi ∆∞u gi·ªØa c√°c nh√≥m ch·ªâ s·ªë hay kh√¥ng, ch·ª©ng minh t√≠nh ∆∞u vi·ªát c·ªßa ki·∫øn tr√∫c k·∫øt h·ª£p:**
                    
                    #### 1. Ch·ªâ s·ªë ∆Øu ti√™n (NDCG@10/20)
                    - **NDCG l√† th∆∞·ªõc ƒëo ch√≠nh c·ªßa ch·∫•t l∆∞·ª£ng x·∫øp h·∫°ng.**
                    - **Hybrid ph·∫£i ƒë·∫°t NDCG cao nh·∫•t** so v·ªõi CBF v√† GNN ri√™ng l·∫ª.
                    - NDCG cao cho th·∫•y m√¥ h√¨nh c√≥ kh·∫£ nƒÉng x·∫øp h·∫°ng c√°c items relevant ·ªü v·ªã tr√≠ cao h∆°n.
                    
                    #### 2. Ch·ªâ s·ªë H·ªó tr·ª£ (Diversity/Coverage)
                    - **ƒê√¢y l√† b·∫±ng ch·ª©ng cho kh·∫£ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ Cold Start v√† tr√°nh ƒê·ªãnh ki·∫øn Ph·ªï bi·∫øn.**
                    - **Hybrid ph·∫£i c√≥ Diversity v√† Coverage cao h∆°n GNN.**
                    - Diversity cao: Danh s√°ch ƒë·ªÅ xu·∫•t ƒëa d·∫°ng, kh√¥ng ch·ªâ t·∫≠p trung v√†o popular items.
                    - Coverage cao: Nhi·ªÅu items ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t, gi√∫p kh√°m ph√° items m·ªõi.
                    
                    #### 3. Ch·ªâ s·ªë V·∫≠n h√†nh (Inference Time)
                    - **M·∫∑c d√π Hybrid c√≥ Inference Time cao nh·∫•t**, s·ª± tƒÉng n√†y ph·∫£i ƒë∆∞·ª£c c√¢n b·∫±ng b·ªüi:
                      - S·ª± c·∫£i thi·ªán ƒë√°ng k·ªÉ v·ªÅ NDCG
                      - S·ª± c·∫£i thi·ªán v·ªÅ Diversity v√† Coverage
                    - Inference Time c·ªßa Hybrid = GNN Inference + CBF Inference + Combination Time
                    
                    #### 4. Ph√¢n t√≠ch T·ªïng h·ª£p
                    - **M√¥ h√¨nh t·ªëi ∆∞u:** Hybrid n√™n ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng t·ªët nh·∫•t gi·ªØa:
                      - ‚úÖ NDCG cao nh·∫•t (ch·∫•t l∆∞·ª£ng x·∫øp h·∫°ng)
                      - ‚úÖ Diversity v√† Coverage cao (kh·∫£ nƒÉng kh√°m ph√°)
                      - ‚úÖ Inference Time ch·∫•p nh·∫≠n ƒë∆∞·ª£c (hi·ªáu su·∫•t v·∫≠n h√†nh)
                    
                    #### 5. K·∫øt lu·∫≠n
                    - N·∫øu Hybrid ƒë·∫°t ƒë∆∞·ª£c c·∫£ 3 m·ª•c ti√™u tr√™n, n√≥ ch·ª©ng minh t√≠nh ∆∞u vi·ªát c·ªßa ki·∫øn tr√∫c k·∫øt h·ª£p.
                    - N·∫øu kh√¥ng, c·∫ßn ƒëi·ªÅu ch·ªânh tr·ªçng s·ªë Œ± ho·∫∑c c·∫£i thi·ªán t·ª´ng th√†nh ph·∫ßn (GNN ho·∫∑c CBF).
                    """)
                    
                    # Automatic analysis
                    st.markdown("### ü§ñ Ph√¢n t√≠ch T·ª± ƒë·ªông")
                    
                    if len(comparison_df) >= 3:
                        # Extract numeric values for comparison
                        def extract_value(value_str):
                            try:
                                return float(value_str)
                            except:
                                return 0.0
                        
                        # Compare NDCG@10 and NDCG@20
                        ndcg_10_values = {}
                        ndcg_20_values = {}
                        diversity_values = {}
                        coverage_values = {}
                        inference_times = {}
                        
                        for idx, row in comparison_df.iterrows():
                            model = row['Model']
                            ndcg_10_values[model] = extract_value(row.get('NDCG@10', '0.0000'))
                            ndcg_20_values[model] = extract_value(row.get('NDCG@20', '0.0000'))
                            diversity_values[model] = extract_value(row.get('Diversity (ILD@K)', '0.0000'))
                            coverage_values[model] = extract_value(row.get('Coverage', '0.0000'))
                            inference_times[model] = extract_value(row.get('Inference Time (s)', '0.0000'))
                        
                        # Find best model for each metric
                        best_ndcg_10 = max(ndcg_10_values.items(), key=lambda x: x[1])
                        best_ndcg_20 = max(ndcg_20_values.items(), key=lambda x: x[1])
                        best_diversity = max(diversity_values.items(), key=lambda x: x[1])
                        best_coverage = max(coverage_values.items(), key=lambda x: x[1])
                        fastest_inference = min(inference_times.items(), key=lambda x: x[1] if x[1] > 0 else float('inf'))
                        
                        col_analysis1, col_analysis2 = st.columns(2)
                        
                        with col_analysis1:
                            st.markdown("#### üèÜ M√¥ h√¨nh T·ªët nh·∫•t theo t·ª´ng Metric")
                            st.write(f"**NDCG@10:** {best_ndcg_10[0]} ({best_ndcg_10[1]:.4f})")
                            st.write(f"**NDCG@20:** {best_ndcg_20[0]} ({best_ndcg_20[1]:.4f})")
                            st.write(f"**Diversity:** {best_diversity[0]} ({best_diversity[1]:.4f})")
                            st.write(f"**Coverage:** {best_coverage[0]} ({best_coverage[1]:.4f})")
                            st.write(f"**Inference Time (nhanh nh·∫•t):** {fastest_inference[0]} ({fastest_inference[1]:.3f}s)")
                        
                        with col_analysis2:
                            st.markdown("#### üìä ƒê√°nh gi√° Hybrid Model")
                            
                            hybrid_ndcg_10 = ndcg_10_values.get('Hybrid (GNN+CBF)', 0.0)
                            hybrid_ndcg_20 = ndcg_20_values.get('Hybrid (GNN+CBF)', 0.0)
                            hybrid_diversity = diversity_values.get('Hybrid (GNN+CBF)', 0.0)
                            hybrid_coverage = coverage_values.get('Hybrid (GNN+CBF)', 0.0)
                            gnn_diversity = diversity_values.get('GNN', 0.0)
                            gnn_coverage = coverage_values.get('GNN', 0.0)
                            
                            if hybrid_ndcg_10 >= max([v for k, v in ndcg_10_values.items() if k != 'Hybrid (GNN+CBF)']):
                                st.success("‚úÖ Hybrid c√≥ NDCG@10 cao nh·∫•t")
                            else:
                                st.warning("‚ö†Ô∏è Hybrid kh√¥ng c√≥ NDCG@10 cao nh·∫•t")
                            
                            if hybrid_ndcg_20 >= max([v for k, v in ndcg_20_values.items() if k != 'Hybrid (GNN+CBF)']):
                                st.success("‚úÖ Hybrid c√≥ NDCG@20 cao nh·∫•t")
                            else:
                                st.warning("‚ö†Ô∏è Hybrid kh√¥ng c√≥ NDCG@20 cao nh·∫•t")
                            
                            if hybrid_diversity > gnn_diversity:
                                st.success("‚úÖ Hybrid c√≥ Diversity cao h∆°n GNN")
                            else:
                                st.warning("‚ö†Ô∏è Hybrid kh√¥ng c√≥ Diversity cao h∆°n GNN")
                            
                            if hybrid_coverage > gnn_coverage:
                                st.success("‚úÖ Hybrid c√≥ Coverage cao h∆°n GNN")
                            else:
                                st.warning("‚ö†Ô∏è Hybrid kh√¥ng c√≥ Coverage cao h∆°n GNN")
                            
                            if hybrid_ndcg_10 >= max([v for k, v in ndcg_10_values.items() if k != 'Hybrid (GNN+CBF)']) and \
                               hybrid_diversity > gnn_diversity:
                                st.success("üéØ **K·∫øt lu·∫≠n:** Hybrid ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng t·ªëi ∆∞u!")
                            else:
                                st.info("üí° **G·ª£i √Ω:** C√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh tr·ªçng s·ªë Œ± ho·∫∑c c·∫£i thi·ªán t·ª´ng th√†nh ph·∫ßn.")
                else:
                    st.info("üí° Vui l√≤ng ch·∫°y c√°c b∆∞·ªõc evaluation (2.5, 3.5, 4.4) ƒë·ªÉ c√≥ d·ªØ li·ªáu so s√°nh.")
            else:
                st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu metrics t·ª´ b·∫•t k·ª≥ m√¥ h√¨nh n√†o. Vui l√≤ng ch·∫°y c√°c b∆∞·ªõc evaluation tr∆∞·ªõc.")
    else:
        st.markdown("## üëó Recommendations")
        st.write("T·∫°o danh s√°ch g·ª£i √Ω c√° nh√¢n h√≥a v√† outfit d·ª±a tr√™n Hybrid (GNN + CBF).")

        products_df = load_products_data()
        users_df = load_users_data()
        interactions_df = load_interactions_data()

        if products_df is None or users_df is None:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu `products.csv` ho·∫∑c `users.csv`. Vui l√≤ng ch·∫°y b∆∞·ªõc xu·∫•t d·ªØ li·ªáu (1.1).")
            st.stop()

        user_index = users_df.index.astype(str)
        product_index = products_df.index.astype(str)

        # Ch·ªâ hi·ªÉn th·ªã c√°c user ƒë√£ c√≥ predictions (ƒë·ªß ƒëi·ªÅu ki·ªán) n·∫øu c√≥
        eligible_user_ids = None
        try:
            # ∆Øu ti√™n Hybrid ‚Üí GNN ‚Üí CBF
            pred_sources = [
                st.session_state.get("hybrid_predictions"),
                st.session_state.get("gnn_predictions") or st.session_state.get("gnn_training"),
                st.session_state.get("cbf_predictions"),
            ]
            for src in pred_sources:
                if not src or not isinstance(src, dict):
                    continue
                preds = src.get("predictions")
                if preds:
                    eligible_user_ids = {str(uid) for uid in preds.keys()}
                    break
        except Exception:
            eligible_user_ids = None

        if eligible_user_ids:
            # Ch·ªâ gi·ªØ l·∫°i nh·ªØng user n·∫±m trong t·∫≠p c√≥ predictions
            user_index_filtered = user_index[user_index.isin(eligible_user_ids)]
            user_options = user_index_filtered.tolist()
        else:
            # Fallback: hi·ªÉn th·ªã to√†n b·ªô user n·∫øu ch∆∞a c√≥ predictions n√†o
            user_options = user_index.tolist()

        product_options = product_index.tolist()

        def format_user_option(uid: str) -> str:
            row = get_user_record(uid, users_df)
            if row is None:
                return uid
            name = row.get('name') or row.get('email') or 'Unknown'
            return f"{name} ({uid})"

        def format_product_option(pid: str) -> str:
            row = get_product_record(pid, products_df)
            if row is None:
                return pid
            name = row.get('productDisplayName') or row.get('articleType') or 'Product'
            return f"{name} ({pid})"

        input_cols = st.columns(2)
        with input_cols[0]:
            selected_user = st.selectbox(
                "Ch·ªçn User",
                options=user_options,
                format_func=format_user_option,
                key="rec_user_select"
            )
            active_user_id = selected_user

        with input_cols[1]:
            selected_product = st.selectbox(
                "Ch·ªçn Product",
                options=product_options,
                format_func=format_product_option,
                key="rec_product_select"
            )
            active_product_id = selected_product

        config_cols = st.columns(3)
        with config_cols[0]:
            alpha = st.slider("Tr·ªçng s·ªë Hybrid Œ± (GNN ‚Üî CBF)", 0.0, 1.0, 0.5, 0.05)
        with config_cols[1]:
            top_k_personalized = st.number_input(
                "S·ªë l∆∞·ª£ng s·∫£n ph·∫©m Personalized",
                min_value=3,
                max_value=50,
                value=6,
                step=1
            )
        with config_cols[2]:
            top_outfits = st.number_input(
                "S·ªë l∆∞·ª£ng outfit mu·ªën xem",
                min_value=1,
                max_value=5,
                value=3,
                step=1
            )

        if active_product_id:
            st.markdown("### üìå S·∫£n ph·∫©m ƒë·∫ßu v√†o (payload)")
            payload_row = get_product_record(active_product_id, products_df)
            display_product_info(payload_row.to_dict() if payload_row is not None else {}, score=None)
            if payload_row is not None:
                st.caption(
                    f"ArticleType: {payload_row.get('articleType', 'N/A')} ‚Ä¢ "
                    f"Usage: {payload_row.get('usage', 'N/A')} ‚Ä¢ "
                    f"Gender: {payload_row.get('gender', 'N/A')}"
                )

        run_button = st.button("‚ú® T·∫°o g·ª£i √Ω", type="primary", use_container_width=True)

        if run_button:
            if not active_user_id or not active_product_id:
                st.warning("Vui l√≤ng ch·ªçn ƒë·∫ßy ƒë·ªß User v√† Product ƒë·ªÉ ti·∫øp t·ª•c.")
                st.stop()

            candidate_pool = max(int(top_k_personalized * 3), 100)
            hybrid_data = ensure_hybrid_predictions(alpha, candidate_pool)
            if hybrid_data is None:
                st.error("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu hybrid predictions. Vui l√≤ng ch·∫°y c√°c b∆∞·ªõc Training tr∆∞·ªõc.")
                st.stop()

            user_record = get_user_record(active_user_id, users_df)
            user_age = None
            if user_record is not None and pd.notna(user_record.get('age')):
                try:
                    user_age = int(user_record.get('age'))
                except (ValueError, TypeError):
                    user_age = None
            user_gender = user_record.get('gender') if user_record is not None else None

            personalized_items = build_personalized_candidates(
                user_id=active_user_id,
                payload_product_id=active_product_id,
                hybrid_predictions=hybrid_data,
                products_df=products_df,
                users_df=users_df,
                interactions_df=interactions_df,
                top_k=int(top_k_personalized)
            )

            if not personalized_items:
                preds = hybrid_data.get("predictions", {}) or {}
                has_hybrid_for_user = any(str(k) == str(active_user_id) for k in preds.keys())
                if not has_hybrid_for_user:
                    st.warning(
                        "Kh√¥ng c√≥ b·∫•t k·ª≥ ƒëi·ªÉm Hybrid n√†o cho user n√†y (ch∆∞a ƒë∆∞·ª£c train ho·∫∑c ƒë√£ b·ªã l·ªçc ·ªü b∆∞·ªõc tr∆∞·ªõc). "
                        "Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu train ho·∫∑c ch·ªçn user kh√°c."
                    )
                else:
                    st.warning(
                        "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o th·ªèa **articleType = articleType c·ªßa s·∫£n ph·∫©m ƒë·∫ßu v√†o** "
                        "trong Top candidate Hybrid. Vui l√≤ng th·ª≠ s·∫£n ph·∫©m kh√°c ho·∫∑c n·ªõi l·ªèng ƒëi·ªÅu ki·ªán."
                    )
            else:
                st.subheader("üéØ Personalized Products")
                allowed_genders = get_allowed_genders(user_age, user_gender)
                st.caption(f"∆Øu ti√™n gi·ªõi t√≠nh theo lu·∫≠t: {', '.join(allowed_genders)}")

                personal_table = []
                for idx, item in enumerate(personalized_items, start=1):
                    row = item['product_row']
                    personal_table.append({
                        "Rank": idx,
                        "Product ID": item['product_id'],
                        "Name": row.get('productDisplayName', 'N/A'),
                        "ArticleType": row.get('articleType', 'N/A'),
                        "Usage": row.get('usage', 'N/A'),
                        "Gender": row.get('gender', 'N/A'),
                        "Hybrid Score": round(item['base_score'], 4),
                        "Priority Score": round(item['score'], 4),
                        "Highlights": " ‚Ä¢ ".join(item['reasons']) or "-"
                    })

                st.dataframe(pd.DataFrame(personal_table), use_container_width=True)

                for idx, item in enumerate(personalized_items, start=1):
                    with st.expander(f"#{idx} - {item['product_row'].get('productDisplayName', 'Product')}"):
                        display_product_info(item['product_row'].to_dict(), score=item['score'])
                        st.write(f"- ArticleType: {item['product_row'].get('articleType', 'N/A')}")
                        st.write(f"- Usage: {item['product_row'].get('usage', 'N/A')}")
                        st.write(f"- Gender: {item['product_row'].get('gender', 'N/A')}")
                        if item['reasons']:
                            st.write(f"- ∆Øu ti√™n: {', '.join(item['reasons'])}")

                st.subheader("üß• Outfit Suggestions")
                
                # T√≠nh to√°n d·ªØ li·ªáu c·∫ßn thi·∫øt cho outfit suggestions
                payload_row = get_product_record(active_product_id, products_df)
                if payload_row is not None:
                    outfit_data = prepare_outfit_data(
                        payload_product_id=active_product_id,
                        payload_row=payload_row,
                        products_df=products_df,
                        personalized_items=personalized_items,
                        hybrid_predictions=hybrid_data,
                        user_id=active_user_id,
                        user_age=user_age,
                        user_gender=user_gender
                    )
                    
                    # Hi·ªÉn th·ªã c√°c b∆∞·ªõc th·ª±c t·∫ø
                    with st.expander("üìã C√°c b∆∞·ªõc x√¢y d·ª±ng Outfit Suggestions (Item-Item) - √Åp d·ª•ng th·ª±c t·∫ø", expanded=True):
                        display_outfit_building_steps(
                            payload_product_id=active_product_id,
                            payload_row=payload_row,
                            products_df=products_df,
                            personalized_items=personalized_items,
                            hybrid_predictions=hybrid_data,
                            user_id=active_user_id,
                            outfit_data=outfit_data
                        )
                        
                outfits = build_outfit_suggestions(
                    user_id=active_user_id,
                    payload_product_id=active_product_id,
                    personalized_items=personalized_items,
                    products_df=products_df,
                    hybrid_predictions=hybrid_data,
                    user_age=user_age,
                    user_gender=user_gender,
                    max_outfits=int(top_outfits)
                )

                if not outfits:
                    # Ki·ªÉm tra payload c√≥ ph·∫£i Unisex kh√¥ng ƒë·ªÉ hi·ªÉn th·ªã message ph√π h·ª£p
                    payload_row = get_product_record(current_product_id, products_df)
                    is_unisex = False
                    if payload_row is not None:
                        payload_gender = str(payload_row.get('gender', '')).strip().lower()
                        is_unisex = payload_gender == 'unisex'
                    
                    if is_unisex:
                        st.info("Ch∆∞a ƒë·ªß th√†nh ph·∫ßn ƒë·ªÉ t·∫°o outfit tho·∫£ ƒëi·ªÅu ki·ªán (Accessories / Topwear / Bottomwear / Footwear c√πng usage).")
                    else:
                        st.info("Ch∆∞a ƒë·ªß th√†nh ph·∫ßn ƒë·ªÉ t·∫°o outfit tho·∫£ ƒëi·ªÅu ki·ªán (Accessories / Topwear / Bottomwear / Footwear c√πng gender v√† c√πng usage).")
                else:
                    for idx, outfit in enumerate(outfits, start=1):
                        st.markdown(f"#### üëó Outfit #{idx}")
                        for pid in outfit['products']:
                            product_row = get_product_record(pid, products_df)
                            if product_row is not None:
                                display_product_info(product_row.to_dict(), score=None)
                        st.divider()

if __name__ == "__main__":
    main()
